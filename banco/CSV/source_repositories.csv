repo,repo_url,forks,creation,creation_date,creation_time,creation_year,last_update,update_date,update_time,update_year,description,readme,readme_snippet,contributors,stars,commits,open_issues,score,watchers,wiki
NVIDIA-AI-IOT/jetson-cloudnative-demo,https://github.com/NVIDIA-AI-IOT/jetson-cloudnative-demo,39,2020-05-06T18:05:58Z,2020-05-06,18:05:58Z,2020,2021-01-12T01:16:55Z,2021-01-12,01:16:55Z,2021,Multi-container demo for Jetson Xavier NX and Jetson AGX Xavier,"Cloud-Native Demo on Jetson  The NVIDIA Jetson platform supports cloud-native technologies and workflows such as containerization and orchestration. This support enables application development, deployment and management at scale, which is essential to deliver AI at the edge to millions of devices. This demo is built around example AI applications for a service robot use case. It was created specifically to showcase the capabilities of Jetson Xavier NX. (The complete demo will also run on Jetson AGX Xavier, but not other Jetsons, as some parts leverage Tensor cores not present in other Jetsons.) Service robots are autonomous robots that need to interact with people in retail, hospitality, healthcare, warehouse, and other settings.  For example, consider a customer service robot in a retail setting, interacting with customers and providing helpful answers to customer queries. Such a robot will need to perform the following tasks:  Identify humans Detect when a customer is talking to the robot Understand where a customer is pointing to while interacting with the robot Understand what a customer is asking Provide useful answers  Hence the robot will need multiple AI models such as:  People identification to identify humans Gaze detection to detect when a customer is talking to the robot (as opposed to someone else) Pose detection to detect customer’s pose Speech recognition to detect words in sentences spoken by the customer Natural language processing to understand the sentence, including context, to provide relevant answers back to the customer.  Following the cloud-native approach to application development, these individual models can be developed independently. Once an individual model is developed, it can be containerized with all dependencies included, and deployed to any Jetson device. For this demo we have developed and containerized the models, which are hosted on NVIDIA NGC. This demo runs seven models simultaneously as described below:  DeepStream Container with people detection  Resnet-18 model with input image size of 960X544X3. The model was converted from TensorFlow to TensorRT.   Pose container with pose detection  Resnet-18 model with input image resolution of 224X224. The model was converted from PyTorch to TensorRT.   Gaze container with gaze detection  MTCNN model for face detection with input image resolution of 260X135. The model was converted from Caffe to TensorRT. NVIDIA Facial landmarks model with input resolution of 80X80 per face. The model was converted from TensorFlow to TensorRT. NVIDIA Gaze model with input resolution of 224X224 per left eye, right eye and whole face. The model was converted from TensorFlow to TensorRT.   Voice container with speech recognition and Natural Language Processing  Quartznet-15X5 model for speech recognition which was converted from PyTorch to TensorRT. BERT Base/Large models for language model for NLP which were converted from TensorFlow to TensorRT.    These containers provide the building blocks of a service robot use case. Modifying applications and deploying updates is easy because of containerization. Other containers won't be affected by updates, giving zero down time and a seamless experience. You would need to install JetPack 4.3 to run this demo on Jetson Xavier NX Running the individual demo containers To run the demo containers individually, refer to the corresponding instructions at each container's NGC page:  DeepStream container with people detection Pose container with pose detection Gaze container with gaze detection Voice container with speech recognition and Natural Language Processing  Running the Cloud-Native Demo This demo requires two items in addition to a Jetson Xavier NX Developer Kit: 1) an M.2 NVMe drive, and 2) a USB Headset with microphone such as Logitech H110 or H390. Why is NVMe Required for this Demo? Since these demo containers are not yet fully optimized for storage and memory size, this demo requires NVMe for extra storage and adding swap space for extra virtual memory. The usual path for deploying containers into production involves optimization for size and memory usage. These demo containers have not yet gone through such optimizations. Instructions to Set up NVMe Drive The NVMe drive can be connected to the M.2 connector underneath the Jetson Xavier NX Developer Kit. Power down the developer kit and then connect the NVMe as shown in this picture:  Boot the developer kit, Format the NVMe, prepare the mount point, and then mount the NVMe.  NOTE that these examples and following refer to /home/nvidia, but you should replace with path to your home directory. sudo mkfs.ext4 /dev/nvme0n1 sudo mkdir /home/nvidia/nvme sudo mount /dev/nvme0n1 /home/nvidia/nvme Once the NVMe is mounted, add the following to /etc/fstab and then reboot the developer kit. the NVMe storage will be automounted going forward. /dev/nvme0n1 /home/nvidia/nvme ext4 defaults 0 1 Next, change the docker registry to point to NVMe so that the docker images are stored in NVMe.  NOTE that the second command below is optional unless you have previously pulled some containers, in which case it is required to move those docker images to NVMe: sudo mkdir /home/nvidia/nvme/docker sudo mv /var/lib/docker/* /home/nvidia/nvme/docker/. sudo ln -s /home/nvidia/nvme/docker /var/lib/docker Next, create a file swap on NVMe by following these instructions: ​	Turn off zram: cd /etc/systemd sudo mv nvzramconfig.sh nvzramconfig.sh.orig sudo reboot ​	Add swap file on nvme and verify: sudo fallocate -l 32G /home/nvidia/nvme/swapfile sudo chmod 600 /home/nvidia/nvme/swapfile sudo mkswap /home/nvidia/nvme/swapfile sudo swapon /home/nvidia/nvme/swapfile sudo swapon -s Add the line below to /etc/fstab so swap file will be automounted going forward: /home/nvidia/nvme/swapfile swap swap defaults 0 0 Reboot the developer kit after saving the /etc/fstab changes. Pulling the Containers Pull the 4 demo containers using the pull instruction mentioned in each container’s NGC page. Running the Demo First clone this repository: git clone https://github.com/NVIDIA-AI-IOT/jetson-cloudnative-demo Install the xdotool app by running the  command below: sudo apt-get install xdotool Go to the directory cd jetson-cloudnative-demo Launch the demo sudo ./run_demo.sh  The script will ask you to ensure that the USB Headset with Mic is connected. Once you make sure it is connected, please hit the Enter key. The script will take approximately two and half minutes to launch all four containers and begin running the concurrent inference workloads.  We recommend that you CLOSE all other applications (e.g, Chrome browser, Word document, etc.) before starting the demo, and that you do not interact with the containers during the launch process. The launch process is memory intensive and interactions may cause further slowdown. These containers were created for demo purpose only and (unlike real-world applications) are not optimized for memory and system resource usage.   When all four containers are successfully loaded, you can now start interacting with the demo.  Top Left Quadrant - People Detection Container The top left quadrant of the demo is running a containerized people detection inferencing task using NVIDIA DeepStream. It is analyzing four concurrent video streams to identify the number of people in each stream. Top Right Quadrant - Natural Language Processing Container The top right quadrant of the demo is running a containerized Natural Language Processing (NLP) demo using the demanding BERT NLP neural network. This demo takes your questions through voice input on specific topics and provides relevant answers based on the content available under each topic. Please follow these instructions to experience this part of the demo:  Select one of the several available topics by using the left/right arrow key on your keyboard. Read the content of each topic to come up with a question. Press the ‘space’ key on the keyboard and keep it pressed while asking your question (clearly, and with good volume) into the headset microphone. For example, under the topic titled GTC, you may want to ask questions such as “What is GTC?” or “Who is presenting the keynote”, “At what time is the keynote?” and other such questions. If relevant information is available in the content for your question, then the NLP network will provide a text answer that is shown on the screen. NOTE that the very first question may take a couple of seconds to register. You can also create your own topic, add your content and ask questions on that content using the “New” topic menu item. These neural networks have been trained but are not finely optimized like commercial assistants such as Google Assistant or Alexa. These are provided only for demo purposes to convey that Jetson Xavier NX is capable of running multiple networks concurrently while delivering real-time performance. Much more time would be spent fine-tuning the neural network performance for a commercial application.  Bottom Left Quadrant - Pose Estimation This container is running a pose estimation neural network to estimate the pose of people in the input video stream. For example, this information could be used by a retail service robot to figure out whether the person is pointing at a specific product in the store, or asking a delivery robot to stop based on the pose of the person’s hands, etc. Bottom Right Quadrant - Gaze Estimation This container is running a gaze estimation neural network to figure out whether the person in the frame is looking at the robot or looking somewhere else. Whenever the person looks at the robot, the boxes around the person’s eyes turn green. This information could be used to help the robot know when to interacting with the person. To end the demo, please go back to the terminal window by clicking on the terminal icon on the left side of your screen and hit “Enter” to close all containers."," built around example ai applications for a service robot use case. it was created specifically to showcase the capabilities of jetson xavier nx. (the complete demo will also run on jetson agx xavier, but not other jetsons, as some parts leverage tensor cores not present in other jetsons.) service robots are autonomous robots that need to interact with people robotbot use case. it was created specifically to showcase the capabilities of jetson xavier nx. (the com",2,106,21,0,1,0,1
AIIAL/HIAS,https://github.com/AIIAL/HIAS,10,2020-04-27T20:41:37Z,2020-04-27,20:41:37Z,2020,2021-01-10T06:26:02Z,2021-01-10,06:26:02Z,2021,HIAS is an open-source Hospital Intelligent Automation Server designed to control and manage an intelligent network of IoT connected devices.,"Peter Moss Leukemia AI Research HIAS - Hospital Intelligent Automation System         Table Of Contents  Introduction Key Features HIAS Network HIAS UI HIAS Blockchain HIAS Data Systems Interface  HDSI IoT Agents  MQTT AMQP     HIAS iotJumpWay Network  HIAS IoT Data Smart Contract HIAS IoT Zones HIAS IoT Devices HIAS IoT Sensors/Actuators HIAS IoT Applications HIAS IoT Data   HIAS Artificial Intelligence  HDSI AI Model Schemas HIAS Acute Lymphoblastic Leukemia Detection System (CNN) HIAS COVID-19 Detection System (CNN) HIAS Facial Recognition API HIAS Natural Language Understanding Engines   Installation HIAS Data Analysis  HIAS COVID-19 Data Analysis   EMAR / EMAR Mini Modular Addons Acknowledgement Contributing  Contributors   Versioning License Bugs/Issues       Introduction The Peter Moss Leukemia AI Research HIAS Network is an open-source Hospital Intelligent Automation System. The HIAS server powers an intelligent network providing secure access to devices on the network via a proxy. These devices/applications and databases all run and communicate on the local network. This means that premises have more control and security when it comes to their hardware, data and storage. Devices and applications on the HIAS network communicate with the server and each other using a local MQTT broker. The server hosts a private Ethereum blockchain which is integrated with the UI and provides upholds network access permissions, provides data integrity and accountability. This project is a proof of concept, and is still a work in progress.   Key Features   Local Web Server  Locally hosted webserver using NGINX.   High Grade SSL Encryption  High grade (A+) encryption for the web server, proxy and network.   Proxy  Secure access to local devices from the outside world.   HIAS Blockchain  Private Ethereum blockchain for access permissions, and providing data integrity & accountability.   HDSI Context Broker  Context Broker handles contextual data for iotJumpWay IoT Agents, AI Models, Devices, Applications, Staff & Patients.   HDSI IoT Agents  MQTT, AMQP & CoAP IoT Agents translate and push data coming from iotJumpWay Devices and Applications to the HDSI Context Broker.   System Database  MySQL database powering the HIAS UI.   IoT Database  The network IoT database is a Mongo database that stores all data from the HIAS network devices and applications, as well as iotJumpWay IoT Agents, AI Models, Devices, Applications, Staff & Patients contextual data.   Local Samba Server  A local Samba file server allowing controlled individual and group access to files on your local network.   Local IoT Broker  Local and private MQTT/Websockets broker based on the  iotJumpway Broker.   Server UI  A control panel to monitor and manage your HIAS network.   Facial Identification Systems  Facial identification systems based on TassAI.   Natural Language Understanding (NLU) Server  Natural Language Understanding server based on GeniSysAI.   COVID Data Analysis System  A data anaysis system for monitoring the COVID 19 pandemic. This system collects data from the Johns Hopkins University COVID-19 Daily Reports on Github.   AI Detection Systems  Detection systems for classsifying Acute Lymphoblastic Leukemia and COVID-19.   HIS/HMS  Hospital management system providing online tools for managing and running day to day activities and resources for the hospital.      HIAS Network  The HIAS Network consists of a range of open-source IoT devices and applications including data analysis systems, diagnosis systems, robots, facial recognition security systems and natural language understanding engines. The network devices are designed and optimized to run on low resource devices. Devices and applications can communicate autonomously using rules and the iotJumpWay MQTT broker.   HIAS UI  The HIAS UI is the central control panel for the server, and all of the modular devices and applications that can be installed on it. The server UI provides the capabalities to manage the network of open-soruce intelligent devices and applications.   HIAS Blockchain The HIAS Blockchain is a private Ethereum blockchain network that provides an immutable history of everything that happens on the HIAS network. Every user/device and application has a HIAS Blockchain address, meaning their actions can be recorded on the blockchain. Smart contracts provide additional security when it comes to verifying permissions, data hashes are stored on the blockchain providing data integrity. and each action made by staff members in the UI is recorded. The HIAS Blockchain network can be extended by installing additional full miner nodes which help to create blocks, seal transaction blocks, and also have a full copy of the entire HIAS Blockchain which remain synchronized.   HIAS Data Systems Interface  The HIAS Data Services Interface is a context broker that handles contextual information for the HIAS network. The broker implements the HDSI V1 API and allows easy management of HIAS iotJumpWay IoT Agents, AI Models, Devices, Applications, and HIAS Staff & Patient accounts and their related contextual data. HDSI is based on Open Mobile Alliance's NGSI, and has been customized to meet the requirements of the HIAS network. All iotJumpWay IoT Agents, AI Models, Devices, Applications, and HIAS Staff & Patient accounts have unique schemas that standardize their information and setup, these schemas are stored in the context broker. HDSI IoT Agents  The HDSI IoT Agents translate and push data sent from iotJumpWay Devices & Applications to the HDSI Context Broker. Each IoT Agent is responsible for it's own communication protocol, or transport. Current supported IoT protocols include HTTP, MQTT, Websockets & AMQP, with a CoAP broker and IoT Agent in development. The IoT Agents listen to data being sent on using their protocol and sends contextual data to the context broker, and historical data is stored in directly in the Mongo database. MQTT The MQTT (Message Queuing Telemetry Transport) protocol is one of the most well known and popular machine to machine communication protocols. Developed by IBM's Dr. Andy Stanford-Clark, the protocol is a lightweight publish and subscribe protocol designed for constrained devices. MQTT is primary communciation on the HIAS network. AMQP The AMQP (Advanced Message Queuing Protocol) is another popular machine to machine communication protocol. Although different to MQTT, it has the same function, allowing constrained devices and applications to communicate with each other.   HIAS iotJumpWay Network  The HIAS IoT network is powered by a new, fully open-source version of the iotJumpWay. The HIAS iotJumpway dashboard is your control panel for managing all of your network iotJumpWay zones, devices, sensors/actuators and applications. The iotJumpWay devices and applications that make up the HIAS network are connected to the HIAS network via the iotJumpWay. Each device and application has a unique identifier and credentials that allow communication with the server and devices behind the firewall and proxy. A HIAS network is represented by an iotJumpWay location. Within each location you can have multiple zones, devices and applications. HIAS IoT Zones  iotJumpWay Zones represent a room or area within a location. For instance, in a hospital you may have zones such as Reception, Waiting Room, Operating Room 1 etc.  HIAS IoT Devices  iotJumpWay Devices represent physical devices on the network. Each device is attached to a location and zone, allowing staff to know where each of their devices are, all devices publish their location to the network allowing for real-time tracking within the network.  HIAS IoT Sensors/Actuators  iotJumpWay Sensors & Actuators represent physical sensors and actuators included on network devices and allows direct communication with each sensor/actuator. This feature is still in development HIAS IoT Applications  iotJumpWay Devices represent applications that can communicate with the  network. Each application is attached to a location, soon all applications will publish their location to the system allowing for real-time tracking.  HIAS IoT Data  All data sent from devices and applications connected to the HIAS network is stored locally in a Mongo database (NoSQL). This means that staff can monitor all data on their network, and kall data stays on the network giving organizations total control of their data. HIAS IoT Data Smart Contract  The HIAS Blockchain hosts an iotJumpWay smart contract responsible for veryifing read and write access for iotJumpWay devices and applications, and storing immutable records of hashes of the data that is stored in the IoT database. The hashes provide the ability to verify data integrity by comparing the data in the database with the hash on the blockchain. The HIAS IoT Data Dashboard provides the functionality for checking the existing data against the hash on the blockchain.   HIAS Artificial Intelligence The HIAS network is made up of multiple open-source AI models, these models are provided through our official Github organization, as well as our research project Githubs. Current models available for the HIAS network include Leukemia and COVID classifiers, facial recognition and natural language understanding. HDSI AI Model Schemas  The HDSI schemas provide an easy way to manage and use the HIAS AI models. The schemas provide a standardization that allows you to not only use our models, but easily create your own and use them with the HIAS network. HIAS Acute Lymphoblastic Leukemia Detection System (CNN)  The HIAS Acute Lymphoblastic Leukemia Detection System (CNN) used the oneAPI Acute Lymphoblastic Leukemia Classifier, based on the proposed architecture in the Acute Leukemia Classification Using Convolution Neural Network In Clinical Decision Support System paper and using the Acute Lymphoblastic Leukemia Image Database for Image Processing dataset. The classifier achieves 98% accuracy at detecting Acute Lymphoblastic Leukemia in unseen data. HIAS COVID-19 Detection System (CNN)  The HIAS COVID-19 Detection System (CNN) system uses the COVID-19 Tensorflow DenseNet Classifier project, a Tensorflow 2 DenseNet implementation using the SARS-COV-2 Ct-Scan Dataset by our collaborators, Plamenlancaster: Professor Plamen Angelov from Lancaster University/ Centre Director @ Lira, & his researcher, Eduardo Soares PhD. The classifier achieves 92% accuracy at detecting COVID-19 in unseen data. HIAS Facial Recognition API  The HIAS facial recognition API is based on TassAI. The API allows for facial identification using authenticated HTTP requests from devices and applications that are authorized to communicate with the HIAS network. A range of open-source facial recognition systems can be attached to the network and use web and IP cameras attached to devices that process frames from the cameras in real-time, before streaming the processed framed to a local server endpoint. Multiple TassAI facial recognition devices can be configured. The cameras track known and unknown users and can communicate with the Natural Language Understanding Engines allowing conversations to be triggered based on facial recognition identifications.    HIAS Natural Language Understanding Engines  The HIAS UI allows Natural Language Understanding Engines to be connected to the network. These NLUs can be communicated with via the network allowing applications and devices to have realtime spoken interactions with known and unknown users.   Installation Installation scripts and tutorials for setting up your HIAS - Hospital Intelligent Automation System & UI are provided. To get started, please follow the installation guides provided below in the order they are given:    ORDER GUIDE INFORMATION AUTHOR     1 Main Installation Guide Primary installation guide covering most of the information needed to do the core installation Adam Milton-Barker      HIAS Data Analysis The HIAS network hosts a number of AI models that monitor data from local and external sources to make predictions based on the raw data. You can monitor real-time data using the HIAS UI. HIAS COVID-19 Data Analysis  Functionality is now available to set up a basic COVID-19 tracker that powers graphs in the HIAS UI. This system pulls data from the COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University and displays the stats in the UI.   EMAR / EMAR Mini  Functionality to update, monitor and control EMAR/EMAR Mini. These features allow you to create EMAR/EMAR Mini devices, update the settings, monitor the camera streams and send commands to the robotic arm to move it.      Modular Addons The HIAS network is made up of modular, intelligent devices. Below are some of the completed tutorials that can be used with the HIAS UI. Each project provides the details on how to connect them to the HIAS network, allowing them to controlled and monitored via the UI.    GITHUB README INFORMATION AUTHOR     Acute Myeloid & Lymphoblastic Leukemia AI Research Project oneAPI Acute Lymphoblastic Leukemia Classifier Uses an Acute Lymphoblastic Leukemia CNN based on the proposed architecture in the Acute Leukemia Classification Using Convolution Neural Network In Clinical Decision Support System paper, using the Acute Lymphoblastic Leukemia Image Database for Image Processing dataset. Adam Milton-Barker   Acute Myeloid & Lymphoblastic Leukemia AI Research Project Magic Leap 1 Acute Lymphoblastic Leukemia Detection System The Acute Lymphoblastic Leukemia Detection System 2020 uses Tensorflow 2 & Magic Leap to provide a mixed reality detection system. Uses the oneAPI Acute Lymphoblastic Leukemia Classifier. Adam Milton-Barker   COVID-19 AI Research Project COVID-19 Tensorflow DenseNet Classifier For Raspberry Pi 4 Uses DenseNet and SARS-COV-2 Ct-Scan Dataset, a large dataset of CT scans for SARS-CoV-2 (COVID-19) identification created by our collaborators, Plamenlancaster: Professor Plamen Angelov from Lancaster University/ Centre Director @ Lira, & his researcher, Eduardo Soares PhD Adam Milton-Barker   COVID-19 AI Research Project Magic Leap 1 COVID-19 Detection System The Magic Leap 1 COVID-19 Detection System 2020 uses Tensorflow 2, Raspberry Pi 4 & Magic Leap 1 to provide a spatial computing detection system. Uses the COVID-19 Tensorflow DenseNet Classifier For Raspberry Pi 4 Adam Milton-Barker   COVID-19 AI Research Project COVID-19 Detection System For Oculus Rift The Oculus Rift COVID-19 Detection System 2020 uses Tensorflow 2, Raspberry Pi 4 & Oculus Rift to provide a virtual detection system. Uses the COVID-19 Tensorflow DenseNet Classifier For Raspberry Pi 4 Adam Milton-Barker   COVID-19 AI Research Project EMAR Mini EMAR Mini is a minature version of EMAR, an open-source Emergency Robot Assistant to assist doctors, nurses and hospital staff during the COVID-19 pandemic, and similar situations we may face in the future. Adam Milton-Barker   Peter Moss Leukemia AI Research GeniSysAI HIAS GeniSysAI provides Natural Language Understanding. The projects provided in this repository are based on the original GeniSysAI projects. Adam Milton-Barker   Peter Moss Leukemia AI Research TassAI HIAS TassAI provides Facial Recognition security applications for the HIAS network. The projects provided in this repository are based on the original TassAI projects. Adam Milton-Barker   Peter Moss Leukemia AI Research HIAS NFC Authorization System The HIAS NFC Authorization System is an IoT connected NFC reader that can scan NFC implants, cards and fobs to identify users on the HIAS network. Adam Milton-Barker   Peter Moss Leukemia AI Research HIAS Miner Node The HIAS Blockchain Miner Nodes are additional nodes for the HIAS Blockchain. These nodes help to create blocks, seal transaction blocks, and also have a full copy of the entire HIAS Blockchain which remain synchronized. Adam Milton-Barker      Acknowledgement The template used for the UI in this project is a commercial template by  Hencework. We have been granted permission from Hencework to use their template in this project and would to thank them for doing so. Please check out their Portfolio for more examples of their work.   Contributing Peter Moss Leukemia AI Research encourages and welcomes code contributions, bug fixes and enhancements from the Github community. Please read the CONTRIBUTING document for a full guide to forking our repositories and submitting your pull requests. You will also find information about our code of conduct on this page. Contributors  Adam Milton-Barker - Peter Moss Leukemia AI Research Founder & Intel Software Innovator, Sabadell, Spain    Versioning We use SemVer for versioning. For the versions available, see Releases.   License This project is licensed under the MIT License - see the LICENSE file for details.   Bugs/Issues We use the repo issues to track bugs and general requests related to using this project. See CONTRIBUTING for more info on how to submit bugs, feature requests and proposals.","y to day activities and resources for the hospital.      hias network  the hias network consists of a range of open-source iot devices and applications including data analysis systems, diagnosis systems, robots, facial recognition security systems and natural language understanding engines. the network devices are designed and optimized to run on low resource devices. devices and applications can communicate autonomously using rules and the iotjumpway mqtt broker.   hias ui  the hias ui is the central control panel for the server, and all of the modular devices and applications that can be installed on it. the server ui provides the capabalities to manage the network of open-soruce intelligent devices and applications.   hias blockchain the hias blockchain is a private ethereum blockchain network that provides an immutable history of everything that happens on the hias network. every user/device and application has a hias blockchain address, meaning their actions can be recorded on the blockchain. smart contracts provide additional security when it comes to verifying permissions, data hashes are stored on the blockchain providing data integrity. and each action made by staff members in the ui is recorded. the hias blockchain network can be extended by installing additional full miner nodes which help to create blocks, seal transaction blocks, and also have a full copy of the entire hias blockchain which remain synchronized.   hias data systems interface  the hias data services interface is a context broker that handles contextual information for the hias network. the broker implements the hdsi v1 api and allows easy management of hias iotjumpway iot agents, ai models, devices, applications, and hias staff & patient accounts and their related contextual data. hdsi is based on open mobile alliance's ngsi, and has been customized to meet the requirements of the hias network. all iotjumpway iot agents, ai models, devices, applications, and hias staff & patient accounts have unique schemas that standardize their information and setup, these schemas are stored in the context broker. hdsi iot agents  the hdsi iot agents translate and push data sent from iotjumpway devices & applications to the hdsi context broker. each iot agent is responsible for it's own communication protocol, or transport. current supported iot protocols include http, mqtt, websockets & amqp, with a coap broker and iot agent in development. the iot agents listen to data being sent on using their protocol and sends contextual data to the context broker, and historical data is stored in directly in the mongo database. mqtt the mqtt (message queuing telemetry transport) protocol is one of the most well known and popular machine to machine communication protocols. developed by ibm's dr. andy stanford-clark, the protocol is a lightweight publish and subscribe protocol designed for constrained devices. mqtt is primary communciation on the hias network. amqp the amqp (advanced message queuing protocol) is another popular machine to machine communication protocol. although different to mqtt, it has the same function, allowing constrained devices and applications to communicate with each other.   hias iotjumpway network  the hias iot network is powered by a new, fully open-source version of the iotjumpway. the hias iotjumpway dashboard is your control panel for mana robot   hias network  the hias network consists of a range of open-source iot devices and applications in",0,19,149,0,1,0,1
rdbox-intec/rdbox,https://github.com/rdbox-intec/rdbox,39,2019-02-26T05:48:47Z,2019-02-26,05:48:47Z,2019,2021-01-02T19:43:49Z,2021-01-02,19:43:49Z,2021,RDBOX is an advanced IT platform for robotics and IoT developers that highly integrates cloud-native and edge computing technologies.,"RDBOX (A Robotics Developers BOX) RDBOX is an advanced IT platform for robotics and IoT developers that highly integrates cloud-native and edge computing technologies. Prepare RaspberryPi and AWS or Azure or Google Cloud Platform or Laptop(With Vagrant).       Don’t be surprised, there IT infrastructure is built automatically and maintained automatically. (DETAIL: Effect on you) And that is got Effect with only Run the scripts and Burn the SDCARD. (DETAIL: What you do)  Of course, protect the app at all layers of the OSI reference model.  Please do not worry. You can create an SD card easily by app. (Now offering Windows10 and MacOS version. Linux versions will be released soon.) ⬇️ Download Windows10 | ⬇️ Download MacOS | ⬇️ Download Linux  Click here for details. This is just an example of RDBOX functionality. You can start using the useful features right away with a click. The RDBOX App Market is a platform to easily add highly integrates cloud-native and edge computing technologies applications to your RDBOX environment to help you develop service robots and IoT devices. e.g. Jenkins, Harbor, Gogs, elasticsearch, kibana, logstash  Click here for details. In addition, you can easily control robots at multiple locations. Many other functions are available in RDBOX.   Table of Contents  How to use Effect on you What you do??(Materials you'll need to prepare) Features  1. Orchestrate all resources running ""ROS robots"" 2. Make It yourself 3. NETWORK CONNECT   Compared with other robotics platforms.  1. The RDBOX Provides ALL layers (L1 to L7) of the OSI reference model. 2. The RDBOX can be made with general equipment. 3. The RDBOX take in the good points of other companies' robot development platforms.   Components Our Mission  Solve: Short of talented engineers (48,000 people in Japan in 2020) Roadmap   Contributing Support & Contacts Licence  How to use Auto-build Kubernetes cluster(Use Docker as a containerd.) & Secure-Scalable Physical-network optimized for ROS robots.   Prepare RaspberryPi and AWS or Azure or Google Cloud Platform or PC. (Please see the wiki for details.) Please also refer to the latest release notes. If you just want to try RDBOX out, check out the Our Wiki page to give it a whirl. （←英語/日本語の二ヶ国語のマニュアルが用意されています。）  Supplementary information  One of our utilities, flashRDBOX, allows interactive dependency injection (DI) to RaspberryPi. There is no need for difficult operations. If you own TurtleBot3, you can also experience the deployment of ROS applications. Otherwise, you can learn the procedure for building development environment with RDBOX.  Effect on you RDBOX based on HypriotOS (Debian GNU/Linux 10)               .___.             /___/|             |   |/             .---.             RDBOX  - A Robotics Developers BOX -  You can get your only IT infrastructure.  Provides ALL layers (L1 to L7) of the OSI reference model. Mesh Wi-Fi network-covered space by Raspberry Pi Strict security Compute power provided by Kubernetes computer clusters Deploy and update ROS APPs by Kubernetes computer clusters   Full support for your robot working in on-site.  It is easy to separate part of the existing Enterprise network.  SoftEtherVPN_Stable go-transproxy   Other Robot Development Platform not enough support on-site.    What you do  Run the Script on your Virtual machine (VirtualBox or AWS or Azure or Google Cloud Platform). Burn the SDCARD Image for your Raspberry Pi.  Features Make your job easy with 3 features. 1. Orchestrate all resources running ""ROS robots/IoT Devices""  You will get a simpler and creative development experience than deploying with traditional roslaunch. Furthermore, it becomes easy to control a lot of groups of robots. Orchestrate ROS nodes on robots and conputer resources by Kubernetes.  Allow mixing of x86 and ARM architecture CPU. k8s master will run on AWS EC2 or Azure VirtualMachine or Google Cloud Platform ComputeEngine or VirtualBox on your PC.   Connect with the robots and others by Mesh Wi-Fi Network. Connect with the Clouds/On-Premise by VPN Network.   2. Make It yourself!!  The RDBOX Edge devices builds with Raspberry Pi 3B/3B+/4B. There is no worry that the back port will be installed. (All source code and hardware are disclosed.) Raspberry Pi provides you edge computing and Wi-Fi network and environmental sensors and more. Provide assembly procedure and original SD card image.   3. NETWORK CONNECT  Easily set up a dedicated local area network for robots.  Simply connect RDBOX in between the internet and your service robot. In one simple step, you can build a local area network and development environment. No knowledge of internet or networking is necessary.   Many network applications, including NTP, are offered with the product. Automate your network robot management. All you need is a power source. Cover the whole movable range of mobile robots with a Wi-Fi network.   Compared with other robotics platforms 3 Advantages compared to competitor's ""robot development platform"". 1. The RDBOX Provides ALL layers (L1 to L7) of the OSI reference model  Competitor's ""robot development platform"" does not support it. You may need to pay a great deal of money to a specialist for consultation.  Providing access points via mesh Wi-Fi. The robot just connects to the access point. It is possible to get security measures such as VPN and firewall andmore..., and convenient functions such as network application.    2. The RDBOX can be made with general equipment  You can start using it with the ""laptop"" and ""Raspberry Pi3B / 3B + / 4B"" you may already have.  3. The RDBOX take in the good points of other companies' robot development platforms  It can be used by combining ""simulator linkage"" and ""existing API service"" that other companies are good at.  Object Detection API Reinforcement learning by Gazebo. and more..    Components Our Components  RDBOX App Market  The RDBOX App Market is a platform to easily add highly integrates cloud-native and edge computing technologies applications to your RDBOX environment to help you develop service robots and IoT devices.   rdboxGARIBAN  Create an SD card for RDBOX using an easy-to-understand GUI.   go-transproxy  Transparent proxy servers for HTTP, HTTPS, DNS and TCP.   rdbox-middleware  Middleware for RDBOX   image-builder-rpi  SD card image for Raspberry Pi with Docker: HypriotOS    Third Components  hostapd  hostapd is an IEEE 802.11 AP and IEEE 802.1X/WPA/WPA2/EAP/RADIUS Authenticator. We are applying and applying our patch.   SoftEtherVPN_Stable  Open Cross-platform Multi-protocol VPN Software. We are applying and applying our patch.   bridge-utils  Utilities for configuring the Linux Ethernet bridge.   dnsmasq  network services for small networks.   nfs  support for NFS kernel server.   etc.....  Our Mission   Solve: Short of talented engineers (48,000 people in Japan in 2020)  Robotics is a complex technology. Therefore, the backbone of the engineer is different. Naturally, IT skills of each engineer are not constant. The use of IT technology is essential to build a robot system that links various types of service robots and IoT devices.  RDBOX (a IT infrastructure for ROS robots.) Boosts the productivity of engineers with various backbones  for Engineer with various backbones.  Create opportunities to experience ""Advanced IT technologies"".   for Advanced IT Engineer  Provides easy customization and high affinity to existing modules. (like a ToolBOX)   for Robotics beginners  Master the standard development process.(high productivity.)    Roadmap   Portable version  Multipoint connection  Improved fault tolerance (multiple sites version)  App Market  Multi-OS GUI SDWriter  aws and nvidia, cooperation  Azure  Google Cloud Platform  Create dedicated H/W  Docker File Generator  k8s. yaml file Generator  elastic plug-in  Contributing The following is a set of guidelines for contributing to RDBOX. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.  Contributions to our components that make up RDBOX apply similar rules.  RDBOX App Market rdboxGARIBAN go-transproxy rdbox-middleware image-builder-rpi     Fork this repository. Create a branch from master branch. Write code. Send a PR from the branch.  Support & Contacts For help and feedback, please feel free to contact us. ask Stack Overflow questions with #rdbox. or E-mail consultant is also available. RDBOX Project (info-rdbox@intec.co.jp) If you are favorite to RDBOX, please follow my GitHub account. Licence Licensed under the MIT license.",rdbox (a  robotics developers box) rdbox is an advanced it platform for robotics and iot developers that highly int,3,250,116,0,1,0,1
Pawel2357/IoT_home,https://github.com/Pawel2357/IoT_home,0,2019-03-03T18:35:06Z,2019-03-03,18:35:06Z,2019,2021-01-13T13:34:26Z,2021-01-13,13:34:26Z,2021,Self-sufficient AI home,Intro This repository contain hardware and software needed to build smart self-sufficient home. This include  renovable energy storage and production autonomous hydroponics garden smart light smart outlets  Coming soon  air quality home control home heating AI optimization control smart mirror autonomous vacuum cleaner autonomous weed removing robot  Hardware setup How to set up Node MCU v3:  Go to Arduino IDE Set board to NodeMCU 1.0 Connect to right port Upload the sketch  How to set up Arduino Uno3:  Go to Arduino IDE Set board to Arduino Uno Connect to right port Upload the sketch  Remark:  When uploading remove Rx and Tx conncetions. make sure broker IP(hostname -I) adress is correct  Software setup How to install ubuntu/raspberry broker sudo apt-add-repository ppa:mosquitto-dev/mosquitto-ppa sudo apt-get update sudo apt-get install mosquitto sudo apt-get install mosquitto-clients pip3 install paho-mqtt Jupyter lab /usr/bin/python3 /home/pawel/.local/bin/jupyter-lab --NotebookApp.token= --ip=0.0.0.0 Set up no-ip DUC  in first step log in to no-ip and create host https://www.noip.com/support/knowledgebase/getting-started-with-no-ip-com/ ubuntu https://www.noip.com/support/knowledgebase/installing-the-linux-dynamic-update-client-on-ubuntu/  Useful links https://icircuit.net/arduino-connecting-arduino-uno-esp8266/2443 https://techtutorialsx.com/2017/04/09/esp8266-connecting-to-mqtt-broker/ https://thingsboard.io/docs/samples/arduino/temperature/#esp8266-firmware https://slashposts.com/2018/04/mqtt-pubsubclient-tutorial-for-arduino-esp8266-esp32/ https://techtutorialsx.com/2017/04/24/esp32-publishing-messages-to-mqtt-topic/ good paho python tutorial http://www.steves-internet-guide.com/into-mqtt-python-client/ http://www.steves-internet-guide.com/mqtt-python-callbacks/ http://www.steves-internet-guide.com/loop-python-mqtt-client/,mart mirror autonomous vacuum cleaner autonomous weed removing robot  hardware setup how to set up node mcu v3:  go to arduino ide set board to nodemcu 1.0 connect to right port upload the sketch  how to set up arduino uno3:  go to arduino ide set board to arduino uno connect to right port upload the sketch  remark:  when uploadin robotremoving robot  hardware setup how to set up node mcu v3:  go to arduino ide set board to nodemcu 1.,1,0,299,0,1,0,1
Samsung/cotopaxi,https://github.com/Samsung/cotopaxi,76,2019-01-29T08:43:29Z,2019-01-29,08:43:29Z,2019,2021-01-11T17:05:32Z,2021-01-11,17:05:32Z,2021,Set of tools for security testing of Internet of Things devices using specific network IoT protocols,"Cotopaxi Set of tools for security testing of Internet of Things devices using protocols: AMQP, CoAP, DTLS, HTCPCP, mDNS, MQTT, MQTT-SN, QUIC, RTSP, SSDP. License: Cotopaxi uses GNU General Public License, version 2: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html Installation:  Clone code from git:  git clone https://github.com/Samsung/cotopaxi    Enter cotopaxi directory  cd cotopaxi    Install scapy-ssl_tls (in case of any problems with scapy and scapy-ssl_tls see section below)  For Python 2.7: (this will install also scapy in 2.4.2)     pip install git+https://github.com/tintinweb/scapy-ssl_tls@ec5714d560c63ea2e0cce713cec54edc2bfa0833  For Python >= 3.6:     git clone https://github.com/kalidasya/scapy-ssl_tls.git     cd scapy-ssl_tls     git checkout py3_update     sudo python3.6 setup.py install   Install other requirements:  For Python 2.7: sudo python2.7 -m pip install -r requirements_python2.txt   For Python >= 3.6: sudo python3.6 -m pip install --upgrade pip sudo python3.6 -m pip install -r requirements.txt    Run installer:  sudo python setup.py install  Requirements: Currently Cotopaxi works with Python 2.7.* and with Python 3.6.* (not tested with other versions - please report, if you would like to use . Installation of required libraries:  scapy-ssl_tls  For Python 2.7: (this will install also scapy in 2.4.2)     pip install git+https://github.com/tintinweb/scapy-ssl_tls@ec5714d560c63ea2e0cce713cec54edc2bfa0833  Common problems:  If you encounter error: error: [Errno 2] No such file or directory: 'LICENSE', try repeating command - surprisingly it works. If you encounter error: NameError: name 'os' is not defined - add missing import os to scapy/layers/ssl_tls.py.  All other required packages can be installed using requirements.txt file:     pip install -r cotopaxi/requirements.txt  All required packages for developement of Cotopaxi (including libraries for unit tests) can be installed using requirements_devel.txt file:     pip install -r cotopaxi/requirements_devel.txt     pre-commit install  Disclaimer Cotopaxi toolkit is intended to be used only for authorized security testing! Some tools (especially vulnerability tester and protocol fuzzer) can cause some devices or servers to stop acting in the intended way -- for example leading to crash or hang of tested entities or flooding with network traffic another entities. Make sure you have permission from the owners of tested devices or servers before running these tools! Make sure you check with your local laws before running these tools! Acknowlegments Machine learning classificator used in the device_identification tool was trained using corpus ""IMC 2019 payload dataset"" provided by authors of the following paper: Title: Information Exposure for Consumer IoT Devices: A Multidimensional, Network-Informed Measurement Approach Authors: Jingjing Ren, Daniel J. Dubois, David Choffnes, Anna Maria Mandalari, Roman Kolcun, Hamed Haddadi Venue: Internet Measurement Conference (IMC) 2019 URL: https://moniotrlab.ccis.neu.edu/imc19dataset/ We would like to thank above listed authors for sharing this corpus! Tools in this package:  service_ping server_fingerprinter device_identification traffic_analyzer resource_listing protocol_fuzzer (for fuzzing servers) client_proto_fuzzer (for fuzzing clients) vulnerability_tester (for testing servers) client_vuln_tester (for testing clients) amplifier_detector active_scanner  Protocols supported by different tools (left box describes working implementation in Python 2 and right one for Python 3):    Tool AMQP CoAP DTLS HTCPCP mDNS MQTT MQTT-SN QUIC RTSP SSDP     service_ping ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑   server_fingerprinter  ☑☑ ☑☑          device_identification ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑   traffic_analyzer ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑ ☐☑   credential_cracker  N/A N/A N/A N/A   N/A N/A N/A   resource_listing  ☑☑ N/A  ☑☑    ☑☑ ☑☑   protocol_fuzzer ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑   client_proto_fuzzer ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑   vulnerability_tester ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑   client_vuln_tester ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑ ☑☑   amplifier_detector N/A ☑☑ ☑☑ N/A ☑☑ N/A ☑☑ ☑☑ N/A ☑☑   active_scanner   ☑☑           cotopaxi.service_ping Tool for checking availability of network endpoints at given IP and port ranges usage: sudo python -m cotopaxi.service_ping [-h] [--retries RETRIES] [--timeout TIMEOUT]                        [--verbose]                        [--protocol {ALL,UDP,TCP,CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}]                        [--src-ip SRC_IP] [--src-port SRC_PORT]                        dest_addr dest_port  positional arguments:   dest_addr             destination hostname, IP address or multiple IPs                         separated by coma (e.g. '1.1.1.1,2.2.2.2') or given by                         CIDR netmask (e.g. '10.0.0.0/22') or both    dest_port             destination port or multiple ports given by list                         separated by coma (e.g. '8080,9090') or port range                         (e.g. '1000-2000') or both  optional arguments:   -h, --help            show this help message and exit   --retries RETRIES, -R RETRIES                         number of retries   --timeout TIMEOUT, -T TIMEOUT                         timeout in seconds   --verbose, -V, --debug, -D                         Turn on verbose/debug mode (more messages)   --protocol {ALL,UDP,TCP,CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}, -P {ALL,UDP,TCP,CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}                         protocol to be tested (UDP includes all UDP-based                         protocols, while TCP includes all TCP-based protocols,                         ALL includes all supported protocols)   --src-ip SRC_IP, -SI SRC_IP                         source IP address (return result will not be                         received!)   --src-port SRC_PORT, -SP SRC_PORT                         source port (if not specified random port will be                         used)    cotopaxi.server_fingerprinter Tool for software fingerprinting of network endpoints at given IP and port ranges Currently supported servers:  CoAP:  aiocoap, CoAPthon, FreeCoAP, libcoap, MicroCoAP, Mongoose Wakaama (formerly liblwm2m)   DTLS:  GnuTLS, Goldy, LibreSSL, MatrixSSL, mbed TLS, OpenSSL, TinyDTLS    usage: sudo python -m cotopaxi.server_fingerprinter [-h] [--retries RETRIES] [--timeout TIMEOUT]                                [--verbose] [--protocol {CoAP,DTLS}]                                [--src-ip SRC_IP] [--src-port SRC_PORT]                                [--ignore-ping-check]                                dest_addr dest_port  positional arguments:   dest_addr             destination hostname, IP address or multiple IPs                         separated by coma (e.g. '1.1.1.1,2.2.2.2') or given by                         CIDR netmask (e.g. '10.0.0.0/22') or both    dest_port             destination port or multiple ports given by list                         separated by coma (e.g. '8080,9090') or port range                         (e.g. '1000-2000') or both  optional arguments:   -h, --help            show this help message and exit   --retries RETRIES, -R RETRIES                         number of retries   --timeout TIMEOUT, -T TIMEOUT                         timeout in seconds   --verbose, -V, --debug, -D                         Turn on verbose/debug mode (more messages)   --protocol {CoAP,DTLS}, -P {CoAP,DTLS}                         protocol to be tested   --src-ip SRC_IP, -SI SRC_IP                         source IP address (return result will not be                         received!)   --src-port SRC_PORT, -SP SRC_PORT                         source port (if not specified random port will be                         used)   --ignore-ping-check, -Pn                         ignore ping check (treat all ports as alive)    cotopaxi.device_identification Tool for passive identification of IoT devices using captured network traffic Currently supported devices:  Amazon Cloudcam Amazon Echo Dot Amazon Echo Plus Amazon Echo Spot Amazon Fire TV Amcrest Camera Anova Sousvide Apple TV Blink Camera Blink Security Hub Bosiwo Camera D-Llink Mov Sensor Flux Bulb GE Microwave Google Home Google Home Mini Harman Kardon Allure Harman Kardon Invoke Honeywell Thermostat Insteon Hub Lefun Cam LG Smart TV Luohe Cam Magichome Strip Microseven Camera Nest Thermostat Netatmo Weather Station Osram Lightify Hub Philips Hue (Lightbulb) Philips Hue Hub Ring Doorbell Roku TV Samsung Fridge Samsung Dryer Samsung SmartThings Hub Samsung SmartTV Samsung Washer Sengled Smart Hub Smarter Brewer Smarter Coffee Machine Smarter iKettle TP-Link Bulb TP-Link Smart Plug Wansview Camera WeMo Plug WiMaker Charger Camera Wink Hub 2 Xiaomi Mi Cam 2 Xiaomi Mi Robot Cleaner Xiaomi Mi Hub Xiaomi Mi Rice Cooker Xiaomi Mi Power Strip Yi Camera Zmodo Greet (doorbell)  usage: python -m cotopaxi.device_identification usage: [-h] [--verbose] [--min MIN] [--max MAX]                                 [--ip IP] [-S]                                 pcap  Tool for classifying IoT devices based on captured network traffic  positional arguments:   pcap                  Packet capture file (in PCAP or PCAPNG format) with                         recorded traffic for device identification  optional arguments:   -h, --help            show this help message and exit   --verbose, -V, --debug, -D                         turn on verbose/debug mode (more messages)   --min MIN             minimum number of packets to classify device (devices                         with smaller number will not be classified) (default: 3)   --max MAX             maximum number of packets used to classify device                         (default: 1000)   --ip IP, -I IP        use IP filter to identify device   -S, --short           display only short result of classification     cotopaxi.traffic_analyzer Tool for passive identification of network protocol using captured network traffic Currently supported protocols:  AMQP BGP CMP CoAP DHCP DLNA DNS DTLS EIGRP FTP GNUTELLA GRE H323 HSRP HTTP HTCPCP IGMP IPP IPsec IRC LLMNR mDNS MQTT MQTT-SN MSTP NTLM NTP OCSP OSPF QUIC RADIUS RIP RPC RTSP SIP SMB SMTP SNMP SSDP SSH TACACS TELNET TFTP TLS VRRP  usage: python -m cotopaxi.traffic analyzer usage: [-h] [--verbose] [--min MIN] [--max MAX]                                 [--ip IP] [-S]                                 pcap  Tool for classifying network protocols used in traffic flows  positional arguments:   pcap                  Packet capture file (in PCAP or PCAPNG format) with                         recorded traffic for network protocols identification  optional arguments:   -h, --help            show this help message and exit   --verbose, -V, --debug, -D                         turn on verbose/debug mode (more messages)   --min MIN             minimum number of packets to classify                         conversation(conversations with smaller number will                         not be classified) (default: 3)   --max MAX             maximum number of packets used to classify                         conversation (default: 1000)   --ip IP, -I IP        use IP filter to identify protocol   -S, --short           display only short result of classification     cotopaxi.resource_listing Tool for checking availability of resource named url on server at given IP and port ranges. Sample URL lists are available in the urls directory usage: sudo python -m cotopaxi.resource_listing [-h] [--retries RETRIES] [--timeout TIMEOUT]                            [--verbose] [--protocol {CoAP,HTTP,mDNS,RTSP,SSDP}]                            [--src-ip SRC_IP] [--src-port SRC_PORT]                            [--ignore-ping-check]                            [--method {GET,POST,PUT,DELETE,ALL} [{GET,POST,PUT,DELETE,ALL} ...]]                            dest_addr dest_port names_filepath  positional arguments:   dest_addr             destination hostname, IP address or multiple IPs                         separated by coma (e.g. '1.1.1.1,2.2.2.2') or given by                         CIDR netmask (e.g. '10.0.0.0/22') or both    dest_port             destination port or multiple ports given by list                         separated by coma (e.g. '8080,9090') or port range                         (e.g. '1000-2000') or both   names_filepath        path to file with list of names (URLs for CoAP or                         services for mDNS) to be tested (each name in                         separated line)  optional arguments:   -h, --help            show this help message and exit   --retries RETRIES, -R RETRIES                         number of retries   --timeout TIMEOUT, -T TIMEOUT                         timeout in seconds   --verbose, -V, --debug, -D                         Turn on verbose/debug mode (more messages)   --protocol {CoAP,HTTP,mDNS,RTSP,SSDP}, -P {CoAP,HTTP,mDNS,RTSP,SSDP}                         protocol to be tested   --src-ip SRC_IP, -SI SRC_IP                         source IP address (return result will not be                         received!)   --src-port SRC_PORT, -SP SRC_PORT                         source port (if not specified random port will be                         used)   --ignore-ping-check, -Pn                         ignore ping check (treat all ports as alive)   --method {GET,POST,PUT,DELETE,ALL} [{GET,POST,PUT,DELETE,ALL} ...], -M {GET,POST,PUT,DELETE,ALL} [{GET,POST,PUT,DELETE,ALL} ...]                         methods to be tested (ALL includes all supported                         methods)    cotopaxi.protocol_fuzzer Black-box fuzzer for testing protocol servers usage: sudo python -m cotopaxi.protocol_fuzzer [-h] [--retries RETRIES] [--timeout TIMEOUT]                           [--verbose]                           [--protocol {CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}]                           [--hide-disclaimer] [--src-ip SRC_IP]                           [--src-port SRC_PORT] [--ignore-ping-check]                           [--corpus-dir CORPUS_DIR]                           [--delay-after-crash DELAY_AFTER_CRASH]                           dest_addr dest_port  positional arguments:   dest_addr             destination hostname, IP address or multiple IPs                         separated by coma (e.g. '1.1.1.1,2.2.2.2') or given by                         CIDR netmask (e.g. '10.0.0.0/22') or both    dest_port             destination port or multiple ports given by list                         separated by coma (e.g. '8080,9090') or port range                         (e.g. '1000-2000') or both  optional arguments:   -h, --help            show this help message and exit   --retries RETRIES, -R RETRIES                         number of retries   --timeout TIMEOUT, -T TIMEOUT                         timeout in seconds   --verbose, -V, --debug, -D                         Turn on verbose/debug mode (more messages)   --protocol {CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}, -P {CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}                         protocol to be tested   --hide-disclaimer, -HD                         hides legal disclaimer (shown before starting                         intrusive tools)   --src-ip SRC_IP, -SI SRC_IP                         source IP address (return result will not be                         received!)   --src-port SRC_PORT, -SP SRC_PORT                         source port (if not specified random port will be                         used)   --ignore-ping-check, -Pn                         ignore ping check (treat all ports as alive)   --corpus-dir CORPUS_DIR, -C CORPUS_DIR                         path to directory with fuzzing payloads (corpus) (each                         payload in separated file)   --delay-after-crash DELAY_AFTER_CRASH, -DAC DELAY_AFTER_CRASH                         number of seconds that fuzzer will wait after crash                         for respawning tested server   cotopaxi.client_proto_fuzzer Black-box fuzzer for testing protocol clients usage: sudo python -m cotopaxi.client_proto_fuzzer [-h] [--server-ip SERVER_IP]                               [--server-port SERVER_PORT] [--verbose]                               [--protocol {CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}]                               [--corpus-dir CORPUS_DIR]  optional arguments:   -h, --help            show this help message and exit   --server-ip SERVER_IP, -SI SERVER_IP                         IP address, that will be used to set up tester server   --server-port SERVER_PORT, -SP SERVER_PORT                         port that will be used to set up server   --verbose, -V, --debug, -D                         Turn on verbose/debug mode (more messages)   --protocol {CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}, -P {CoAP,DTLS,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}                         protocol to be tested   --corpus-dir CORPUS_DIR, -C CORPUS_DIR                         path to directory with fuzzing payloads (corpus) (each                         payload in separated file)   cotopaxi.vulnerability_tester Tool for checking vulnerability of network endpoints at given IP and port ranges usage: sudo python -m cotopaxi.vulnerability_tester -h usage: vulnerability_tester.py [-h] [--retries RETRIES] [--timeout TIMEOUT]                                [--verbose]                                [--protocol {ALL,UDP,TCP,CoAP,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}]                                [--hide-disclaimer] [--src-ip SRC_IP]                                [--src-port SRC_PORT] [--ignore-ping-check]                                [--vuln {ALL,BEWARD_000,BOTAN_000,...} ...]]                                [--cve {ALL,CVE-2014-4878,CVE-2014-4879,...} ...]]                                [--list]                                dest_addr dest_port  positional arguments:   dest_addr             destination hostname, IP address or multiple IPs                         separated by coma (e.g. '1.1.1.1,2.2.2.2') or given by                         CIDR netmask (e.g. '10.0.0.0/22') or both    dest_port             destination port or multiple ports given by list                         separated by coma (e.g. '8080,9090') or port range                         (e.g. '1000-2000') or both  optional arguments:   -h, --help            show this help message and exit   --retries RETRIES, -R RETRIES                         number of retries   --timeout TIMEOUT, -T TIMEOUT                         timeout in seconds   --verbose, -V, --debug, -D                         Turn on verbose/debug mode (more messages)   --protocol {ALL,UDP,TCP,CoAP,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}, -P {ALL,UDP,TCP,CoAP,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}                         protocol to be tested (UDP includes all UDP-based                         protocols, while TCP includes all TCP-based protocols,                         ALL includes all supported protocols)   --hide-disclaimer, -HD                         hides legal disclaimer (shown before starting                         intrusive tools)   --src-ip SRC_IP, -SI SRC_IP                         source IP address (return result will not be                         received!)   --src-port SRC_PORT, -SP SRC_PORT                         source port (if not specified random port will be                         used)   --ignore-ping-check, -Pn                         ignore ping check (treat all ports as alive)   --vuln {ALL,BEWARD_000,BOTAN_000,...} ...]                         list of vulnerabilities to be tested (by SOFT_NUM id)   --cve {ALL,CVE-2014-4878,CVE-2014-4879,...} ...]                         list of vulnerabilities to be tested (by CVE id)   --list, -L            display lists of all vulnerabilities supported by this                         tool with detailed description    cotopaxi.client_vuln_tester Tool for checking vulnerability of network clients connecting to server provided by this tool usage: sudo python -m cotopaxi.client_vuln_tester [-h] [--server-ip SERVER_IP]                              [--server-port SERVER_PORT] [--verbose]                              [--protocol {CoAP,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}]                              [--vuln {ALL,BEWARD_000,BOTAN_000,...} ...]]                              [--cve {ALL,CVE-2014-4878,CVE-2014-4879,...} ...]]                              [--list]  optional arguments:   -h, --help            show this help message and exit   --server-ip SERVER_IP, -SI SERVER_IP                         IP address, that will be used to set up tester server   --server-port SERVER_PORT, -SP SERVER_PORT                         port that will be used to set up server   --verbose, -V, --debug, -D                         Turn on verbose/debug mode (more messages)   --protocol {CoAP,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}, -P {CoAP,HTCPCP,HTTP,mDNS,MQTT,QUIC,RTSP,SSDP}                         protocol to be tested   --vuln {ALL,BEWARD_000,BOTAN_000,...} ...]                         list of vulnerabilities to be tested (by SOFT_NUM id)   --cve {ALL,CVE-2014-4878,CVE-2014-4879,...} ...]                         list of vulnerabilities to be tested (by CVE id)   --list, -L            display lists of all vulnerabilities supported by this                         tool with detailed description    cotopaxi.amplifier_detector Tool for detection of network devices amplifying reflected traffic by observing size of incoming and outgoing size of packets usage: sudo python -m cotopaxi.amplifier_detector [-h] [--port PORT] [--nr NR] [--verbose] dest_addr  positional arguments:   dest_addr               destination IP address  optional arguments:   -h, --help            show this help message and exit   --interval INTERVAL, -I INTERVAL                         minimal interval in sec between displayed status                         messages (default: 1 sec)   --port PORT, --dest_port PORT, -P PORT                         destination port   --nr NR, -N NR        number of packets to be sniffed (default: 9999999)   --verbose, -V, --debug, -D                         turn on verbose/debug mode (more messages)    cotopaxi.active_scanner Tool for checking security properties of network endpoints at given IP and port ranges usage: sudo python -m cotopaxi.active_scanner [-h] [--retries RETRIES] [--timeout TIMEOUT]                          [--verbose] [--protocol {DTLS}] [--src-ip SRC_IP]                          [--src-port SRC_PORT] [--ignore-ping-check]                          dest_addr dest_port  positional arguments:   dest_addr             destination hostname, IP address or multiple IPs                         separated by coma (e.g. '1.1.1.1,2.2.2.2') or given by                         CIDR netmask (e.g. '10.0.0.0/22') or both    dest_port             destination port or multiple ports given by list                         separated by coma (e.g. '8080,9090') or port range                         (e.g. '1000-2000') or both  optional arguments:   -h, --help            show this help message and exit   --retries RETRIES, -R RETRIES                         number of retries   --timeout TIMEOUT, -T TIMEOUT                         timeout in seconds   --verbose, -V, --debug, -D                         Turn on verbose/debug mode (more messages)   --protocol {DTLS}, -P {DTLS}                         protocol to be tested   --src-ip SRC_IP, -SI SRC_IP                         source IP address (return result will not be                         received!)   --src-port SRC_PORT, -SP SRC_PORT                         source port (if not specified random port will be                         used)   --ignore-ping-check, -Pn                         ignore ping check (treat all ports as alive)    Known issues / limitations There are some known issues or limitations caused by using scapy as network library:  testing services running on the same machine can results in issues occurred by not delivering some packets, multiple tools running against the same target can result in interference between them (packets may be indicated as a response to another request).  See more at: https://scapy.readthedocs.io/en/latest/troubleshooting.html# Source code quality Before uploading your contribution using github Pull Request please check your code using listed below tools: black -t py27 cotopaxi black -t py27 tests  pydocstyle cotopaxi  python -m pylint cotopaxi --rcfile=.pylintrc python -m pylint tests --rcfile=tests/.pylintrc  bandit -r cotopaxi  Unit tests To run all unit tests using unittest use (from upper cotopaxi dir):     sudo python -m unittest discover -v  To run all unit tests using unittest with coverage analysis run (from upper cotopaxi dir):     sudo coverage run --source cotopaxi -m unittest discover     coverage html     firefox htmlcov/index.html  To run all unit tests using pytest with coverage analysis and branch analysis run (from upper cotopaxi dir):     sudo python2.7 -m coverage run --source cotopaxi --branch -m pytest -v     sudo python2.7 -m coverage html     firefox htmlcov/index.html      sudo python3.6 -m coverage run --source cotopaxi --branch -m pytest -v     sudo python3.6 -m coverage html     firefox htmlcov/index.html  To run unit tests for one of tools run (from upper cotopaxi dir):     sudo python -m tests.test_active_scanner     sudo python -m tests.test_amplifier_detector     sudo python -m tests.test_client_proto_fuzzer     python -m tests.test_device_identification     python -m tests.test_traffic_analyzer     sudo python -m tests.test_protocol_fuzzer     sudo python -m tests.test_resource_listing     sudo python -m tests.test_server_fingerprinter     sudo python -m tests.test_service_ping     sudo python -m tests.test_vulnerability_tester  Most of tests are performed against remote tests servers and require preparing test environment, providing settings in tests/test_config.ini and tests/test_servers.yaml.","nsteon hub lefun cam lg smart tv luohe cam magichome strip microseven camera nest thermostat netatmo weather station osram lightify hub philips hue (lightbulb) philips hue hub ring doorbell roku tv samsung fridge samsung dryer samsung smartthings hub samsung smarttv samsung washer sengled smart hub smarter brewer smarter coffee machine smarter ikettle tp-link bulb tp-link smart plug wansview camera wemo plug wimaker charger camera wink hub 2 xiaomi mi cam 2 xiaomi mi robot cleaner xiaomi mi hub xiaomi mi rice cooker xiaomi mi power strip yi camera zmodo greet (doorbell)  usage: python -m cotopaxi.device_identification usage: [-h] [--verbose] [--min min] [--max max]                                 [--ip ip] [-s]                                 pcap  tool for classifying iot devices based on captured network traffic  positional arguments:   pcap                  packet capture file (in pcap or pcapng format) with                         recorded traffic for device identification  optional arguments:   -h, --help            show this help message and exit   --verbose, -v, --debug, -d                         turn on verbose/debug mode (more messages)   --min min             minimum number of packets to classify device (devices                         with smaller number will not be classified) (default: 3)   --max max             maximum number of packets used to classify device                         (default: 1000)   --ip ip, -i ip        use ip filter to identify device   -s, --short           display only short result of classification     cotopaxi.traffic_analyzer tool for passive identification of network protocol using captured network traffic currently supported protocols:  amqp bgp cmp coap dhcp dlna dns dtls eigrp ftp gnutella gre h323 hsrp http htcpcp igmp ipp ipsec irc llmnr mdns mqtt mqtt-sn mstp ntlm ntp ocsp ospf quic radius rip rpc rtsp sip smb smtp snmp ssdp ssh tacacs telnet tftp tls vrrp  usage: python -m cotopaxi.traffic analyzer usage: [-h] [--verbose] [--min min] [--max max]                                 [--ip ip] [-s]                                 pcap  tool for classifying network protocols used in traffic flows  positional arguments:   pcap                  packet capture file (in pcap or pcapng format) with                         recorded traffic for network protocols identification  optional arguments:   -h, --help            show this help message and exit   --verbose, -v, --debug, -d                         turn on verbose/debug mode (more messages)   --min min             minimum number of packets to classify                         conversation(conversations with smaller number will                         not be classified) (default: 3)   --max max             maximum number of packets used to classify                         conversation (default: 1000)   --ip ip, -i ip        use ip filter to identify protocol   -s, --short           display only short result of classification     cotopaxi.resource_listing tool for checking availability of resource named url on server at given ip and port ranges. sample url lists are available in the urls directory usage: sudo python -m cotopaxi.resource_listing [-h] [--retries retries] [--timeout timeout]                            [--verbose] [--protocol {coap,http,mdns,rtsp,ssdp}]                            [--src-ip src_ip] [--src-port src_port]                            [--ignore-ping-check]                            [--method {get,post,put,delete,all} [{get,post,put,delete,all} ...]]                            dest_addr dest_port names_filepath  positional arguments:   dest_addr             destination hostname, ip address or multiple ips                         separated by coma (e.g. '1.1.1.1,2.2.2.2') or given by                         cidr netmask (e.g. '10.0.0.0/22') or both    dest_port             destination port or multiple ports given by list                         separated by coma (e.g. '8080,9090') or port range                         (e.g. '1000-2000') or both   names_filepath        path to file with list of names (urls for coap or                         services for mdns) to be tested (each name in                         separated line)  optional arguments:   -h, --help            show this help message and exit   --retries retries, -r retries                         number of retries   --timeout timeout, -t timeout                         timeout in seconds   --verbose, -v, --debug, -d                         turn on verbose/debug mode (more messages)   --protocol {coap,http,mdns,rtsp,ssdp}, -p {coap,http,mdns,rtsp,ssdp}                         protocol to be tested   --src-ip src_ip, -si src_ip                         source ip address (return result will not be                         received!)   --src-port src_port, -sp src_port                         source port (if not specified random port will be                         used)   --ignore-ping-check, -pn                         ignore ping check (treat all ports as alive)   --method {get,post,put,delete,all} [{get,post,put,delete,all} ...], -m {get,post,put,delete,all} [{get,post,put,delete,all} ...]                         methods to be tested (all includes all supported                         methods)    cotopaxi.protocol_fuzzer black-box fuzzer for testing protocol servers usage: sudo python -m cotopaxi.protocol_fuzzer [-h] [--retries retries] [--timeout timeout]                           [--verbose]                           [--protocol {coap,dtls,htcpcp,http,mdns,mqtt,quic,rtsp,ssdp}]                           [--hide-disclaimer] [--src-ip src_ip]                           [--src-port src_port] [--ignore-ping-check]                           [--corpus-dir corpus_dir]                           [--delay-after-crash delay_after_crash]                           dest_addr dest_port  positional arguments:   dest_addr             destination hostname, ip address or multiple ips                         separated by coma (e.g. '1.1.1.1,2.2.2.2') or given by                         cidr netmask (e.g. '10.0.0.0/22') or both    dest_port             destination port or multiple ports given by list                         separated by coma (e.g. '8080,9090') or port range                         (e.g. '1000-2000') or both  optional arguments:   -h, --help            show this help message and exit   --retries retries, -r retries                         number of retries   --timeout timeout, -t timeout                         timeout in seconds   --verbose, -v, --debug, -d                         turn on verbose/debug mode (more messages)   --protocol {coap,dtls,htcpcp,http,mdns,mqtt,quic,rtsp,ssdp}, -p {coap,dtls,htcpcp,http,mdns,mqtt,quic,rtsp,ssdp}                         protocol to be tested   --hide-disclaimer, -hd                         hides legal disclaimer (shown before starting                         intrusive tools)   --src-ip src_ip, -si src_ip                         source ip address (return result will not be                         received!)   --src-port src_port, -sp src_port                         source port (if not specified random port will be                         used)   --ignore-ping-check, -pn                         ignore ping check (treat all ports as alive)   --corpus-dir corpus_dir, -c corpus_dir                         path to directory with fuzzing payloads (corpus) (each                         payload in separated file)   --delay-after-crash delay_after_crash, -dac delay_after_crash                         number of seconds that fuzzer will wait after crash                         for respawning tested server   cotopaxi.client_proto_fuzzer black-box fuzzer for testing protocol clients usage: sudo python -m cotopaxi.client_proto_fuzzer [-h] [--server-ip server_ip]                               [--server-port server_port] [--verbose]                               [--protocol {coap,dtls,htcpcp,http,mdns,mqtt,quic,rtsp,ssdp}]                               [--corpus-dir corpus_dir]  optional arguments:   -h, --help            show this help message and exit   --server-ip server_ip, -si server_ip                         ip address, that will be used to set up tester server   --server-port server_port, -sp server_port                         port that will be used to set up server   --verbose, -v, --debug, -d                         turn on ve robottrip microseven camera nest thermostat netatmo weather station osram lightify hub philips hue (light",6,217,79,76,1,468,1
NVIDIA-AI-IOT/jetbot,https://github.com/NVIDIA-AI-IOT/jetbot,873,2019-03-09T05:39:39Z,2019-03-09,05:39:39Z,2019,2021-01-12T16:40:22Z,2021-01-12,16:40:22Z,2021,An educational AI robot based on NVIDIA Jetson Nano.,"JetBot  Looking for a quick way to get started with JetBot?  Many third party kits are now available!   JetBot is an open-source robot based on NVIDIA Jetson Nano that is  Affordable - Less than $150 add-on to Jetson Nano Educational - Tutorials from basic motion to AI based collision avoidance Fun! - Interactively programmed from your web browser  Building and using JetBot gives the hands on experience needed to create entirely new AI projects. To get started, read the JetBot documentation. Get involved We really appreciate any feedback related to JetBot, and also just enjoy seeing what you're working on!  There is a growing community of Jetson Nano and JetBot users.  It's easy to get involved involved...  Ask a question and discuss JetBot related topics on the JetBot GitHub Discussions Report a bug by creating an issue Share your project or ask a question on the Jetson Developer Forums",ty kits are now available!   jetbot is an open-source robot based on nvidia jetson nano that is  affordable - less than $150 robotrobot based on nvidia jetson nano that is  affordable - less than $150 add-on to jetson nano educati,7,2200,375,0,1,273,1
NVIDIA-AI-IOT/Gesture-Recognition,https://github.com/NVIDIA-AI-IOT/Gesture-Recognition,37,2018-08-02T23:31:46Z,2018-08-02,23:31:46Z,2018,2021-01-08T06:49:30Z,2021-01-08,06:49:30Z,2021,Gesture recognition neural network to classify various hand gestures,"Wave! by Neural Ninja    Wave! is a gesture recognition neural network that uses tf-pose-estimation, a tensorflow wrapper of a pose estimation neural network (OpenPose), to classify various hand gestures in images. Wave! runs on top of OpenPose, which identifies various joints of humans in a given camera frame, such as their elbow or nose. In most cases, it returns the x and y positions of each bodypart in terms of percentage of the frame, a confidence score for each of these bodyparts, and a confidence score for each human it identifies. During Data Collection, bodypart movement of the human with the highest confidence score is tracked and summed over a series of frames. This data, along with the confidence scores for each bodypart, is passed on to Wave! for classification. The most successful classification model at the moment is the Post OpenPose Neural Network (POPNN) in TensorFlow. This model is a fully connected feed forward binary classification model. It classifies images as either a ""wave"" or ""no wave"". We also worked on PyTorch LSTM and POPNN models that are more modular than POPNN in TensorFlow and support multiclass classification. We are adding this capability to the TensorFlow models as well. Application This project was initially used with a humanoid robot to create an interactive user experience. Wave! does not require to be paired with a robot. In this interactive display, the robot waved if a human in its field of view waved at it. Data from each network, Wave! and tf-pose-estimation, was sent using Robot Operating System (ROS). Inference is done primarily on the Nvidia Jetson TX2, but can also be run on a PC if needed. Wave! can run on any robot that support ROS.  Note: Wave! is only compatible with Python 2, due to ROS requirements and syntax.  Setup Unless specified, perform the following setup commands on your Jetson TX2 and PC (used for training). Follow the instructions here to setup Ubuntu and flash additional software on your TX2. Clone the Wave! repository: $ git clone --recursive https://github.com/NVIDIA-Jetson/Gesture-Recognition.git  Install program dependencies:  This installs argparse, dill, matplotlib, psutil, requests, scikit-image, scipy, slidingwindow, tqdm, setuptools, Cython, pyTorch, and sklearn.  $ sudo pip install -r requirements.txt  Install OpenCV: $ sudo apt-get install python-opencv  Clone the tf-pose-estimation repository: $ git clone --recursive https://github.com/ildoonet/tf-pose-estimation.git  Configure tf-pose-estimation for our purposes:  This replaces tf-openpose's estimator.py with a custom version for Wave!, and then installs tf-openpose.  $ cp -f wavenet/estimator.py tf-openpose/tf_pose/estimator.py $ cd tf-openpose $ python setup.py install If you get an error along the lines of ""can't find Arrow package"" when installing tensorpack in the setup.py install, particularly when installing tf-pose on a Jetson our suggested fix is to delete the install of tensorpack (Take out any references to tensorpack or the github link for tensorpack in the setup.py) and then manually install tensorflow with TRT support on your Jetson with this link. After downloading the pip wheel, install it as follows: $ sudo pip install tensorflow-1.7.0-cp27-cp27mu-linux_aarch64.whl  Install rospy on the Jetson TX2:  Make sure you restart your terminal after setting up rospy (on both the Jetson TX2 and PC) before running anything with rospy to ensure everything runs smoothly.  $ git clone --recursive https://github.com/jetsonhacks/installROSTX2 $ cd installROSTX2 $ ./installROS.sh -p ros-kinetic-desktop-full $ ./setupCatkinWorkspace.sh Install rospy on your PC by following the instructions here Install pyTorch here, and follow the instructions on site to install the right version depending on your version of CUDA. Project Pipeline    Network Pipeline Using generate_video.py, we generate videos of people performing the activities of different data classes (""waving"", ""X-Posing"", etc.). These videos are given to generate_data_from_video.py, which runs inference on tf-openpose and extracts data for the networks. You can train a network by either running lstm.py, popnn_torch.py, or popnn4.py, with their corresponding arguments. Finally, you can run inference on this model by running either lstm_multi_inference.sh, popnn_torch_inference.sh, or popnn_inference.sh. These bash scripts run thread_inf.py, which creates multiple threads for collecting data for inference, and publishes them for inference, and either lstm_inference.py ,popnn_torch_inference.py, or popnn4inference.py, which run inference on the models. Multi-Threaded Inference As mentioned in Project Pipeline, thread_inf.py, the program that runs inference on tf-openpose to collect real-time data to feed to the network, is multi-threaded to increase the FPS of inference. The main thread starts 3 different threads which capture webcam frames, run pose estimation on these frames, and display the frame with pose estimation drawn on it. The main thread then publishes the pose estimation data on ROS for the main inference script. This functionality is also available on the TensorFlow Models. LSTM The Wave! LSTM was made to take advantage of the sequential nature of camera feed data. One of the newer features of the PyTorch models (not in the TensorFlow models) is their adaptability and multiclass ability. Thanks to data collection and loading that can accomadate as many data features as needed (default: position, score, movement vector components). If you want to add your own data features or modify the PyTorch models or inference, check out the PyTorch Modification portion of this README. The parameter centralization class, var, in var.py, allows for tinkering of model hyperparameters, such as number of classes, over all scripts that use them. As a result, it is easy to rapidly train and deploy Wave! PyTorch nets. At the moment, both of the PyTorch Models are not as accurate as POPNN (Post OpenPose Neural Network) in TensorFlow (by a small margin). As mentioned before, feel free to run, modify, or reference them. LSTM Network Architecture The LSTM is comprised of a two layer, stacked LSTM, followed by 3 linear layers. After each of the linear layers is a dropout layer, during training, and an activation function, RELU after the first two layers, and a log sigmoid after the final layer to get the output between 0 and 1.     Note: Due to the increased complexity of the model, the LSTM runs slower than both POPNNs.  POPNN The Wave! Post OpenPose Neural Network (POPNN) is the original, and most accurate model (the TensorFlow version). At the moment, it is currently integrated with the var class, but does not allow for multiclass classification or more than one feature. The model itself trains much faster than the LSTM, with 1200 epochs finishing in around five minutes. As mentioned before, feel free to run, modify, or reference this code, and contribute back if you would like. POPNN Network Architecture Just like the TensorFlow version, the PyTorch POPNN consists of 4 linear layers. After the first three of these layers is a dropout layer and a RELU, and the final linear layer has a softmax after it to squeeze the output between 0 and 1. It also has dropout layers during training to prevent overfitting.    Var.py var.py is the parameter hub of the repository. It contains the hyperparameters of the models and other often referenced by many other scripts, such as the classification classes and number of data features. var.py offers control over the more nitpicky hyperparameters of the models that their argparse flags do not, like hidden layer sizes, while allowing you to do so in only one place. Because so many file reference var.py, it is imperative that any modifications made be reflected in var.py (This mainly applies to data collection programs). For example, if you add a new feature to data collection that you want to train on, you must change the features variable in var.py accordingly. Usage var.py is a class that when initialized in a program, contains data relating to many different paraments. To import and create an instance of the var class (if you need to): from var import var v = var(use_arm)  use_arm in this case is a boolean which if True, means that only 8 joints (out of 18 total) are being used to collect data on or train on (arm joints, nose). Var comes with several helpful methods to extract parameters from itself. input_size = v.get_size(): # Get the input size of data/Number of joints being saved lstm_vars  = v.get_LSTM(): # Get full list of LSTM variables''' popnn_vars  = v.get_POPNN() # Get full list of POPNN variables classes  = v.get_classes() # Get classification classes num_features  = v.get_num_features(): '''Get number of features of data collection. For example, using x position, y position, and score would be 3 features.''' num_classes = v.get_num_classes(): # Get number of classes  Data Collection Wave! uses the difference in position of joints, as well as their confidence scores, to classify an image. During data collection, Wave! runs inference on OpenPose, and saves the positions of bodyparts from each iteration. The differences between bodypart positions over 4 (this number can be changed in the data collection phase) consecutive frames is summed and saved as an .npz, along with the averages of bodypart scores over the same frames. Every datapoint is saved in terms of percentage of frame in the GestureData directory. Our data collection process consists of two steps : video generation, and data generation. Because the videos are generated and saved separately, you can reuse videos to extract different features from them with different argparse flags as shown below. Collecting Data via Video  Collects Data and stores it in the .avi format. Data can be used to create data for Wave!  To collect video data, run generate_video.py: $ python generateVideo.py # --camera=<port number> --time=<recording seconds> --dir=<datafile> # Uncomment the flags above to either manually set camera number (default=1), change video length or change save directory  When prompted, type in the data class (""wave"", ""no wave"", etc.) that you want to collect data for. To generate training data from these videos, run generate_data_from_video.py: $ python generateDataFromVideo.py #--exit_fps=<fps> -f=<numFrames> -a -o -s=<startVideoNum> -e=<endVideoNum>   Uncomment  exit_fps to set the desired fPS of the data collected (inference on Jetson can be slower than inference on a PC). The default is 5. Uncomment -f to set the the number of frames that movement is aggregated over. The default is 4. Add the -a flag to collect data on net distance and angles (vectors) instead of change in x and y. When running this file, the default data that is generated is for every keypoint on every person's body. However, when recognizing waves, the network only cares about the shoulders, elbows, wrists, and nose. So, to only generate data for these seven keypoints, add the -o flag. By default, generate_data_from_video.py generates data from all files in your data directory. To only generate data from a couple of videos in the data directory, specify a video number to start with using the -s flag and a video number to end on with the -e flag. Smart Labeler Just like POPNN, there is a smart labeler, which can detect edge cases that fail and create valid data that you can transfer learn off of. To collect data with the smart labeler, run this command: $ bash smartLabeler.sh # --camera=<port number> --debug --wave # Uncomment the flags above to either manually set camera number (default=1) or say that the user is waving (no wave without flag). # If using all flags, write in the above order  If you are generating videos on multiple computers and are compiling all video data on git, you may need to rename your label and data files to avoid merge conflicts or overwritten data. For your convenience, we have created renamer.py, which you can use to change the numbering of files in the data/video/videos and data/video/labels folders. So, you will be able to merge data from different machines seamlessly. To rename the video files, run renamer.py: $ python renamer.py -b=<buffer>  The -b flag should be an integer by which the program increments the filenames. Training Before training, ensure all your data is in the appropriate folders. The best results are usually obtained when the amount of ""no wave"" data is close the amount of ""wave"" data. To check how much of each type of data you have accumulated, run data_count.py as follows: $ python data_count.py -f=<numFrames>   Set the -f flag to the number of frames specified when creating data from videos. LSTM Run lstm.py in order to start training the LSTM (run popnn.py with the same arguments to train a POPNN model). You can adjust the learning rate, hidden layer size, and more to experiment with different net structures. $ python lstm.py # -s=<save_fn> -t -c=<ckpt_fn> -lr=<learning_rate> -d=<learning_rate_decay> -e=<numEpochs> -f=<numFrames> -b=<batch_size> -o  To keep more than one checkpoint file, specify a filename to save the generated checkpoint file to save to. The default is lstm000.ckpt, and is saved in the lstmpts directory. If left unchanged, newly generated checkpoint files will overwrite one another. Checkpoint files are given the .ckpt filename extension in this project. The LSTM gives you control over the network's finer details, including learning rate, number of epochs, epoch batch size, and the rate of decay of the learning rate (if not specified, learning rate will stay constant). If you would like to transfer learn from one model to the next, uncomment -t in the command above. Specify a checkpoint filename after the -c flag to transfer learn from. The default is lstm307.ckpt, our best performing model. Also, if you only want to train on the data from the arm keypoints, uncomment the -o flag. To save a model while it's training, press Ctrl + C. POPNN (TensorFlow) Before training, ensure all your data is in the appropriate folders. To train the network, run popnn4.py: $ python popnn4.py --dest_ckpt_name=<saveFileName> # -t -f=<numFrames> --debug --bad_data  The --dest_ckpt_name or -d flag is required, and should be set the the filename the generated checkpoint file will be saved to. Checkpoint files are given the .ckpt filename extension in this project. The program will print the loss after every epoch, as well as the training accuracy and validation accuracy. For transfer learning, add the commented out -t tag, and the program will prompt you for a file to transfer learn off of. This loads the previously trained weights from the pretrained checkpoint file, and trains with them. Uncomment the --debug flag for some helpful print statements for debugging purposes. Finally, uncomment the --bad_data or -bd flag to only use data in which humans in frame are not flickering in and out of frame. Note: POPNN is tensorboard compatible. We have currently implemented some basic tensorboard summaries so you can visualize your graphs as specified in the TensorBoard API. POPNN (PyTorch) Graphs After training a model, a graph of the checkpoint file will be generated (for PyTorch models), showing the loss, training accuracy, and validation accuracy for every 25 epochs. This can help you visualize how good your model is. Below, you can see an example of a generated graph from model 307. Inference LSTM To run inference on the LSTM, run the lstm_multi_inference.sh with the following command: $ bash lstmMultiInference.sh  # <port number> <numFrames>-c=<checkpoint file number> -o -a  Like the other inferences, uncomment the flags to specify a camera port number, or debug the code. Keep the flags in the same order. Even if you are not using them all, you must type out all flags before the ones you want to use (i.e if you want to specify a checkpoint file, you must specify a camera port number and the number of frames to aggregate over). Change the -c flag to the name of the new checkpoint file you would like to run inference on. If you only want to run inference on the seven keypoints necessary to recognize a wave, simply uncomment the -o flag. POPNN (TensorFlow) Wave! can run inference on multiple people in a frame. To run inference, run the following command: $ bash popnn_inference.sh  popnn_inference.sh launches the roscore, runs inference on Wave!, and sends data through ROS publishers. However, the data collection process in popnn_inference.sh is modified to return data of every human tf-pose-estimation detects in each frame. If the number of humans changes, the data collector looks at a new batch of 4 images, because we do not know who left or entered the image. To specify which camera port is being used, add the --camera=<port number> flag. To debug, add the --debug flag after the --camera flag. POPNN (PyTorch) Graphs After training a model, a graph of the checkpoint file will be generated (for PyTorch models), showing the loss, training accuracy, and validation accuracy for every 25 epochs. This can help you visualize how good your model is. Below, you can see an example of a generated graph from model 307.    Other Notes For all the networks we have made, there have been a few naming conventions we have used within our program, some root files we have modified, and other short things to note for the user. PyTorch Modification As mentioned earlier, the PyTorch models, data collection methods, and inference code was made to make modification as easy as possible. A lot of this is thanks to the var class, which allows for all programs to refer to a single list of important parameters. Data Collection The primary data creation program is generate_data_from_video.py, which extracts certain data features, such as x and y position, score, and distance traveled. If you want to extract other features, or extract less features, modify the code loop to accumulate data in a numpy array, and add it to the list, features in the following line.     features = [xDifferences, yDifferences]  By default, the scoreAvgs variable is added to the end of features to allow for the mulitiplication of score and other features. Additionally, change the num_features variable in var.py to reflect the number of features you are using. After training the network, you must modify the post_n_pub method in the PosePub class to collect the same data during inference and publish it, as shown here. You can also follow the usage of publishers in our code. In order to do so, you must : create a rospy publisher, accumuluate data in a numpy array, and publish the data (most likely as a string). To collect data for different classes, add them to the classes dictionary in var.py in the following format. classes = {previous classes, number : ""class name""}  When generating video using generate_video.py, you will be prompted for what data class you are recording data for. Inference If you add more features in the data collection phase, train the network on these features, and send these features through post_n_pub, you must change the inference scripts, lstm_inference.py and popnn_torch_inference.py to accomadate the new data. First, you must create a rospy subscriber to listen to the features from post_n_pub, as shown here. You can also follow the usage of subscribers in our code. Data Collection Naming Conventions To avoid confusion between the two POPNN models, the popnn models in PyTorch, as well as their accompanying files, have _torch in their name. For all video data collected we use the form #.avi, where # is the number of the latest file stored. Thus, we encourage you to keeping using that so that generate_data_from_video.py does not incure bugs. For all processed data in npz format, we use the format gestureData#.npz, where # is the number of the latest file stored. Again, for the sake of bugs, we encourage you to keep using that in the event you inject your own custom npz files. The standard dataloader.py should take care of this kind of data well. All labels are in the format #.txt, similar to the video data, for the same reasons. Transfer Learning and Model Saving Conventions After training a model, they will save a checkpoint file which stores the previous weights of the model and can be used to inference on the previous model and to transfer learn for new models. The naming convention for our checkpoint files is in the format fooxyz.ckpt, where foo is the name of the network type, either lstm or popnn depending on your network, x is for the network architecture (e.g. currently 4 for the best stable version of popnn), y is for the type of data collected (e.g. currently 3 for our 3rd major data change in version 4) and can also represent image frames per data instance (e.g. 7 for 7 frames per data instance and 10 for 10 frames per data instance), and z is for the latest training iteration. .ckpt is the suffix used fo rthe checkpoint files. Some of our most stable popnn versions include 4.2.7, 4.3.4, and 4.3.11. There tend to be gaps between many versions due to many bad training iterations, and/or versions we felt were inferior to the other versions. The most stable lstm version is 3.0.7. General File and Code Naming Conventions When naming python files, we use underscores to separate words, rather than using camel case. This helps with file readibilty. When adding detailed comments to the code, the comments are in the format ''' comment ''', instead of using the format #comment. Modifications to TF-Pose If the tf_pose library has been giving you issues, make sure you refer to our setup portion up above. tf_pose is fantastic for tensorflow pose-estimation (the alternatives are written in the less general user-friendly caffe) and thus can be easily ported to other tensorflow networks or models. If you are modifying code in TF-Pose, you may need to see which numbers correspond to which joints in numJoints[]. Here is a list of numbers and their corresponding joints:    Joint Number Joint Name     0 Nose   1 Neck   2 Right Shoulder   3 Right Elbow   4 Right Wrist   5 Left Shoulder   6 Left Elbow   7 Left Wrist   8 Right Hip   9 Right Knee   10 Right Ankle   11 Left Hip   12 Left Knee   13 Left Ankle   14 Right Eye   15 Left Eye   16 Right Ear   17 Left Ear    GPU Configuration tf_pose takes up a lot of compute, and sometimes can eat of your memory if you're not careful with limiting how much memory it can take per operation. If you are getting GPU sync errors or fatal errors (NOT warnings) to do with RAM or GPU RAM allocation, you can limit the memory used by tf_pose per process for inference in the wave_inference file as follows: Line 93: s_tf_config = tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=n))  Where n is any fraction between 0 and 1, set to 0.2 by default. Reduce this if you run into RAM allocation problems X-Pose, Y-Pose, and Dab Recognition In addition to detecting ""waves"" and ""no waves,"" we have built in recognition for other gestures - x-poses, y-poses, and dabs. To make an x-pose, cross your arms in front of your body to make an ""x"" shape. To make a y-pose, put your arms above your head in a ""y"" shape. In order to recognize these poses, we took two different approaches-one where we hard coded recognition, and another where we trained a neural network to recognize the poses. Hard Coded Recognition When running lstm_wave_inference, we print whether someone in the frame is doing one of these gestures. Neural Net Recognition Bad Data Nuances When looking through our code you may see numerous references to 'bad data', and as could probably be inferred, bad data refers to data mistakes that TF_Pose produces when doing the preprocessing for our network. While, of course, not all data mistakes can be accounted for, there are a few big ones we can account for, that do in fact affect our dataset and our inference. Up and Down Arms The first major data deficiency we need to account for is that when arms are down in a frame, we multiple the joint scores by -1 as a preprocess to our network. However, when lifting that arm to the point where its positive, this results in a large perceived shift in distance by our network, even when there is none. In fact, if you look through our code in thread_inf.py, you will see that we've accounted for this by labeling changes from positive to negative arm position and vice versa so that the network doesn't receive that bad data (we send a null array instead using pub_null()). This is something we have not left as an option for the user to change simply with a flag because it is a very big deficiency. If you are indeed curious, however, please do check it out in the code! Flashing Arms This deficiency, while minor, occurs when TF_Pose can't properly identify arms from frame to frame, so the arms are sometimes there and sometiems nonexistent. As a result, we've added a flag which can be toggled to judge whether or not the inference should account for that and send a null array when it happens. The upside of enabling the flag is that you will have less messy data, but the downside is that your inference can take a bit longer if the person isn't sitting right in the frame for a while, and the network sensitivity can decrease as well. License MIT, see LICENSE Authors Wave! was made Maddie Waldie, Nikhil Suresh, Jackson Moffet, and Abhinav Ayalur, four NVIDIA High School Interns, during the summer of 2018.","plication this project was initially used with a humanoid robot to create an interactive user experience. wave! does not require to be paired with a robot. in this interactive display, the robot waved if a human in its field of view waved at it. data from each network, wave! and tf-pose-estimation, was sent using robot operating system (ros). inference is done primarily on the nvidia jetson tx2, but can also be run on a pc if needed. wave! can run on any robot that support ros.  note: wave! is only compatible with python 2, due to ros requirements and syntax.  setup unless specified, perform the following setup commands on your jetson tx2 and pc (used for training). follow the instructions here to setup ubuntu and flash additional software on your tx2. clone the wave! repository: $ git clone --recursive https://github.com/nvidia-jetson/gesture-recognition.git  install program dependencies:  this installs argparse, dill, matplotlib, psutil, requests, scikit-image, scipy, slidingwindow, tqdm, setuptools, cython, pytorch, and sklearn.  $ sudo pip install -r requirements.txt  install opencv: $ sudo apt-get install python-opencv  clone the tf-pose-estimation repository: $ git clone --recursive https://github.com/ildoonet/tf-pose-estimation.git  configure tf-pose robotoid robot to create an interactive user experience. wave! does not require to be paired with a robot",4,101,1,0,1,0,1
NVIDIA-AI-IOT/redtail,https://github.com/NVIDIA-AI-IOT/redtail,335,2017-09-07T16:34:37Z,2017-09-07,16:34:37Z,2017,2021-01-12T18:52:04Z,2021-01-12,18:52:04Z,2021,Perception and AI components for autonomous mobile robotics.,"NVIDIA Redtail project Autonomous visual navigation components for drones and ground vehicles using deep learning. Refer to wiki for more information on how to get started. This project contains deep neural networks, computer vision and control code, hardware instructions and other artifacts that allow users to build a drone or a ground vehicle which can autonomously navigate through highly unstructured environments like forest trails, sidewalks, etc. Our TrailNet DNN for visual navigation is running on NVIDIA's Jetson embedded platform. Our arXiv paper describes TrailNet and other runtime modules in detail. The project's deep neural networks (DNNs) can be trained from scratch using publicly available data. A few pre-trained DNNs are also available as a part of this project. In case you want to train TrailNet DNN from scratch, follow the steps on this page. The project also contains Stereo DNN models and runtime which allow to estimate depth from stereo camera on NVIDIA platforms. IROS 2018: we presented our work at IROS 2018 conference as a part of Vision-based Drones: What's Next? workshop. CVPR 2018: we presented our work at CVPR 2018 conference as a part of Workshop on Autonomous Driving. References and Demos  Stereo DNN, CVPR18 paper, Stereo DNN video demo TrailNet Forest Drone Navigation, IROS17 paper, TrailNet DNN video demo GTC 2017 talk: slides, video Demo video showing 250m autonomous flight with TrailNet DNN flying the drone Demo video showing our 1 kilometer autonomous drone flight with TrailNet DNN Demo video showing TrailNet DNN driving a robotic rover around the office Demo video showing TrailNet generalization to ground vehicles and other environments  News   2020-02-03: Alternative implementations. redtail is no longer being developed, but fortunately our community stepped in and continued developing the project. We thank our users for the interest in redtail, questions and feedback! Some alternative implementations are listed below.  @mtbsteve: https://github.com/mtbsteve/redtail    2018-10-10: Stereo DNN ROS node and fixes.  Added Stereo DNN ROS node and visualizer node. Fixed issue with nvidia-docker v2.    2018-09-19: Updates to Stereo DNN.  Moved to TensorRT 4.0 Enabled FP16 support in ResNet18 2D model, resulting in 2x performance increase (20fps on Jetson TX2). Enabled TensorRT serialization in ResNet18 2D model to reduce model loading time from minutes to less than a second. Better logging and profiler support.    2018-06-04: CVPR 2018 workshop. Fast version of Stereo DNN.  Presenting our work at CVPR 2018 conference as a part of Workshop on Autonomous Driving. Added fast version of Stereo DNN model based on ResNet18 2D model. The model runs at 10fps on Jetson TX2. See README for details and check out updated sample_app.    GTC 2018: Here is our Stereo DNN session page at GTC18 and the recorded video presentation   2018-03-22: redtail 2.0.  Added Stereo DNN models and inference library (TensorFlow/TensorRT). For more details, see the README. Migrated to JetPack 3.2. This change brings latest components such as CUDA 9.0, cuDNN 7.0, TensorRT 3.0, OpenCV 3.3 and others to Jetson platform. Note that this is a breaking change. Added support for INT8 inference. This enables fast inference on devices that have hardware implementation of INT8 instructions. More details are on our wiki.    2018-02-15: added support for the TBS Discovery platform.  Step by step instructions on how to assemble the TBS Discovery drone. Instructions on how to attach and use a ZED stereo camera. Detailed instructions on how to calibrate, test and fly the drone.    2017-10-12: added full simulation Docker image, experimental support for APM Rover and support for MAVROS v0.21+.  Redtail simulation Docker image contains all the components required to run full Redtail simulation in Docker. Refer to wiki for more information. Experimental support for APM Rover. Refer to wiki for more information. Several other changes including support for MAVROS v0.21+, updated Jetson install script and few bug fixes.    2017-09-07: NVIDIA Redtail project is released as an open source project. Redtail's AI modules allow building autonomous drones and mobile robots based on Deep Learning and NVIDIA Jetson TX1 and TX2 embedded systems. Source code, pre-trained models as well as detailed build and test instructions are released on GitHub.   2017-07-26: migrated code and scripts to JetPack 3.1 with TensorRT 2.1. TensorRT 2.1 provides significant improvements in DNN inference performance as well as new features and bug fixes. This is a breaking change which requires re-flashing Jetson with JetPack 3.1.","ht with trailnet dnn demo video showing trailnet dnn driving a robotic rover around the office demo video showing trailnet generalization to ground vehicles and other environments  news   2020-02-03: alternative implementations. redtail is no longer being developed, but fortunately our community stepped in and continued developing the project. we thank our users for the interest in redtail, questions and feedback! some alternative implementations are listed below.  @mtbsteve: https://github.com/mtbsteve/redtail    2018-10-10: stereo dnn ros node and fixes.  added stereo dnn ros node and visualizer node. fixed issue with nvidia-docker v2.    2018-09-19: updates to stereo dnn.  moved to tensorrt 4.0 enabled fp16 support in resnet18 2d model, resulting in 2x performance increase (20fps on jetson tx2). enabled tensorrt serialization in resnet18 2d model to reduce model loading time from minutes to less than a second. better logging and profiler support.    2018-06-04: cvpr 2018 workshop. fast version of stereo dnn.  presenting our work at cvpr 2018 conference as a part of workshop on autonomous driving. added fast version of stereo dnn model based on resnet18 2d model. the model runs at 10fps on jetson tx2. see readme for details and check out updated sample_app.    gtc 2018: here is our stereo dnn session page at gtc18 and the recorded video presentation   2018-03-22: redtail 2.0.  added stereo dnn models and inference library (tensorflow/tensorrt). for more details, see the readme. migrated to jetpack 3.2. this change brings latest component robotriving a robotic rover around the office demo video showing trailnet generalization to ground vehicl",5,859,71,2,1,8,1
suculent/thinx-device-api,https://github.com/suculent/thinx-device-api,7,2017-04-09T10:01:56Z,2017-04-09,10:01:56Z,2017,2021-01-11T12:22:38Z,2021-01-11,12:22:38Z,2021,Remote IoT Device Management Platform,"☢ thinx-device-api IoT Device Management Server running on node.js. CircleCI Build status:    Component Status     thinx-device-api    arduino-docker-build    platformio-docker-build    mongoose-docker-build    micropython-docker-build    nodemcu-docker-build     Other badges             The CircleCI build is limited and therefore returns mostly bad results. Closer look may show better numbers. The Purpose  Update IoT device by pushing a code to a Git repository. We'll build it. Swap operating system for another over-the-air. Migrate multiple devices at once between WiFi networks. THiNX provides complete IoT infrastructure for your device (where the data storage and visualisation can be fully up to you). automatic updates for headless devices, or semi-automatic (with user consent after build and tests succeed)   As a user I have already many IoT new and/or legacy devices at home and new platforms are coming every day.   Sometimes we need to change WiFi credentials on a wireless switch mounted on a ceiling. The other day I we want to swap whole firmware for new one, but not always to rewrite working legacy Lua or Micropython code to PlatformIO.  That's why we have decided to create the über-platform: THiNX. Supported hardware Currently the platform supports building firmware for Arduino, PlatformIO (also with ESP-IDF), NodeMCU, Mongoose, Micropython and features JavaScript library that is intended to use on any hardware capable of running a Node.js server. Features   Remote Things Management console for monitoring devices, attaching source code, pushing data, managing incoming payloads and firmware updates.   Supports running in Docker Swarm, allowing to pass firmware-builds to specific nodes, etc.   Continuous Integration practices to update device apps/configurations from a GitHub repository using commit hooks.   Building secure MQTTS infrastructure as an optional side-chain transport layer.   Device registration endpoint while storing device data using CouchDB server and Redis session-store.   API is a back-end data provider (security agent) for RTM Admin Console Application.   Provides control to a dockerized build servers and pushes new firmware versions to client applications (FCM push) and devices (MQTT).   Provides HTTP-to-HTTPS proxy to secure legacy IoT devices that are not capable of TLS and/or AES-level encryption.   Allows transfer of device ownership (e.g. for pre-configured devices).   Custom firmware builder for MongooseOS, NodeMCU and Micropython (allow module selection, add THiNX as an OS-level library)   Transfer device to another owner along with sources/firmware.   Device status messages can be transformed using custom JavaScript lambda-style functions.   Supports OAuth login with Google and GitHub.   Supports LoRaWan server integration.   Supports Rollbar, Sqreen and Crisp.chat integrations.   Message-queue integration using single broker across many instances.   Supports Traefik for SSL offloading.   Supports external changes to device Environment object using API/API-Key. Changes in Environment for a firmware build cause firmware update even if the version of firmware is same, allowing seamless change of WiFi credentials or device build attributes).   Supported IoT Platforms   PlatformIO and Arduino IDE (ESP8266P/ESP32)   Micropython   Lua   MongooseOS   NodeJS (Mac/Linux/Windows)   Tested on Wemos D1 Mini, Wemos D1 Mini Pro, RobotDyn D1, RobotDyn D1 Mini, RobotDyn MEGA WiFi and various NodeMCU (Lolin, AI-THINKER) boards with Mongoose, Arduino Core, ESP-IDF, Lua and Micropython-based core firmwares...   Expected: Arduino and BigClown with networking support   Base THiNXLib Platform Library in C++: THiNXLib for ESP8266 THiNXLib for ESP32 THiNX Platform Library repositories for various IDEs and firmwares: Platform.io Arduino NodeMCU/Lua Micropython MongooseOS NodeJS Custom Firmwares With built-in THiNX Client Library: NodeMCU/Lua Micropython Arduino, Plaform.io and MongooseOS are firmwares by nature. Dockerized Firmware Builders PlatformIO Arduino MongooseOS NodeMCU/Lua Micropython Prerequisites for running own THiNX Server  Linux Server (min. 2 GB RAM, 32GB SSD, Ubuntu) Docker (docker-compose)  Port mapping  API runs on HTTP port 7442 (HTTPS 7443) MQTTS runs on port 8883 Admin runs on HTTP/HTTPS port (80/443) Status Transformers (internal network only, 7475)  Logging Use your favourite service and log-sender agent. Tested successfully with Logz.io, Instana and Sematext Installation Prerequisites Suggested:  Fully Qualified Domain Name (if you're testing on localhost, configure conf/config.json to set debug: { allow_http_login: true } ) Mailgun account (recently added) Rollbar integration  Optional:  Google Analytics integration Sqreen integration Slack integration Crisp.chat integration  Using Docker Compose Make sure you have valid directory structure available at /mnt/data (default) and edit the .env file to suit your needs. You don't need Mailgun for developer installation, just copy-paste the activation URL from api log using docker-compose logs -f while creating your first admin account. git clone http://github.com/suculent/thinx-device-api cd thinx-device-api cp .env.dist .env nano .env ./copy-envs.sh docker-compose up -d --build  Optionally see the ./circleci/config.yml for reference on installation steps. Using Docker Swarm on Manager Node THiNX expects Traefik load-balancer (e.g. with Swarmpit) in Docker Swarm. Example swarm compose file contains appropriate labels for Traefik in Swarm mode. THiNX also expects some kind of shared storage folder in /mnt/data default path. We have this mounted using GluserFS across all nodes so it does not matter where you run thinx app, console, mosquitto broker, worker or builders... all flatfile data are always available everywhere.  Install GlusterFS to make sure /mnt is available on all nodes in cluster.  Start THiNX on manager node: 	git pull git@github.com/suculent/thinx-device-api -b swarm 	 	cd thinx-device-api 	 	docker network create --scope=swarm nat  	docker stack deploy -c ./docker-swarm.yml  It's perfectly possible to run multiple instances of THiNX in Swarm. Just keep in mind that in order to support legacy HTTP transport devices, you need to have THiNX API port set differently for each instance (e.g. 7442 for production, 7441 for staging) because Swarm does not allow exposing same port twice across different services in same swarm. GitHub Webhook support You can direct your GitHub web-hooks to https://thinx.cloud:9001/ after adding a valid deploy key from GitHub to THiNX RTM. Endpoints See 03-test.sh. There is no point of maintaining documentation for this at current stage of development and user base zero. Upgrading After upgrading from versions before 1.1.5200, you may have issue with accessing CouchDB database. It's known issue, which can be fixed by editing the /opt/couchdb/etc/vm.args file inside the couchdb container. Just change the auto-generated domain on last line to couchdb@localhost and you should regain your access. Platforms State of Union Overall Platform libraries are now stabilised on the basic level, approaching first release version 1.0 with default HTTPS with optional fallback to HTTP for development. THiNX has now passed version 1.0 upgrading to swarm/docker-compose installation with separate container services (CouchDB, Redis, Transformers, THiNX, Traefik and optional monitoring services). Data and configuration are being moved to configurable location, which is by default /mnt/data:  deploy/ # build products ready for deployment to devices mosquitto/ # auth, log, config, data, ... repos/ # fetched/watched repositories ssh_keys/ # will be moved to vault and provided exlusively to builder ssl/ # shared SSL certificates, may be generated by Traefik/ACME/Letsencrypt ...etc  Arduino ESP8266/ESP32  Docker builder works. OTA update works.  PlatformIO  Docker builder works. OTA update is ready to be tested.  Pine 64  On horizon, builder and platform support is ready, waits for firmware...  Micropython  Docker builder works fine but needs tighter integration with sources. Deployment is not verified, therefore update cannot be tested now.  NodeMCU  File-based update has been pre-tested. Docker builder works fine but needs tighter integration with sources ($workdir). Deployment is not verified, therefore update cannot be tested. Will probably deprecate, because the toolset has not been updated for almost 3 years.  License","ttributes).   supported iot platforms   platformio and arduino ide (esp8266p/esp32)   micropython   lua   mongooseos   nodejs (mac/linux/windows)   tested on wemos d1 mini, wemos d1 mini pro, robotdyn d1, robotdyn d1 mini, robotdyn mega wifi and various nodemcu (lolin, ai-thinker) boards with mongoose, arduino core, esp-idf, lua and micropython-based core firmwares...   expected: arduino and bigclown with networking support   base thinxlib platform library in c++: thinxlib for esp8266 thinxlib for esp32 thinx platform library repositories for various ides and firmwares: platform.io arduino nodemcu/lua micropython mongooseos nodejs custom firmwares with built-in thinx client library: nodemcu/lua micropython arduino, plaform.io and mongooseos are firmwares by nature. dockerized firmware builders platformio arduino mongooseos nodemcu/lua micropython prerequisites for running own thinx server  linux server (min. 2 gb ram, 32gb ssd, ubuntu) docker (docker-compose)  port mapping  api runs on http port 7442 (https 7443) mqtts runs on port 8883 admin runs on http/https port (80/443) status transformers (internal network only, 7475)  logging use your favourite service and log-sender agent. tested successfully with logz.io, instana and sematext installation prerequisites suggested:  fully qualified domain name (if you're testing on localhost, configure conf/config.json to set debug: { allow_http_login: true } ) mailgun account (recently added) rollbar integration  optional:  google analytics integration sqreen integration slack integration crisp.chat integration  using docker compose make sure you have valid directory structure available at /mnt/data (default) and edit the .env file to suit your needs. you don't need mailgun for developer installation, just copy-paste the activation url from api log using docker-compose logs -f while creating your first admin account. git clone http://github.com/suculent/thinx-device-api cd thinx-device-api cp .env.dist .env nano .env ./copy-envs.sh docker-compose up -d --build  optionally see the ./circleci/config.yml for reference on installation steps. using docker swarm on manager node thinx expects traefik load-balancer (e.g. with swarmpit) in docker swarm. example swarm compose file contains appropriate labels for traefik in swarm mode. thinx also expects some kind of shared storage folder in /mnt/data default path. we have this mounted using gluserfs across all nodes so it does not matter where you run thinx app, console, mosquitto broker, worker or builders... all flatfile data are always available everywhere.  install glusterfs to make sure /mnt is available on all nodes in cluster.  start thinx on manager node: 	git pull git@github.com/suculent/thinx-device-api -b swarm 	 	cd thinx-device-api 	 	docker network create --scope=swarm nat  	docker stack deploy -c ./docker-swarm.yml  it's perfectly possible to run multiple instances of thinx in swarm. just keep in mind that in order to support legacy http transport devices, you need to have thinx api port set differently for each instance (e.g. 7442 for production, 7441 for staging) because swarm does not allow exposing same port twice across different services in same swarm. github webhook support you can direct your github web-h robot arduino ide (esp8266p/esp32)   micropython   lua   mongooseos   nodejs (mac/linux/windows)   tested",10,15,7599,0,1,0,1
Robo4J/robo4j,https://github.com/Robo4J/robo4j,16,2016-05-01T20:27:59Z,2016-05-01,20:27:59Z,2016,2021-01-08T13:42:50Z,2021-01-08,13:42:50Z,2021,Robo4j.io robotics/IoT framework,"Robo4J Robo4J provides an easy way of getting started with building custom hardware and creating software for it running on the JVM.  Robo4j.io is a robotics framework running on the JVM Robo4j.io provides a library of hardware abstractions for RaspberryPi and Lego EV3 out of the box Robo4j.io provides a library of configurable units that allows hardware to be enabled and configured through XML Robo4j.io provides a threading model controlled by annotations  The current Robo4j.io version is 0.5 Requirements Git, Gradle, Java JDK 11 If you are looking for a JDK 11 ARM hard float build for Raspbian, we recommend looking into Liberica JDK or Azul Zulu Embedded. Documentation See current Robo4j documentation.  Note: Under construction.  Building from Source The Robo4j framework uses Gradle for building. The following will build all components: ./gradlew jar The individual bundles will be available under robo4j/<component>/build/libs. To install the bundles and make them available to downstream dependencies, run the following: ./gradlew install  Note: If you are not using Robo4J as the standard user (pi) on a Raspberry Pi, you will have to specify the path to the local maven repository in the file libraries.gradle, variable: mavenRepository   Note: Robo4J currently requires OpenJDK 11. Ensure that you build and run with OpenJDK 11.  Staying in Touch Follow @robo4j or authors: @miragemiko, @hirt on Twitter. In-depth articles can be found at Robo4j.io, miragemiko blog or marcus blog License Robo4J is released under General Public License v3.",software for it running on the jvm.  robo4j.io is a robotics framework running on the jvm robo4j.io provides a library of hardware abstractions fo robotbotics framework running on the jvm robo4j.io provides a library of hardware abstractions for raspbe,6,63,1515,0,1,1,1
aws/aws-iot-device-sdk-cpp,https://github.com/aws/aws-iot-device-sdk-cpp,109,2016-10-18T18:12:05Z,2016-10-18,18:12:05Z,2016,2021-01-11T08:48:36Z,2021-01-11,08:48:36Z,2021,SDK for connecting to AWS IoT from a device using C++,"New Version Available A new AWS IoT Device SDK is now available. It is a complete rework, built to improve reliability, performance, and security. We invite your feedback! This SDK will no longer receive feature updates, but will receive security updates. AWS IoT C++ Device SDK  Overview Features Design Goals Collection of Metrics Getting Started Installation Porting to different platforms Quick Links Sample APIs License Support   Overview This document provides information about the AWS IoT device SDK for C++.  Features The Device SDK simplifies access to the Pub/Sub functionality of the AWS IoT broker via MQTT and provides APIs to interact with Thing Shadows. The SDK has been tested to work with the AWS IoT platform to ensure best interoperability of a device with the AWS IoT platform. MQTT Connection The Device SDK provides functionality to create and maintain a MQTT Connection. It expects to be provided with a Network Connection class that connects and authenticates to AWS IoT using either direct TLS or WebSocket over TLS. This connection is used for any further publish operations. It also allows for subscribing to MQTT topics which will call a configurable callback function when these messages are received on these topics. Thing Shadow This SDK implements the specific protocol for Thing Shadows to retrieve, update and delete Thing Shadows adhering to the protocol that is implemented to ensure correct versioning and support for client tokens. It abstracts the necessary MQTT topic subscriptions by automatically subscribing to and unsubscribing from the reserved topics as needed for each API call. Inbound state change requests are automatically signalled via a configurable callback. Jobs This SDK also implements the Jobs protocol to interact with the AWS IoT Jobs service. The IoT Job service manages deployment of IoT fleet wide tasks such as device software/firmware deployments and updates, rotation of security certificates, device reboots, and custom device specific management tasks. For additional information please see the Jobs developer guide.  Design Goals of this SDK The C++ SDK was specifically designed for devices that are not resource constrained and required advanced features such as Message queueing, multi-threading support and the latest language features Primary aspects are:  Designed around the C++11 standard Platform neutral, as long as the included CMake can find a C++11 compatible compiler and threading library Network layer abstracted from the SDK. Can use any TLS library and initialization method Support for multiple platforms and compilers. Tested on Linux, Windows (with VS2015) and Mac OS Flexibility in picking and choosing functionality, can create Clients which only perform a subset of MQTT operations Support for Rapidjson allowing use of complex shadow document structures   Collection of Metrics Beginning with Release v1.2.0 of the SDK, AWS collects usage metrics indicating which language and version of the SDK is being used. This allows us to prioritize our resources towards addressing issues faster in SDKs that see the most and is an important data point. However, we do understand that not all customers would want to report this data by default. In that case, the sending of usage metrics can be easily disabled by the user by using the overloaded Connect action which takes in a boolean for enabling or disabling the SDK metrics: p_iot_client_->Connect(ConfigCommon::mqtt_command_timeout_, ConfigCommon::is_clean_session_,                                         mqtt::Version::MQTT_3_1_1, ConfigCommon::keep_alive_timeout_secs_,                                         std::move(client_id), nullptr, nullptr, nullptr, false); // false for disabling metrics   How to get started ? Ensure you understand the AWS IoT platform and create the necessary certificates and policies. For more information on the AWS IoT platform please visit the AWS IoT developer guide.  Installation This section explains the individual steps to retrieve the necessary files and be able to build your first application using the AWS IoT C++ SDK. The SDK uses CMake to generate the necessary Makefile. CMake version 3.2 and above is required. Prerequisites:  Make sure to have latest CMake installed. Minimum required version is 3.2 Compiler should support C++11 features. We have tested this SDK with gcc 5+, clang 3.8 and on Visual Studio 2015. Openssl has version 1.0.2 and libssl-dev has version 1.0.2. OpenSSL v1.1.0 reference wrapper implementation is not included in this version of the SDK. You can find basic information on how to set up the above on some popular platforms in Platform.md  Build Targets:  The SDK itself builds as a library by default. All the samples/tests link to the library. The library target is aws-iot-sdk-cpp Unit tests - aws-iot-unit-tests Integration tests - aws-iot-integration-tests Sample - pub-sub-sample Sample - shadow-delta-sample  This following sample targets are generated only if OpenSSL is being used:  Sample - discovery-sample. Sample - robot-arm-sample. Sample - switch-sample  Steps:  Clone the SDK from the github repository Change to the repository folder. Create a folder called build to hold the build files and change to this folder. In-source builds are NOT allowed Run cmake ../. to build the SDK with the CLI. The command will download required third party libraries automatically and generate a Makefile Type make <target name> to build the desired target. It will create a folder called bin that will have the build output   Porting to different platforms The SDK has been written to adhere to C++11 standard without any additional compiler specific features enabled. It should compile on any platform that has a modern C++11 enabled compiler without issue. The platform should be able to provide a C++11 compatible threading implementation (eg. pthread on linux). TLS libraries can be added by simply implementing a derived class of NetworkConnection and providing an instance to the Client. We provide the following reference implementations for the Network layer:  OpenSSL - MQTT over TLS using OpenSSL v1.0.2. Tested on Windows (VS 2015) and Linux  The provided implementation requires OpenSSL to be pre-installed on the device Use the mqtt port setting from the config file while setting up the network instance   MbedTLS - MQTT over TLS using MbedTLS. Tested on Linux  The provided implementation will download MbedTLS v2.3.0 from the github repo and build and link to the libraries. Please be warned that the default configuration of MbedTLS limits packet sizes to 16K Use the mqtt port setting from the config file while setting up the network instance   WebSocket - MQTT over WebSocket. Tested on both Windows (VS 2015) and Linux. Uses OpenSSL 1.0.2 as the underlying TLS layer  The provided implementation requires OpenSSL to be pre-installed on the device Please be aware that while the provided reference implementation allows initialization of credentials from any source, the recommended way to do so is to use the aws cli to generate credential files and read the generated files Use the https port setting from the config file while setting up the network instance    Cross-compiling the SDK for other platforms The included ToolchainFile.cmake file can be used to cross-compile the SDK for other platforms. Procedure for testing cross compiling (if using OpenSSL):  build/download toolchain for specific platform modify the ToolchainFile.cmake with location and target of toolchain.  # specify toolchain directory SET(TOOLCHAIN_DIR /home/toolchain/dir/here/bin)  # specify cross compilation target SET(TARGET_CROSS target-here)`    Cross-compile OpenSSL using the same toolchain   modify network/CMakeLists.txt.in and change OpenSSL library location to cross-compiled OpenSSL    cd build cmake ../. -DCMAKE_TOOLCHAIN_FILE=../ToolchainFile.cmake make   Scp the application binary, certs and config for the application into the platform you're testing Run ./<application>  For MbedTLS, you don't need to cross-compile MbedTLS as it gets compiled when you run make with the same compiler as pointed to by the toolchain file. Also included is a simple example 'toolchain' which is used for setting the default compiler as clang++ instead of g++ as an example to show how the toolchain file can be modified.  Quick Links  SDK Documentation - API documentation for the SDK Platform Guide - This file lists the steps needed to set up the pre-requisites on some popular platforms Developers Guide - Provides a guide on how the SDK can be included in custom code Greengrass Discovery Support Guide - Provides information on support for AWS Greengrass Discovery Service Network Layer Implementation Guide - Detailed description about the Network Layer and how to implement a custom wrapper class Sample Guide - Details about the included samples Test Information - Details about the included unit and integration tests MQTT 3.1.1 Spec - Link to the MQTT v3.1.1 spec that this SDK implements   Sample APIs Sync Creating a basic MQTT Client requires a NetworkConnection instance and MQTT Command timeout in milliseconds for any internal blocking operations. std::shared_ptr<NetworkConnection> p_network_connection = <Create Instance>; std::shared_ptr<MqttClient> p_client = MqttClient::Create(p_network_connection, std::chrono::milliseconds(30000));  Connecting to the AWS IoT MQTT platform rc = p_client->Connect(std::chrono::milliseconds(30000), false, mqtt::Version::MQTT_3_1_1, std::chrono::seconds(60), Utf8String::Create(""<client_id>""), nullptr, nullptr, nullptr);  Subscribe to a topic util::String p_topic_name_str = <topic>; std::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str); mqtt::Subscription::ApplicationCallbackHandlerPtr p_sub_handler = std::bind(&<handler>, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3); std::shared_ptr<mqtt::Subscription> p_subscription = mqtt::Subscription::Create(std::move(p_topic_name), mqtt::QoS::QOS0, p_sub_handler, nullptr); util::Vector<std::shared_ptr<mqtt::Subscription>> topic_vector; topic_vector.push_back(p_subscription); rc = p_client->Subscribe(topic_vector, std::chrono::milliseconds(30000));  Publish to a topic util::String p_topic_name_str = <topic>; std::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str); rc = p_client->Publish(std::move(p_topic_name), false, false, mqtt::QoS::QOS1, payload, std::chrono::milliseconds(30000));  Unsubscribe from a topic util::String p_topic_name_str = <topic>; std::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str); util::Vector<std::unique_ptr<Utf8String>> topic_vector; topic_vector.push_back(std::move(p_topic_name)); rc = p_client->Subscribe(topic_vector, std::chrono::milliseconds(30000));  Async Connect is a sync only API in this version of the SDK. Subscribe to a topic uint16_t packet_id_out; util::String p_topic_name_str = <topic>; std::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str); mqtt::Subscription::ApplicationCallbackHandlerPtr p_sub_handler = std::bind(&<handler>, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3); std::shared_ptr<mqtt::Subscription> p_subscription = mqtt::Subscription::Create(std::move(p_topic_name), mqtt::QoS::QOS0, p_sub_handler, nullptr); util::Vector<std::shared_ptr<mqtt::Subscription>> topic_vector; topic_vector.push_back(p_subscription); rc = p_client->SubscribeAsync(topic_vector, nullptr, packet_id_out);  Publish to a topic uint16_t packet_id_out; util::String p_topic_name_str = <topic>; std::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str); rc = p_client->PublishAsync(std::move(p_topic_name), false, false, mqtt::QoS::QOS1, payload, packet_id_out);  Unsubscribe from a topic uint16_t packet_id_out; util::String p_topic_name_str = <topic>; std::unique_ptr<Utf8String> p_topic_name = Utf8String::Create(p_topic_name_str); util::Vector<std::unique_ptr<Utf8String>> topic_vector; topic_vector.push_back(std::move(p_topic_name)); rc = p_client->Subscribe(topic_vector, packet_id_out);  Logging To enable logging, create an instance of the ConsoleLogSystem in the main() of your application as shown below: std::shared_ptr<awsiotsdk::util::Logging::ConsoleLogSystem> p_log_system =     std::make_shared<awsiotsdk::util::Logging::ConsoleLogSystem>(awsiotsdk::util::Logging::LogLevel::Info); awsiotsdk::util::Logging::InitializeAWSLogging(p_log_system);  Create a log tag for your application to distinguish it from the SDK logs: #define LOG_TAG_APPLICATION ""[Application]""  You can now add logging to any part of your application using AWS_LOG_ERROR or AWS_LOG_INFO as shown below: AWS_LOG_ERROR(LOG_TAG_APPLICATION, ""Failed to perform action. %s"",               ResponseHelper::ToString(rc).c_str());   License This SDK is distributed under the Apache License, Version 2.0, see LICENSE and NOTICE.txt for more information.  Support If you have any technical questions about AWS IoT C++ SDK, use the AWS IoT forum. For any other questions on AWS IoT, contact AWS Support. A list of known issues is maintained in KnownIssues.md. Note: customers have reported deadlocks while using the AWS IoT Device SDK for C++. If you are affected, a fix is available in the locking-fixes branch. This issue is also resolved in the new AWS IoT Device SDK for C++, which is currently in Developer Preview.","a-sample  this following sample targets are generated only if openssl is being used:  sample - discovery-sample. sample - robot-arm-sample. sample - switch-sample  steps:  clone the sdk from the github repository change to the repository folder. create a folder called build to hold the build files and change to this folder. in-source builds are not allowed run cmake ../. to build the sdk with the cli. the command will download required third party libraries automatically and generate a makefile type make <target name> to build the desired target. it will create a folder called bin that will have the build output   porting to different platforms the sdk has been written to adhere to c++11 standard without any additional compiler specific features enabled. it should compile on any platform that has a modern c++11 enabled compiler without issue. the platform should be able to provide a c++11 compatible threading implementation (eg. pthread on linux). tls libraries can be added by simply implementing a derived class of networkconnection and providing an instance to the client. we provide the following reference implementations for the network layer:  openssl - mqtt over tls using openssl v1.0.2. tested on windows (vs 2015) and linux  the provided implementation requires openssl to be pre-installed on the device use the mqtt port setting from the config file while setting up the network instance   mbedtls - mqtt over tls using mbedtls. tested on linux  the provided implementation will download mbedtls v2.3.0 from the github repo and build and link to the libraries. please be warned that the default configuration of mbedtls limits packet sizes to 16k use the mqtt port setting from the config file while setting up the network instance   websocket - mqtt over websocket. tested on both windows (vs 2015) and linux. uses openssl 1.0.2 as the underlying tls layer  the provided implementation requires openssl to be pre-installed on the device please be aware that while the provided reference implementation allows initialization of credentials from any source, the recommended way to do so is to use the aws cli to generate credential files and read the generated files use the https port setting from the config file while setting up the network instance    cross-compiling the sdk for other platforms the included toolchainfile.cmake file can be used to cross-compile the sdk for other platforms. procedure for testing cross compiling (if using openssl):  build/download toolchain for specific platform modify the toolchainfile.cmake with location and target of toolchain.  # specify toolchain directory set(toolchain_dir /home/toolchain/dir/here/bin)  # specify cross compilation target set(target_cross target-here)`    cross-compile openssl using the same toolchain   modify network/cmakelists.txt.in and change openssl library location to cross-compiled openssl    cd build cmake ../. -dcmake_toolchain_file=../toolchainfile.cmake make   scp the application binary, certs and config for the application into the platform you're testing run ./<application>  for mbedtls, you don't need to cross-compile mbedtls as it gets compiled when you run make with the same compiler as pointed to by the toolchain file. also included is a simple example 'toolchain' which is used for setting the default compiler as clang++ instead of g++ as an example to show how the toolchain file can be modified.  quick links  sdk documentation - api documentation for the sdk platform guide - this file lists the steps needed to set up the pre-requisites on some popular platforms developers guide - provides a guide on how the sdk can be included in custom code greengrass discovery support guide - provides information on support for aws greengrass discovery service network layer implementation guide - detailed description about the network layer and how to implement a custom wrapper class sample guide - details about the included samples test information - details about the included unit and integration tests mqtt 3.1.1 spec - link to the mqtt v3.1.1 spec that this sdk implements   sample apis sync creating a basic mqtt client requires a networkconnection instance and mqtt command timeout in milliseconds for any internal blocking operations. std::shared_ptr<networkconnection> p_network_connection = <create instance>; std::shared_ptr<mqttclient> p_client = mqttclient::create(p_network_connection, std::chrono::milliseconds(30000));  connecting to the aws iot mqtt platform rc = p_client->connect(std::chrono::milliseconds(30000), false, mqtt::version::mqtt_3_1_1, std::chrono::seconds(60), utf8string::create(""<client_id>""), nullptr, nullptr, nullptr);  subscribe to a topic util::string p_topic_name_str = <topic>; std::unique_ptr<utf8string> p_topic_name = utf8string::create(p_topic_name_str); mqtt::subscription::applicationcallbackhandlerptr p_sub_handler = std::bind(&<handler>, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_ robotonly if openssl is being used:  sample - discovery-sample. sample - robot-arm-sample. sample - switc",23,103,89,0,1,0,1
hybridgroup/cylon,https://github.com/hybridgroup/cylon,374,2013-10-18T06:57:08Z,2013-10-18,06:57:08Z,2013,2021-01-07T11:46:42Z,2021-01-07,11:46:42Z,2021,"JavaScript framework for robotics, drones, and the Internet of Things (IoT)","Cylon.js is a JavaScript framework for robotics, physical computing, and the Internet of Things (IoT). It provides a simple, but powerful way to create solutions that incorporate multiple, different hardware devices concurrently. Want to use Node.js for robots, drones, and IoT devices? You are in the right place. Want to use Ruby on robots? Check out our sister project, Artoo. Want to use Golang to power your robots? Check out our sister project, Gobot. Build Status:    Getting Started Installation All you need to get started on a new robot is the cylon module: npm install cylon  With the core module installed, now install the modules for whatever hardware support you need. For the Arduino + LED blink example, we'll need the firmata, gpio, and i2c modules: npm install cylon-firmata cylon-gpio cylon-i2c  Examples Arduino + LED The below example connects to an Arduino over a serial connection, and blinks an LED once per second. The example requires that the Arduino have the Firmata sketch installed; which can be obtained either through the Ardunio IDE or the gort arduino upload firmata command available in gort. var Cylon = require('cylon');  // define the robot var robot = Cylon.robot({   // change the port to the correct one for your Arduino   connections: {     arduino: { adaptor: 'firmata', port: '/dev/ttyACM0' }   },    devices: {     led: { driver: 'led', pin: 13 }   },    work: function(my) {     every((1).second(), my.led.toggle);   } });  // connect to the Arduino and start working robot.start(); Parrot ARDrone 2.0 var Cylon = require('cylon');  Cylon.robot({   connections: {     ardrone: { adaptor: 'ardrone', port: '192.168.1.1' }   },    devices: {     drone: { driver: 'ardrone' }   },    work: function(my) {     my.drone.takeoff();      after((10).seconds(), my.drone.land);     after((15).seconds(), my.drone.stop);   } }).start(); Cat Toy (Leap Motion + Digispark + Servos) var Cylon = require('cylon');  Cylon.robot({   connections: {     digispark: { adaptor: 'digispark' },     leapmotion: { adaptor: 'leapmotion' }   },    devices: {     servo1: { driver: 'servo', pin: 0, connection: 'digispark' },     servo2: { driver: 'servo', pin: 1, connection: 'digispark' },     leapmotion: { driver: 'leapmotion', connection: 'leapmotion' }   },    work: function(my) {     my.x = 90;     my.z = 90;      my.leapmotion.on('hand', function(hand) {       my.x = hand.palmX.fromScale(-300, 300).toScale(30, 150);       my.z = hand.palmZ.fromScale(-300, 300).toScale(30, 150);     });      every(100, function() {       my.servo1.angle(my.x);       my.servo2.angle(my.z);        console.log(my.servo1.currentAngle() + "", "" + my.servo2.currentAngle());     });   } }).start(); Multiple Spheros + HTTP API Plugin To use the HTTP API plugin, first install it's NPM module: $ npm install cylon-api-http  Then it can be used in scripts: var Cylon = require('cylon');  // tell the HTTP API plugin to listen for requests at https://localhost:4000 Cylon.api(""http"", { port: 4000 });  var bots = [   { port: '/dev/rfcomm0', name: 'Thelma' },   { port: '/dev/rfcomm1', name: 'Louise' } ];  bots.forEach(function(bot) {   Cylon.robot({     name: bot.name,      connections: {       sphero: { adaptor: ""sphero"", port: bot.port }     },      devices: {       sphero: { driver: ""sphero"" }     },      work: function(my) {       every((1).second(), function() {         console.log(my.name);         my.sphero.setRandomColor();         my.sphero.roll(60, Math.floor(Math.random() * 360));       });     }   }); });  // start up all robots at once Cylon.start(); Fluent Syntax For those more familiar with jQuery, D3, or other fluent-style JavaScript libraries, Cylon.JS also supports a chainable syntax: var Cylon = require('cylon');  Cylon   .robot()   .connection('arduino', { adaptor: 'firmata', port: '/dev/ttyACM0' })   .device('led', { driver: 'led', pin: 13 })   .on('ready', function(bot) {     setInterval(function() {       bot.led.toggle();     }, 1000);   });  Cylon.start(); Hardware Support Cylon.js has an extensible syntax for connecting to multiple, different hardware devices. The following 36 platforms are currently supported:    Platform Support     ARDrone cylon-ardrone   Arduino cylon-firmata   Arduino YUN cylon-firmata   AT&T M2X cylon-m2x   Audio cylon-audio   Beaglebone Black cylon-beaglebone   Bebop cylon-bebop   Bluetooth LE cylon-ble   Crazyflie cylon-crazyflie   Digispark cylon-digispark   Electric Imp cylon-imp   Intel Edison cylon-intel-iot   Intel Galileo cylon-intel-iot   Intel IoT Analytics cylon-intel-iot-analytics   Joystick cylon-joystick   Keyboard cylon-keyboard   Leap Motion cylon-leapmotion   MiP cylon-mip   MQTT cylon-mqtt   Nest cylon-nest   Neurosky cylon-neurosky   OpenCV cylon-opencv   Phillips Hue cylon-hue   Pebble cylon-pebble   Pinoccio cylon-pinoccio   PowerUp 3.0 cylon-powerup   Rapiro cylon-rapiro   Raspberry Pi cylon-raspi   Salesforce cylon-force   Skynet cylon-skynet   Spark cylon-spark   Speech cylon-speech   Sphero cylon-sphero   Sphero BLE cylon-sphero-ble   Tessel cylon-tessel   WICED Sense cylon-wiced-sense    Our implementation of GPIO (General Purpose Input/Output) allows for a shared set of drivers supporting 14 different devices:  GPIO <=> Drivers  Analog Sensor Button Continuous Servo Direct Pin IR Range Sensor LED Makey Button (high-resistance button like the MakeyMakey) Maxbotix Ultrasonic Range Finder Motor Relay RGB LED Servo Temperature Sensor TP401 Gas Sensor    We also support 14 different I2C (Inter-Integrated Circuit) devices through a shared cylon-i2c module:  I2C <=> Drivers  BlinkM RGB LED BMP180 Barometric Pressure + Temperature sensor Direct I2C HMC6352 Digital Compass JHD1313M1 LCD with RGB Backlight LCD LIDAR-Lite LSM9DS0G 9 Degrees of Freedom IMU LSM9DS0XM 9 Degrees of Freedom IMU MAG3110 3-Axis Digital Magnetometer MPL115A2 Digital Barometer & Thermometer MPU6050 Triple Axis Accelerometer and Gyro PCA9544a 4-Channel I2C Mux PCA9685 16-Channel 12-bit PWM/Servo Driver    In addition to our officially supported platforms, we have the following 8 user contributed platforms:    Platform Support     APC UPS cylon-apcupsd   iBeacon cylon-beacon   Myo cylon-myo   One-Wire cylon-one-wire   Parrot Rolling Spider cylon-rolling-spider   PCDuino cylon-pcduino   Telegram cylon-telegram   WeMo cylon-wemo    We'll also have many more platforms and drivers coming soon, follow us on Twitter for updates. Browser & Mobile Support Cylon.js can be run directly in-browser, using the browserify NPM module. You can also run it from withing a Chrome connected app, or a PhoneGap mobile app. For more info on browser support, and for help with different configurations, you can find more info in our docs. API Plugins Cylon.js has support for different API plugins that can be used to interact with your robots remotely. At this time we have support for http/https, mqtt, and socket.io with more coming in the near future. To use an API plugin, install it alongside Cylon: $ npm install cylon-api-http cylon-api-socketio  Then, all you need to do is call Cylon#api in your robot's script: var Cylon = require(""cylon"");  // For http Cylon.api('http');  // Or for Socket.io Cylon.api('socketio'); Then visit https://localhost:3000/ and you are ready to control your robots from a web browser!  You can check out more information on the Cylon API in the docs here. CLI Cylon uses the Gort http://gort.io Command Line Interface (CLI) so you can access important features right from the command line. We call it ""RobotOps"", aka ""DevOps For Robotics"". You can scan, connect, update device firmware, and more! Cylon also has its own CLI to generate new robots, adaptors, and drivers. You can check it out at https://github.com/hybridgroup/cylon-cli. Documentation We're busy adding documentation to our website, check it out at cylonjs.com/documentation. If you want to help with documentation, you can find the code for our website at on the https://github.com/hybridgroup/cylon-site. Contributing For our contribution guidelines, please go to CONTRIBUTING.md. Release History For the release history, please go to RELEASES.md. License Copyright (c) 2013-2016 The Hybrid Group. Licensed under the Apache 2.0 license.","cylon.js is a javascript framework for  robotics, physical computing, and the internet of things (iot). it provides a simple, but powerful way to",20,3800,1324,2,1,0,1
PTiagorio/fikalab,https://github.com/PTiagorio/fikalab,0,2019-11-20T22:56:04Z,2019-11-20,22:56:04Z,2019,2020-08-27T03:19:57Z,2020-08-27,03:19:57Z,2020,Fikalab'19 Project repository Blockchain for IoT: a Smart Contract Approach,"Blockchain for IoT: a Smart Contracts Approach This project consists of a proof-of-concept based on a conceptually simple problem: a system built for a robot car to be controlled by several mobile phones. However, the approach to this problem was made with cutting-edge, complex, innovative and technically challenging technologies: Internet of Things, Cloud and Blockchain. This project is based on the idea of bringing these two technologies together, taking advantage of the best features of both and creating a scalable system in which multiple entities control multiple devices. Reaching a system that guarantees:  Auditability and immutability from Blockchain Cloud processing delegation capability and scalability of node numbers from the Cloud and the Internet of Things  Path Taken After conceiving the basic idea for our project, a solution was needed to demonstrate that the system could be implemented in reality. A simple problem where there is a robot-car being controlled by someone has been identified as a good approach. The group looked at relatively inexpensive and intuitive technologies. We had to get hardware to simulate three different entities in our problem:  The controlling entity The communication entity between the controlling entity and the entity to be controlled The entity to be controlled  For the first entity, smartphones were identified, as it would be an easy way to demonstrate several controlling entities, because smartphones are a fairly common technology today. For the second entity the Raspberry Pi was identified, since it is a computer about the size of a credit card, being versatile and cheap. Finally for the third entity was discovered GoPiGo, a robot with wheels prefabricated to support a Raspberry Pi. The discovery of the GoPiGo was an important catalyst to cement our solution, as it is relatively easy to demonstrate results when the third entity has real actions, in this case moving to where the user commands it. All that remained was to identify how the technologies would be used together. After several brainstorming sessions and several iterations, the group came up with a system architecture:  In this mockup the entities identified earlier are present. Bridging the gap between them and this mockup: the controlling entities are the mobile phones; The communication entity is the Raspberry Pi, which is commonly referred to as RaspEstatico in our project; and finally the GoPiGo which is the controlled entity. The blockchain is present between mobile phones and  the RaspEstatico. Blockchain will allow all commands made by mobile phones to be known to all nodes of the network and to have immutability and auditability characteristics. The cloud / IoT serves as a bridge between the RaspEstatico and GoPiGo. Software will be used to have GoPiGo subscribed to the cloud, which can make all subscribed devices move when RaspEstatico communicates a command. In our case there is only one GoPiGo, but scalability is one of the main features of this system, because of the use of IoT through the Cloud. The technical aspects of the technologies used will be explained in the read-me files of other folders, but in general the communication between system nodes can be summarized by:  A command given by a mobile phone user to move the car (touch of a button) RaspEstatico is listening to an event triggered by Blockchain RaspEstatico communicates the command to the cloud Cloud tells GoPiGo the command to perform  Repository Structure  Blockchain This directory is for all Blockchain information and content Applications This directory contains the two Apps from this project Cloud This directory has the information about the Cloud technologies Presentations In this directory are stored presentations developed by the group throughout the project  GLOBAL PRE-REQUISITES npm Installation Windows:  npm  Ubuntu: $ sudo apt-get update $ sudo apt-get install nodejs $ sudo apt-get install npm  Authors  Tiago Ferreira - Project Manager Luís Silvestre - Blockchain Developer Rodrigo Rafael - Software Developer","conceptually simple problem: a system built for a robot car to be controlled by several mobile phones. however, the approach to this problem was made w robott car to be controlled by several mobile phones. however, the approach to this problem was made with",1,0,1527,0,1,4,1
kaiwaehner/iiot-integration-apache-plc4x-kafka-connect-ksql-opc-ua-modbus-siemens-s7,https://github.com/kaiwaehner/iiot-integration-apache-plc4x-kafka-connect-ksql-opc-ua-modbus-siemens-s7,4,2019-07-19T11:52:16Z,2019-07-19,11:52:16Z,2019,2020-11-16T22:19:08Z,2020-11-16,22:19:08Z,2020,"Industrial IoT (IIoT) Integration and Data Processing with Apache PLC4X, Kafka Connect, KSQL (OPC-UA, Modbus, Siemens S7)","Demo Under Construction !!! Target Date: October 2019 Kafka-native end-to-end IIoT Data Integration and Processing with Kafka Connect, KSQL and Apache PLC4X This project shows how to integrate, process and analyze data from Industrial IoT (IIoT) devices and machines in real time, reliable and scalable. We leverage Apache Kafka, Kafka Connect, KSQL and Apache PLC4X. Use Cases in Industry 4.0 / IIoT - Big Data, Analytics, Real Time Processing Analytics  Ingest data into cloud for analytics to a) reduce cost: Leverage open frameworks instead of paying very expensive licenses per machine and b) flexible integration (select data to ingest, flexible changes over time) Machine Learning / Data Science  Manufacturing  Collect data from machines --> Preprocess + monitoring to optimize assembly line and reduce cost Aggregate data from different machines / companies —> Leverage (and sell?) insights Sell services on top of machines —> Predictive maintenance (remote) Scale up (add more sites, add more data)  Production Robots  Ingest, process and monitor large volumes data (where the proprietary monolith does not scale)  Smart Factories  Monitor and manage the whole factory (at scale, in real time, flexible) Integration with legacy proprietary protocols and modern cloud-native technologies  Unfortunately, there are some huge challenges to implement these IIoT use cases in the real world: Challenges in Industry 4.0 / IIoT In short: IoT != IIoT:  IoT == Connected cars, smart home, etc. Large scale, secure, scalable, open, modern technologies IIoT == Slow, unsecure, not scalable, proprietary  IIoT usually uses incompatible protocols, which typically are proprietary. Machines have a very long lifecycle (tens of years). Factories cost millions, no simple changes / upgrades possible. Factories are built out of monoliths without scalability, extendibility, or real failover. Security is completely missing - there is no authentication or authorization. In real world, you typically see one of two situations:  A company does not have access to the valuable data of its machines and devices A proprietary, inflexible, monolithic, expensive IoT solution is bought and installed for each PLC technology like Siemens S7, Modbus, etc. (often combined with another ""central general middleware solution"")   As a side note: This project is not about the central integration middleware, but specifically about IIoT integration. If you need a better understanding of how to build a scalable, flexible and reliable middleware, check out Event Streaming Platform vs. Traditional Middleware like MQ, ETL, ESB for more details. Opportunities in Industry 4.0 / IIoT Huge opportunities exist in IIoT and Industry 4.0, but you need to get rid of the old legacy architecture (while you need to keep the old machines and legacy protocols, unfortunately).  How to get from legacy, proprietary to cloud, big data, machine learning to realize all these valuable use cases? Architecture We demonstrate how to achieve all characteristics (cost reduction, flexibility , standards-based, scalability, extendibility, security) leveraging the following open source technologies:  Apache Kafka: An Event Streaming platform to process huge volumes of data reliably in real time Kafka Connect: A general integration framework built on top of Kafka (and part of the same Apache project) to leverage all of its features under the hood to integrate with any source or sink Apache PLC4X: Integration framework dedicated to  integration with legacy proprietary IIoT protocols like Siemens S7, Modbus, Beckhoff ADS, Allen Bradley Ethernet, et al.  This combination provides a scalable, flexible and secure infrastructure to build an integration between machines and devices on IoT side and the rest of the enterprise like big data analytics tools, ERP systems,cloud services or any other business application:  TODO Share Slide Deck and Video Recording Open Source vs. Commercial IoT Platform Solutions - Friends, Enemies, Frenemies? Before we come to the live demo, here a few more thoughts:  Open source solutions like Apache Kafka or PLC4X are open and flexible, but also battle-tested in many production deployments. Commercial IoT Platform Solutions like Siemens MindSphere, Cisco Kinetic or IoT solutions from cloud providers like GCP, AWS or Azure are proprietary, costly and inflexible (because you typically cannot easily adjust them - they are built for one specific infrastructure or IIoT product line) Commercial IoT Platform Solutions also have many pros like being a completely platform instead of a framework, and providing tooling like nice drag&drop user interfaces  Therefore, this is typically not an ""either... nor"" decision. In many scenarios, both options can be combined and are complementary. It is no surprise that most IoT Platform Solutions also provide a Kafka adapter to build a scalable streaming platform and integrate with the rest of the enterprise in a proven, battle-tested way leveraging open source components.  Live Demo - Try it out yourself! Use Case TODO Kai Requirements TODO Kai Scripts / Config / Code  MVP: 1 PLC --> PLC4X --> Kafka Connect --> Kafka --> KSQL Consumer V2: 2 or more different PLC --> PLC4X --> Kafka Connect --> Kafka --> KSQL Consumer + Simple UI (whatever tool)  Step-by-Step Guide TODO Kai","ce (remote) scale up (add more sites, add more data)  production robots  ingest, process and monitor large volumes data (where the proprietary monolith does not scale)  smart factories  monitor and manage the whole factory (at scale, in real time, flexible) integration with legacy proprietary protocols and modern cloud-native technologies  unfortunately, there are some huge challenges to implement these iiot use cases in the real world: challenges in industry 4.0 / iiot in short: iot != iiot:  iot == connected cars, smart home, etc. large scale, secure, scalable, open, modern technologies iiot == slow, unsecure, not scalable, proprietary  iiot usually uses incompatible protocols, which typically are proprietary. machines have a very long lifecycle (tens of years). factories cost millions, no simple changes / upgrades possible. factories are built out of monoliths without scalability, extendibility, or real failover. security is completely missing - there is no authentication or authorizatio robotproduction robots  ingest, process and monitor large volumes data (where the proprietary monolith do",1,24,8,0,1,1,1
NVIDIA-AI-IOT/Foresee-Navigation,https://github.com/NVIDIA-AI-IOT/Foresee-Navigation,17,2018-08-02T23:29:56Z,2018-08-02,23:29:56Z,2018,2020-12-29T03:34:46Z,2020-12-29,03:34:46Z,2020,Semantic-Segmentation based autonomous indoor navigation for mobile robots,"Foresee Navigation Semantic-Segmentation based autonomous indoor navigation for mobile robots. The Problem The principle sensor for autonomously navigating robots is the LiDAR. However, low-cost 2D LiDARs cannot detect many man-made obstacles such as mesh-like railings, glass walls, or descending staircases, and 3D LiDARs that can detect some of these are prohibitively expensive. This project is a proof-of-concept showing that Deep Learning can provide a solution for detecting such obstacles. Video Demo  Our Approach The basis of our strategy is to integrate detected obstacle locations into proven tools for autonomous navigation from the Robot Operating System. The process begins by segmenting the floor using DeepLab with ResNet V2 50. To learn about how this works in more detail, see this.  A perspective transform is then applied to get top-down view of safe and unsafe areas. The resulting image is cut into discrete sectors, and the lowest point in each sector is calculated. These points, after a linear transform to real-world robot-relative coordinates, form a ROS pointcloud that is fed into our navigation stack.  This data is then fed into SLAM through gmapping and we use pathfinder and nonlinear reference tracking to follow a path autonomously. See more information about pathfinder_ros and reftracking_ros. Our Platform This repository includes an implementation of the above based on the Jackal robot platform with a NVIDIA Jetson TX2 inside. We integrate the above pipeline with a low-cost RPLiDAR A1. For a camera we used an iBUFFALO BSW20KM11BK mounted about 25cm above the ground.  Repository Structure The ROS workspace that is deployed to the robot is located in ros_workspace. Inside src, robot_control contains the coordinating launch files and safety nodes and deep_segmentation contains the Deep Learning pipeline. The superpixel-seg directory contains an earlier approach to segmenting the floor based on superpixels and a random forest classifier. SXLT contains a small labeling tool for segmentation. Since this project depends on OpenCV, (and superpixel-seg on OpenCV_contrib), a version of JetsonHacks's buildOpenCVTX2 that allows enabling and disabling certain contrib modules is included. Usage Our code assumes a Jackal development platform with a BNO055 IMU integrated, an RPLidar A1 with udev rules configured, and a 120-degree-fov iBUFFALO BSW20KM11BK with distortion and mounting characteristics identical to our own. Software dependencies include OpenCV, ROS, NumPy, and Tensorflow. To get segmentation to work, it is highly recommended that you use CUDA 8 and Tensorflow 1.3. CUDA 8 can be installed via JetPack 3.1 without re-flashing. Begin by running provision.sh in the root directory. This downloads files that could not be re-distributed for licensing reasons and the binary model files. To run the code, first setup the ROS Workspace. cd ros_workspace, then execute setup.sh, and finally source install_isolated.sh. This will install all necessary ROS packages and compile certain ones from source inline. If you want to edit the code, remember to re-source install_isolated.sh after each edit. Finally, roslaunch robot_control auton.launch and, in a separate terminal, rosrun driver_station ds.py. To enable, use C-x C-e and press space to disable. License Apache 2.0, see LICENSE.","ion based autonomous indoor navigation for mobile robots. the problem the principle se robotts. the problem the principle sensor for autonomously navigating robots is the lidar. however, low-c",4,77,3,0,1,15,1
vomyrak/BuddyHub,https://github.com/vomyrak/BuddyHub,1,2018-07-11T10:08:37Z,2018-07-11,10:08:37Z,2018,2020-03-02T09:28:59Z,2020-03-02,09:28:59Z,2020,A universal controller platform for assistive technology,"BuddyHub The Universal Controller Platform for Assistive Technology BuddyHub is a free and open-source platform that leverages the Internet of Things (IoT) to allow people with different disabilities to control various output devices in their environment via accesible inputs through a simple-to-navigate and user-friendly interface. About BuddyHub was born out of a joint collaboration between Imperial College London and Wooden Spoon: The Children's Charity of Rugby, the latter of whom invests heavily in assistive technologies and aims to promote their development and make them more accessible to end users. BuddyHub aims to be an affordable, end-to-end solution for people with disabilities to utilize smart devices in their surroundings through a variety of accessible input devices that caters to their individual needs in a plug-and-play fashion. This is all done through the user interface of BuddyHub, which was designed with a special needs audience in mind. For instance, a user with limited motion capabilities could control smart appliances, robotic arms and other forms of hardware they might need, as long as they are integrated into BuddyHub's system. BuddyHub interacts with third-party devices via HTTP requests to the RESTful API endpoints of integrated devices which are supplied by developers through our developer portal. This project also aims to promote the development of assistive technology by providing a platform for developers and companies to build on and integrate their products with. The ultimate goal is to make such technologies more affordable as well as integrate them with the Internet of Things for the benefit of end users. As more devices become interconnected, this universal controller platform can act as a central hub of control for these devices, giving users with various disabilities the ability to utilize a wide range of devices that cater to their individual needs. The team that developed the platform focused on establishing the groundwork and software architecture to achieve the above goals. The first iteration of the product demonstrates the viability and usefulness of such a universal and accessible software platform that can be used for controlling technologies in a familiar and easy manner by people with disabilities. Due to time constraints, not all features we wanted to implement / issues we wanted to fix were resolved; the team hopes that a community of developers / other students could collaborate and improve on what has been done so far. Installation and Setup BuddyHub currently has web and desktop (Windows) versions, both of which are functional and augment the intent of having a user-friendly and accessible UI for end users. To access the web version of BuddyHub, please visit BuddyHub (Web). To install the Windows version, follow the steps below:  Download zip file from here. Extract the zip file to desired location. Among all other files, there should be two separate executables that include the server and the desktop UI. To run BuddyHub, run server.exe followed by UCUI.exe to launch the UI.  Guide to BuddyHub's UI Detailed below are the accessibility settings that we have implemented on the web and desktop versions of BuddyHub's UI. This should assist future developers should they wish to edit current / add new accessibility features. To configure the accessibility settings of BuddyHub, check out this guide. BuddyHub (Web)  BuddyHub (Desktop)  List of Compatible Devices (Updated: 30/8/18) To demonstrate the universal plug-and-play capabilities of the prototype that the team developed, we cross-tested the universal controller platform with the following input and output devices: Input Devices: (Any input devices that are natively mapped to a PC's mouse and keyboard inputs can be used to interact with BuddyHub's UI.)  Tobii Eye Tracker 4C Most accesible joysticks (we used Slimline Joystick) Buddy buttons  Output Devices:  Web-based Voice Synthesizer for Alexa (allows a user with limited vocal capabilities to interact with Alexa through BuddyHub) Philips Hue Go (API Documentation) Lynxmotion AL5D PLTW robotic arm (we wrote a C# API to interact with the robotic arm's native firmware)  Add a Device (Developer Documentation) Wish to integrate a device with BuddyHub? Requirements:  RESTful Web API that accepts HTTP requests (for devices with a TCP/UDP connection) C# wrapper that implements our IDevice interface (for USB devices) Easy-to-understand device names and descriptions for hardware functionality Supports API authentication via OAuth 2.0 and hardware authentication (to be implemented)  Guide:  Visit BuddyHub@Developers to add a new device Choose to add a device with either a RESTful API or a C# interface  Link your project with UCProtocol and UCUtility projects under ./BuddyHub/ C# libraries have to implement an C# IDevice interface In your main device object source file, link your project to System.ComponentModel using System.ComponentModel.Composition Export your main object class using [Export(typeof(CSharpServer.IDevice))]   Fill up the form with user-friendly names and descriptions (these will be what users see in the main BuddyHub UI) Specify the HTTP methods and resources that BuddyHub will use to interact with your device Select if a text input interface is required for your device Submit your device for approval by the admin team and we'll get back to you as soon as possible  Device suggestions: Users can also suggest devices to be added to BuddyHub by clicking on the ""Suggest Device"" button on BuddyHub's UI. Simply just fill in the form and the details will be sent for a review. The admin team will look into adding the suggested devices on a regular basis. Future Improvements Wish to contribute to BuddyHub? Fork the repo and follow the installation guide as shown here. List of features we wish to implement but have not: Accesibility  One and two-switch modes for navigation Tremor filtering for push buttons Adjustable wait time for click-on-hover option Accesibility options for the visually impaired such as screen reader support Implementing clear and intuitive button icons for the web interface Ability to save user settings and preferences Ability to add futher language options Integration with other softwares and open APIs to contain day-to-day applications within BuddyHub's interface (e.g. instant-messaging apps, Fitbit etc.) Integration of an AAC (Augmentative and Alternative Communication) board alongside the current text-to-speech capability Setting up of macros Android and iOS mobile app More audio-visual feedback options  Security  API authentication via OAuth 2.0 Hardware authentication to register device's IP Address GDPR compliance to ensure user's data security  Development  An interface for admins to approve and process device suggestions and submissions Improve on the workflow for developers to integrate new devices to BuddyHub CLI (Command Line Interface) tool for the integration of new devices Adding more devices to BuddyHub and possible integration with other smart hubs such as Apple HomeKit and Samsung SmartThings Allow the desktop UI to render buttons dynamically based on delevopers' specifications as they are currently rendered statically  List of issues can be found here. Contributors and Acknowledgements This project was initiated and funded by Wooden Spoon: The Children's Charity of Rugby, and was developed by a team of 10 undergraduate students from Imperial College London under the supervision and guidance of Dr. Ian Radcliffe from Imperial College London, Department of Bioengineering. The team comprised of Maksim Lavrov, May McCool, Terence Seow, Rachel Tan, Yong Shen Tan, Sung Kyun Yi, Fiona Boyce, Joan Tso, Balint Hodossy and Husheng Deng. The team would like acknowledge the fair use of code or code snippets of the following people:  Rémy Dispagne Lynxmotion SSC32/AL5x Robotic Arm Library David Simple C# Web Server Sagar Pardeshi Detect Insertion and Removal of USB Drive C#  The team would like to thank the following people for their contributions and advice to the team throughout the course of the project:  Mahendran Subramanian, Balasundaram Kadirvelu and Pavel Orlov from Dr. Faisal's Research Group, Imperial College London Paschal Egan and Niraj Kanabar from Imperial College London, Department of Bioengineering Dr. Thomas Clarke and Professor Cheung, Peter Y from Imperial College London, Department of Electrical and Electronics Engineering Ben Crundwell from Cambridge Design Partnership Barrie Ellis from Special Effects Charlie Danger, Simon Bull, Christian Dryden, Rachel Moore, Hélio Lourenço and Diane Arthurs from ACE Center  Contact Should you have any queries about the above project or wish to find out more about how you could contribute, please kindly reach out to us at wsurop2018@gmail.com.","d motion capabilities could control smart appliances, robotic arms and other forms of hardware they might need, as long as they are integrated into buddyhub's system. buddyhub interacts with third-party devices via http requests to the restful api endpoints of integrated devices which are supplied by developers through our developer portal. this project also aims to promote the development of assistive technology by providing a platform for developers and companies to build on and integrate their products with. the ultimate goal is to make such technologies more affordable as well as integrate them with the internet of things for the benefit of end users. as more devices become interconnected, this universal controller platform can act as a central hub of control for these devices, giving users with various disabilities the ability to utilize a wide range of devices that cater to their individual needs. the team that developed the platform focused on establishing the groundwork and software architecture to achieve the above goals. the f robotrobotic arms and other forms of hardware they might need, as long as they are integrated into buddyh",6,3,479,2,1,0,1
usnistgov/ucef,https://github.com/usnistgov/ucef,6,2017-06-05T10:05:15Z,2017-06-05,10:05:15Z,2017,2020-10-19T07:31:15Z,2020-10-19,07:31:15Z,2020,Universal CPS Environment for Federation,"UCEF Background on UCEF and Cyber-Physical Systems Cyber-Physical Systems (CPS) are smart systems that include co-engineered interacting networks of physical and computational components [1].  CPS integrate computation, communication, sensing and actuation with physical systems to fulfill time-sensitive functions with varying degrees of interaction with the environment, including human interaction.  These highly interconnected systems provide new functionalities to improve quality of life and enable technological advances in critical areas, such as personalized health care, emergency response, traffic flow management, smart manufacturing, defense and homeland security, and energy supply and use.  CPS and related systems (including the Internet of Things (IoT) and the Industrial Internet) are widely recognized as having potential to enable innovative applications and impact multiple economic sectors in the worldwide economy [2]. The impacts of CPS will be revolutionary and pervasive – this is evident today in emerging smart cars, intelligent buildings, robots, unmanned vehicles and medical devices [3]. The development of these systems cuts across all industrial sectors and demands high-risk, collaborative research between research and development teams from multiple institutions. Realizing the future promise of CPS will require interoperability between heterogeneous systems and development processes supported by robust platforms for experimentation and testing across domains. Meanwhile, current design and management approaches for these systems are domain-specific and would benefit from a more universally applicable approach. Cyber-Physical Systems (CPS) experimentation suffers from isolated simulation tools and many cross-platform custom adaptors which increases complexity and cost. Yet the demand for more sophisticated experiments and configurations is growing. The National Institute of Standards and Technology (NIST) and its partner, the Institute for Software Integrated Systems at Vanderbilt University [4], have developed a collaborative experiment development environment across heterogeneous architectures integrating best-of-breed tools including programming languages, communications co-simulation, simulation platforms, hardware in the loop, and others. This environment combines these simulators and emulators from many researchers and companies with a standardized communications protocol, IEEE Standard 1516 High Level Architecture (HLA) [5]. NIST calls this a Universal CPS Environment for Federation (UCEF). UCEF is provided as an open source toolkit that:  comprises a portable, self-contained, Linux Virtual Machine which allows it to operate on any computing platform; contains a graphical experiment and federate design environment – Web-based graphical modeling environment (WebGME) developed by Vanderbilt University that provides code generation for adapting models to the simulators; defines a language for exercising the collection of federates, the federation, in the course of theexperiment; separates the design of experiments from the design of the models composed in an experiment; manages its own scope in the definition of federated interfaces, federations, experiments – but not model design and implementations; develops experiments that can be deployed independently on a variety and combination of platforms from large cloud systems to small embedded controllers; allows experiments to be composed among local simulations, hardware in the loop (HIL), cloud simulations, and collaborative experiments across the world; integrates federates designed in (expected as of this workshop): Java, C++, Omnet++, Matlab, LabView, Gridlab-D.  To Build the Virtual Machine or Download One See Build which contains code to allow for the automated generation of the UCEF Virtual Machine UCEF Website Follow this link to the UCEF Collaboration Website","oday in emerging smart cars, intelligent buildings, robots, unmanned vehicles and medical devices [3]. the development of these systems cuts across all industrial sectors and demands high-risk, collaborative research between research and development teams from multiple institutions. realizing the future promise of cps will require interoperability between heterogeneous systems and development processes supported by robust platforms for experimentation and testing across domains. meanwhile, current design and management approaches for these systems are domain-specific and would benefit from a more universally applicable approach. cyber-physical systems (cps) experimentation suffers from isolated simulation tools and many cross-platform custom adaptors which increases complexity and cost. yet the demand for more sophisticated experiments and configurations is growing. the national institute of standards and technology (nist) and its partner, the institute for software integrated systems at vanderbilt university [4], have developed a collaborative expe robotbots, unmanned vehicles and medical devices [3]. the development of these systems cuts across all in",5,13,189,0,1,1,0
pyaiot/pyaiot,https://github.com/pyaiot/pyaiot,11,2017-04-21T07:41:48Z,2017-04-21,07:41:48Z,2017,2020-03-21T18:39:14Z,2020-03-21,18:39:14Z,2020,A set of Python services to interact and transport data from IoT devices,"Pyaiot, connecting small things to the web  Pyaiot provides a set of services for interacting and transporting data coming from IoT devices using regular web protocols (HTTP). Pyaiot relies on Python asyncio core module and on other more specific asyncio based packages such as Tornado, aiocoap or HBMQTT. Pyaiot tries to only use standard protocols to connect the IoT devices to the web: CoAP, MQTT, HTTP, etc The devices Pyaiot main goal is to provide high level services for communicating with constrained devices. Those devices are generally microcontrollers and thus not able to run Linux. Thus, we need a specific OS to run on those kind of devices. For this, we initially chose RIOT because it provides an hardware independent layer along with the standard network stacks required to communicate with the devices from a network. The source code of RIOT firmwares running on the devices is available in another repository on GitHub. Other devices with communication capabilities can also be used. In this repository, we also provide a Micropython script that can be used on Pycom devices. This script only works with the mqtt gateway service. Available services Pyaiot is built around several microservices:  A public central broker A public web application for the dashboard Private distributed gateways   The role of the broker is to put in relation gateways and web clients in order to be able to transfer in a bi-directionnal way messages coming from devices, via the gateways, to clients and vice versa. The broker is in charge of the management of the list of gateways. The role of the gateways is to convert protocols used by the devices to the web protocols used internally by Pyaiot to transfer information between the different services. In order to guarantee reactivity and security, this internally used protocols rely on HTTP websockets. The Dashboard is a web page with some embbeded javascript that displays the list of available devices and their status. It also allows to interact with the devices (LED control, Robot control, etc)  3 examples of gateways are provided by pyaiot:  A CoAP gateway that manages a list of alive sensor devices by running its own CoAP server A MQTT gateway that manages a list of alive sensor devices by subscribing and publishing messages to a MQTT broker. A Websocket gateway dedicated to devices: each node is connected via a websocket  The CoAP gateway Here we describe how the CoAP gateway interacts with devices. When a node starts, it notifies itself to its gateway by sending a CoAP post request. On reception, the gateway converts and forwards this message to the broker server. In the mean time, the gateway initiates a discovery of the resources provided by the node (using the CoAP .well-known/core resource). Once available resources on the node are known, the gateway sends to the broker update messages. The broker simply broadcasts those notification messages to all connected web clients. To keep track of alive devices, each node has to periodically send a notification message to its gateway. If a sensor node has not sent this notification within 120s (default, but this is configurable), the gateway automatically removes it from the list of alive devices and notifies the broker. The MQTT gateway MQTT do things differently from CoAP: the devices and the gateway have to publish or subscribe to topics to exchange information. A resource discovery mechanism is also required for the gateway to determine the list of available resources on a node. In Pyaiot, the gateway and the devices are used as clients of the same MQTT broker. Since the devices are constrained, we decided to use the mosquitto.rsmb broker. Some documentation and a sample systemd service file is provided in the pyaiot/gateway/mqtt directory. With the MQTT gateway, each node is identified by a <node_id> and this identifier is used to build the topics specific to a given node. Topics the devices publish to/the gateway subscribes to are:  node/check with payload {'id': <node_id>}. Once started, each node publishes periodically (every 30s) on this topic. node/<node_id>/resources with payload [<resource 1>, <resource 1>]. This comes as a reply to the gateway request for resources available on a given node. node/<node_id>/<resource> with payload {'value': <resource_value>}. Depending on the type of resource, a node can publish periodically or not or on demand on this topic.  Topics the devices subscribe to/the gateway publishes to are:  gateway/<node_id>/discover with 2 possible payloads:  resources: then the node publish on node/<node_id>/resources values: then, for each resource, the node publish on node/<node_id>/<resource>   gateway/<node_id>/<resource>/set with a payload depending on the resource: update the value of the resource on the node (LED toggle, text, etc)  The websocket gateway The behavior with a websocket gateway is similar to the CoAP gateway except that the node doesn't have to send notifications periodically: the node is lost when the connection is closed. Available Demos See Pyaiot in action within 2 demos:   RIOT: You can find a permanent demo instance configured as a showroom for RIOT. This showroom is available at http://riot-demo.inria.fr.   IoT-LAB open A8 demo This demo automatically submits an experiment on IoT-LAB with two open A8 devices. The first node is configured as a border router and the second node runs a firmware that integrates automatically in the RIOT Demo Dashboard described above.   Usage  Clone this repository  $ git clone https://github.com/pyaiot/pyaiot.git   Install the command line tools (only Python 3 is supported)  $ pip3 install . --user   Generate authentication keys  $ aiot-generate-keys   Start the broker  $ aiot-broker --debug 2019-04-08 16:11:58,816 -  pyaiot.broker -  INFO - Application started, listening on port 8000  You can get more information on available option using --help: $ aiot-broker --help [...] --broker-host                    Broker host (default localhost) --broker-port                    Broker websocket port (default 8000) --config                         Config file --debug                          Enable debug mode. (default False) --key-file                       Secret and private keys filename. (default                                  /home/<user>/.pyaiot/keys)   Start one of the gateways, let's say the coap gateway  $ aiot-coap-gateway --debug --coap-port=5684  By default the CoAP server running with gateway is using port 5683, but here we specify another one, to not conflict with the test coap node that will be started below.  Start a CoAP test node, just for testing if you don't have a real hardware. This test node simulates a compatible interface and is handy for debugging purposes.  $ python3 utils/coap/coap-test-node.py --gateway-port=5684  --temperature --pressure   Setup and start a local web dashboard  $ cd pyaiot/dashboard/static $ npm install $ aiot-dashboard --debug  Then open http://localhost:8080/ in your web browser. Dashboard local development against an external IoT broker instance Here we take as example the online demo available at http://riot-demo.inria.fr. The websocket server of the broker service is reachable on port 80. As the broker and the dashboard are decoupled in 2 distinct services, it's possible to run a local dashboard application serving dashboard web page that itself connects to the broker. This way your dashboard will display the available devices on the online RIOT demo. Then you can start the dashboard application: $ aiot-dashboard --debug --broker-host=riot-demo.inria.fr --broker-port=80  and open a web browser at http://localhost:8080. When the web page is loaded, it directly connects to the broker websocket server and starts communicating with the devices.","ilable devices and their status. it also allows to interact with the devices (led control, robot control, etc)  3 examples of gateways are provided by pyaiot:  a coap gateway that manages a list of alive sensor devices by running its own coap server a mqtt gateway that manages a list of alive sensor devices by subscribing and publishing messages to a mqtt broker. a websocket gateway dedicated to devices: each node is connected via a websocket  the coap gateway here we describe how the coap gateway interacts with devices. when a node starts, it notifies itself to its gateway by sending a coap post request. on reception, the gateway converts and forwards this message to the broker server. in the mean time, the gateway initiates a discovery of the resources provided by the node (using the coap .well-known/core resource). once available resources on the node are known, the gateway sends to the broker update messages. the broker simply broadcasts those notification messages to all connected web clients. to keep track of alive devices, each node has to periodically send a notification message to its gateway. if a sensor node has not sent this notification within 120s (default, but this is configurable), the gateway automatically removes it from the list of alive devices and notifies the broker. the mqtt gateway mqtt do things differently from coap: the devices and the gateway have to publish or subscribe to topics to exchange information. a resource discovery mechanism is also required for the gateway to determine the list of available resources on a node. in pyaiot, the gateway and the devices are used as clients of the same mqtt broker. since the devices are constrained, we decided to use the mosquitto.rsmb broker. some documentation and a sample systemd service file is provided in the pyaiot/gateway/mqtt directory. with the mqtt gateway, each node is identified by a <node_id> and this identifier is used to build the topics specific to a given node. topics the de roboteract with the devices (led control, robot control, etc)  3 examples of gateways are provided by pya",5,28,463,10,1,0,1
NVIDIA-AI-IOT/Electron,https://github.com/NVIDIA-AI-IOT/Electron,20,2017-06-27T22:38:32Z,2017-06-27,22:38:32Z,2017,2020-12-11T03:20:26Z,2020-12-11,03:20:26Z,2020,An autonomous deep learning indoor delivery robot made with Jetson,"The code in this repository has only been tested on the NVIDIA Jetson TX2. For installation instructions please look to installation guide in the wiki here The project overview This is the GitHub repo for Electron, an indoor delivery robot. The goal of the robot is to deliver items to cubicles in an office building, be it office supplies or food. The robot must be able to autonomously drive around a building while successfully retrieving and delivering these items constantly upon user request. We do this using ROS, deep learning, and various sensors. Hardware Expectations  Turtlebot RPLidar A1 Jetson TX2 Logitech C920 Webcam USB Hub (optional)     Navigating on Turtlebot In order to move around a building autonomously, our robot creates a map of the building using SLAM (gmapping) with an RPLidar A1 and odometry. It later utilizes the same map to navigate throughout the building by localizing itself using AMCL (adaptive monte carlo localization). We do most of this using ROS (Robot Operating System) and RVIZ for visualization.    Making your delivery bot more aware: Using ItemNet trained on top of GoogleNet We added a neural network for the robot to know what kind of object is placed on it and when an object is placed or taken off- so it can act accordingly. ItemNet is a neural network trained on NVIDIA DIGITS with the Caffe framework on high-end NVIDIA GPUs. It has the ability to classify various classes of objects you find in an office space. We were able to learn how to do this from the ground up by following Dusty's tutorial on Jetson Inference. https://github.com/dusty-nv/jetson-inference Below is an example of the visualization of the training of one of our models. We used transfer learning with GoogleNet as a base network to train upon for our image classification. We also used data from the ImageNet Challenge to train. You can access this data by going to the ImageNet Challenge website. All information about how to train such models is on the linked repo above, although we included models in this repo that can be implemented directly on your bot.    User Interface: using a slackbot In our project, we use a slackbot to request the robot to go somewhere. We thought this was a simple and easy way anyone in an office environment could request the bot or some other food or item. We used the slackclient api.","s the github repo for electron, an indoor delivery robot. the goal of the robot is to deliver items to cubicles in an office building, be it office supplies or food. the robot must be able to autonomously drive around a building whi robotot. the goal of the robot is to deliver items to cubicles in an office building, be it office suppli",3,59,105,0,1,0,1
NVIDIA-AI-IOT/jetson-trashformers,https://github.com/NVIDIA-AI-IOT/jetson-trashformers,22,2017-06-20T20:20:30Z,2017-06-20,20:20:30Z,2017,2020-10-16T11:38:08Z,2020-10-16,11:38:08Z,2020,Autonomous humanoid that picks up and throws away trash,"What is this project about? Our project uses neural networks to train the Robotis BioloidGP to detect trash and throw it away in trash cans, effectively keeping the office environment clean. The current stage of the model allows for detection of white cups (as trash) in various backgrounds, as well as trashcan symbols. The purpose of this project is to provide a use-case example for developers who may wish to use the Jetson™ platform. The code in this repository has only been tested on the NVIDIA Jetson TX2. How can I run this project? git clone https://github.com/NVIDIA-Jetson/jetson-trashformers.git cd jetson-trashformers make sh runDetect.sh  The first three commands clone and compile the program. The last command runs a script which starts the program. This program can only be run on the Jetson TX2. When runDetect.sh is run, the robot's webcam is activated and begins searching for a cup. If no cup is found, it will turn and scan around until it finds a cup. Once it has done so, it will walk towards the cup, find its orientation, and attempt to pick it up once within range. The robot then scans for a trashcan symbol and will drop the cup in the trashcan (altered to account for the robot's size). Watch a demo of the runDetect.sh command What is TrashNet? TrashNet is the neural network that we have created in order to detect cups and trashcans. First, a network named CupNet was created with images of only cups and false positives in order to teach the robot about cups. Once a well-defined model was achieved, we added images of trashcans to teach the robot to detect those, while building off its previous knowledge about cups. This neural network has been created and trained on NVIDIA DIGITS using the Caffe framework. We used the help of Dustin Franklin's Jetson Inference tutorial to learn more about using DIGITS and creating our own neural network. To learn more about single and multi-class detection and creating networks with custom data, visit our wiki.     This graph shows the multi-class model's statistics during the training period.      The model learns to draw bounding boxes around cups through training.  Hardware Expectations?  Robotis BioloidGP Connect Tech Orbitty Carrier Board Jetson™ TX2 Logitech C270 Webcam USB2Dynamixel Zig2Serial  Find a Hardware setup guide here. Find a Linux 4 Tegra® with Connect Tech Carrier Board setup guide here.    Libraries Used See 'lib' folder for the specific files.  libdetectnet-camera.so   A shared object library with an edited and compiled version of detectnet-camera.cpp from Dustin's github.   libdxl_sbc_cpp.so  A shared object library for the Dynamixel servos.   libjetson-inference.so  A shared object library of Dustin's jetson-inference.   libzgb.a  A shared object library to control robot commands via ZigBee. (This library is downloaded as a dependancy from the ROBOTIS website)       Authors (Left to Right from picture above)  Ishan Mitra Shruthi Jaganathan Mark Theis Michael Chacko  Licenses Used  ROBOTIS:   SDK OBTAINED FROM https://github.com/ROBOTIS-GIT/DynamixelSDK on June 29 2017 Copyright (c) 2016, ROBOTIS CO., LTD. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:   Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.   Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.   Neither the name of ROBOTIS nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.   ZigBee   Source code available at http://support.robotis.com/en/software/zigbee_sdk/zig2serial/linux.htm No license was found as of June 29 2017.","ut? our project uses neural networks to train the robotis bioloidgp to de robottis bioloidgp to detect trash and throw it away in trash cans, effectively keeping the office enviro",4,44,146,0,1,3,1
iotJumpway/Intel-Examples,https://github.com/iotJumpway/Intel-Examples,27,2017-01-19T22:52:01Z,2017-01-19,22:52:01Z,2017,2020-11-02T20:31:18Z,2020-11-02,20:31:18Z,2020,Get started with the Internet of Things & Artificial Intelligence using Intel® technologies and the iotJumpWay.,"IoT JumpWay Intel® Examples  Introduction The IoT JumpWay is an IoT platform that provides a high performance, scalable and efficient solution for IoT developers and Makers. The platform provides the fundamental services that allow you to securely distribute data from IoT devices and manage and control them securely via applications. The platform provides an IoT PaaS for IoT communication, tutorials and examples. The IoT JumpWay Intel® examples provide example projects that you can use to get started using the IoT JumpWay for your Intel® IoT projects. Connect Intel® devices and sensors to the IoT JumpWay and control/monitor sensors/actuators and data to and from the devices. Intel® AI DevCloud Tutorials & Source code  ""The Intel® AI DevCloud is a cluster of Intel® Xeon® Scalable Processors that will assist you with your machine learning and deep learning training and inference compute needs. It provides access to precompiled software optimized for Intel® architecture on Intel® Xeon® Scalable Processors.""  Python: Intel® AI DevCloud TASS Trainer. Train TASS on the Intel® AI DevCloud platform.  Intel® Movidius Examples  ""Intel® Movidius™ VPUs drive the demanding workloads of modern computer vision and AI applications at ultra-low power. By coupling highly parallel programmable compute with workload-specific hardware acceleration, and co-locating these components on a common intelligent memory fabric, Movidius achieves a unique balance of power efficiency and high performance. Movidius technology allows device makers to deploy deep neural network and computer vision applications in categories such as smartphones, drones, intelligent cameras and augmented reality devices.""   TASS Movidius Classifiers.  The TASS Movidius Classifiers are computer vision programs using a number of different models including Inception V3, Yolo, dlib, OpenCV and Facenet.   Breast Cancer Classification Using Computer Vision & IoT. Breast Cancer Classification Using Computer Vision & IoT combines Computer Vision and the Internet of Things to provide researchers, doctors and students with a way to train a neural network with labelled breast cancer histology images to detect Invasive Ductal Carcinoma (IDC) in unseen/unlabelled images.   Intel® Computer Vision SDK Examples  ""The Intel® Computer Vision SDK is a comprehensive toolkit that can be used for developing and deploying computer vision solutions on Intel® platforms, including autonomous vehicles, digital surveillance cameras, robotics, and mixed-reality headsets.""   Windows Console: TASS PVL Webcam. A Computer Vision security system using the Intel® CV SDK & webcam.   Windows Console: TASS PVL RealSense. A Computer Vision security system using the Intel® CV SDK & RealSense.   Intel® Arduino 101 DFRobot Examples    Arduino: LCD Control Example. Set up an Arduino 101 IoT device that can control other IoT devices on the same network using the LCD Keypad Shield and communication via the IoT JumpWay.   Arduino: LCD Intruder System Example. Set up an Arduino 101 intruder alarm system that is controlled by the DFRobot LCD Keypad Shield and communication via the IoT JumpWay.   Intel® Arduino 101 Examples   Arduino: Basic LED Example. Set up an Arduino/Genuino 101 that allows control of an LED, and also an application that can control the LED via the IoT JumpWay.  Intel® Edison Examples    Python: Basic LED Example. Set up an Intel® Edison that allows control of an LED, and also an application that can control the LED via the IoT JumpWay.   Python: Dev Kit LED Example. Set up an Intel® Edison that allows control of an LED on an IoT Dev Kit and an application that can control the LED via the IoT JumpWay.   Python: Dev Kit IoT Alarm, Set up an Intel® Edison IoT alarm system that is controlled by the IoT JumpWay.   Intel® Galileo Examples    Python: Basic LED Example. Set up an Intel® Galileo that allows control of an LED, and also an application that can control the LED via the IoT JumpWay.   Python: Dev Kit LED Example. Set up an Intel® Galileo that allows control of an LED on an IoT Dev Kit and an application that can control the LED via the IoT JumpWay.   Bugs/Issues Please feel free to create issues for bugs and general issues you come across whilst using this or any other Intel® related IoT JumpWay issues. You may also use the issues area to ask for general help whilst using the IoT JumpWay in your IoT projects. Contributors"," platforms, including autonomous vehicles, digital surveillance cameras, robotics, and mixed-reality headsets.""   windows console: tass pvl webcam. a computer vision security system using the intel® cv sdk & webcam.   windows console: tass pvl realsense. a computer vision security system using the intel® cv sdk & realsense.   intel® arduino 101 dfrobot examples    arduino: lcd control example. set up an arduino 101 iot device that can control other iot devices on the same network using the lcd keypad shield and communication via the iot jumpway.   arduino: lcd intruder system example. set up an arduino 101 intruder alarm system that is controlled by the dfrobot lcd keypad shield and communication via the iot jumpway.   intel® arduino 101 examples   arduino: basic led example. set up an arduino/genuino 101 that allows control of an led, and also an application that can control the led via the iot jumpway.  intel® edison examples    python: basic led example. set up an intel® edison that allows control of an led, and also an application that can control the led via the iot jumpway.   python: dev kit led example. set up an intel® edison that allows control of an led on an iot dev kit and an application that can control the led via the iot jumpway.   python: dev kit iot alarm, set up an intel® edison iot alarm system that is controlled by the iot jumpway.   intel® galileo examples    python: basic led example. set up an intel® galileo that allows control of an led, and also an application that can control the led via the iot jumpway.   python: dev kit led example. set up an intel® galileo that allows control of an led on an iot dev kit and an application that can control the led via the iot jumpway.   bugs/issues please feel free to create issues for bugs and general issues you come across whilst using this or any other intel® related iot jumpway issues. you may also use the issues area to ask for general help whilst using the iot jumpway in your iot projects. contributors robotveillance cameras, robotics, and mixed-reality headsets.""   windows console: tass pvl webcam. a comp",1,46,538,0,1,0,1
scramjs/johnny-five-elements,https://github.com/scramjs/johnny-five-elements,0,2016-11-04T22:29:30Z,2016-11-04,22:29:30Z,2016,2020-06-05T08:58:48Z,2020-06-05,08:58:48Z,2020,HTML custom elements for controlling hardware with the Johnny-Five JavaScript Robotics & IoT platform.,"Johnny-Five Elements HTML custom elements for controlling hardware with the Johnny-Five JavaScript Robotics & IoT platform. Build hardware declaratively! Introduction Here is the concept: <jfive-board>   <jfive-pin pin=""GPIO3"" input=""true""></jfive-pin>   <jfive-led pin=""GPIO21"" interval=""500"" brightness=""50""></jfive-led>   <jfive-button pin=""GPIO23"" on-push=""buttonPush""></jfive-button>   <jfive-motor on=""[[motorOn]]"" speed=""128"" pwm-pin=""GPIO18"" dir-pin=""GPIO21"" cdir-pin=""GPIO22"" reverse></jfive-motor> </jfive-board> We are currently working to support the entire Johnny-Five API. See which elements have been implemented in the Elements section. We could also use your help. To see what needs working on go to the What's Next? section. For more background information, see the following resources:  Universal Web Components article Universal Web Components video presentation Server-side Web Components article React Hardware article  Installation npm install --save johnny-five-elements Use These elements can be run on a variety of devices using Scram.js. Helpful instructions for specific devices are detailed below. Instructions for more devices may come in the future. Raspberry Pi You'll need to install the Raspberry Pi IO plugin: npm install --save raspi-io Indicate your use of the Raspberry Pi IO plugin from your top-level jfive-board component: <jfive-board io-plugin=""raspi-io""> </jfive-board> You must be the root user while using these components: sudo su To understand which pins are available for use on the Raspberry Pi 3, use the following pin layout: http://blog.mcmelectronics.com/post/Raspberry-Pi-3-GPIO-Pin-Layout#.WIN1XvErL0o Documentation for more devices and pin layouts may come later. Elements All of the elements listed below have been implemented with the properties, events, and methods described.  Board Button Led Motor Pin Switch  jfive-board Properties jfive-button Properties jfive-led Properties on: boolean Turns the LED on or off. pin: string The GPIO pin that the positive end of the LED is connected to. jfive-motor Properties pwmPin: string The GPIO pin used to enable, disable, and control the speed of the motor with a 3-pin h-bridge. dirPin: string One of the GPIO pins used to control the direction of the motor with a 3-pin h-bridge. cdirPin: string One of the GPIO pins used to control the direction of the motor with a 3-pin h-bridge. on: boolean Turns the motor on or off. speed: number Controls the speed of the motor. Must be a number between 0 and 255. reverse: boolean Controls the direction of the motor, either clockwise or counterclockwise. The direction will be relative to the direction of the current. You will most likely need a motor driver if you wish to control the direction of your motors. I highly recommend the L293D integrated circuit, which is a 3-pin h-bridge. To understand how to use the L293D: http://www.rakeshmondal.info/L293D-Motor-Driver https://business.tutsplus.com/tutorials/controlling-dc-motors-using-python-with-a-raspberry-pi--cms-20051 http://www.instructables.com/id/How-to-use-the-L293D-Motor-Driver-Arduino-Tutorial/step2/The-Circuit/ http://arduinoguides.blogspot.com/2012/06/using-l239-motor-driver.html jfive-pin Properties jfive-switch Properties Examples Here are some example applications written with Johnny-Five Elements:  https://github.com/scramjs/web-copter  What's Next? Here are the things we are working towards. An item is being worked on if there is a check next to Active development by. If you would like to contribute, open an issue with the name of an item that is not being worked on and we'll have it assigned to you:   Accelerometer component   Active development by:   Discussion under:    Altimeter component   Active development by:   Discussion under:    Animation component   Active development by:   Discussion under:    Barometer component   Active development by:   Discussion under:    Board component   Active development by: lastmjs   Discussion under: #16    Button component   Active development by: bdemann   Discussion under: #17    Compass component   Active development by:   Discussion under:    ESC component   Active development by:   Discussion under:    Expander component   Active development by:   Discussion under:    GPS component   Active development by:   Discussion under:    Gyro component   Active development by:   Discussion under:    Hygrometer component   Active development by:   Discussion under:    IMU component   Active development by:   Discussion under:    Joystick component   Active development by:   Discussion under:    Keypad component   Active development by:   Discussion under:    LCD component   Active development by:   Discussion under:    Led component   Active development by: bdemann   Discussion under: #1    Led-Digits component   Active development by:   Discussion under:    Led-Matrix component   Active development by:   Discussion under:    Led-RGB component   Active development by:   Discussion under:    Light component   Active development by:   Discussion under:    Motion component   Active development by:   Discussion under:    Motor component   Active development by:   Discussion under:    Multi component   Active development by:   Discussion under:    Piezo component   Active development by:   Discussion under:    Pin component   Active development by: lastmjs   Discussion under: #15    Proximity component   Active development by:   Discussion under:    Relay component   Active development by:   Discussion under:    Sensor component   Active development by:   Discussion under:    Servo component   Active development by:   Discussion under:    ShiftRegister component   Active development by:   Discussion under:    Stepper component   Active development by:   Discussion under:    Switch component   Active development by: bdemann   Discussion under: #19    Thermometer component   Active development by:   Discussion under:    Property-based testing structure   Active development by: lastmjs   Discussion under: #18    Acknowledgements Raspberry Pi is a trademark of the Raspberry Pi Foundation Node.js is a trademark of Joyent, Inc. and is used with its permission. We are not endorsed by or affiliated with Joyent.",trolling hardware with the johnny-five javascript robotics & iot platform. build hardware declarat robottics & iot platform. build hardware declaratively! introduction here is the concept: <jfive-board>  ,2,12,111,13,1,0,1
ms-iot/virtual-shields-universal,https://github.com/ms-iot/virtual-shields-universal,32,2015-04-29T07:38:12Z,2015-04-29,07:38:12Z,2015,2020-01-02T21:11:13Z,2020-01-02,21:11:13Z,2020,Virtual Shields Universal Windows App,"Windows Virtual Shields for Arduino (Universal Windows App) This project lets your Arduino sketch control your Windows 10 device (phone). The only code you write is to an Arduino sketch. This repository is a Universal Windows Application which exposes sensors and capabilities to a Windows Virtual Shields for Arduino library. Windows Virtual Shields for Arduino is an open-source library primarily for the Arduino UNO which communicates with this open-source Windows Universal Application running on all Windows 10 devices, primarily focusing on Windows Phones.  The library exposes the Phones's sensors and capabilities to the an Arduino Wiring Sketch. The following sensors and capabilities can be used from Arduino Wiring: Sensors:  Accelerometer Compass Geolocator (GPS) Gyrometer Light Sensor Orientation  Capabilities:  Camera Device info (name, date/time/timezone, os) Email (initiation) Microphone Notifications (Tile/Toast) Screen (Text, Images, Audio/Video, Rectangles, Buttons, Touchscreen) Sms (initiation) Speech to Text and Speech Recognition  can receive table data from previous web search   Vibration Web (Get and Post with result parsing)  XPath JSON Simple text Regular expressions Table iteration (keys/values saved)    Within your Arduino Wiring Sketch, you can enable the following scenarios:  Place buttons on your Windows Phone's screen which directly affect pins, motors on your Arduino.  Shields used:  Screen     Send data gathered on the Arduino (Weather Station) to the cloud through the phone's data connection. or check a web page for a weather status and open or close windows in your house.  Shields used:  GPS Web     Turn the phone's accelerometer into a remote driving controller for an Arduino robot.  Shields used:  Accelerometer     Ask questions and based on voice responses, control pins to turn on lights or motors.  Shields used:  Text to Speech and Speech Recognition.     Security Cam: Sense movement with an Arduino ultrasonic sensor, take a picture, record audio, upload to Azure and alert you.  Shields used:  Web Camera Microphone      Getting Started : Hardware What you need  Arduino Uno or compatible device. Bluetooth module: SparkFun BlueSMiRF Silver (https://www.sparkfun.com/products/12577) and 4 wires to connect. Windows 10 phone (Lumia 520, Lumia 635)  Set up your Arduino using the Windows Virtual Shields for Arduino library   Prepare the Bluetooth module if necessary (the Bluetooth module may need to have headers soldered onto it).   Except for one difference below, connect the Bluetooth module to the Arduino per your wiring diagram (BlueSMiRF wiring diagram). DIFFERENCE: Use pins 0 and 1 instead of 2 and 3: The Bluetooth TX should connect to pin 0 (Arduino RX). The Bluetooth RX should connect to pin 1 (Arduino TX).   Set up your Windows 10 phone  Windows 8 users can get the ""Windows Insider"" application from the app store. This allows the user to opt into receiving Windows 10 Technical Previews as updates. Pair the Bluetooth device in the Bluetooth settings. BlueSMiRF default pin code is 1234. NOTE: The red blinking light on the BlueSMiRF continues to blink red after a successful pairing. This is expected. It only turns green after a connecting with the application. When pairing you should see a device name like ""RNBT-76BC"" the last four characters are the last four characters of the MAC address printed on the sticker of the module.  Getting Started : Software What you need  Arduino IDE 1.6 or better. ArduinoJson library. This repository. Visual Studio 2015 to sideload UWA (phone app) onto developer unlocked phone.  Set up your Arduino IDE using the Windows Virtual Shields for Arduino library  Download and install the Arduino IDE.  Set up ArduinoJson library using the Windows Virtual Shields for Arduino library  From the ArduinoJson repository, branch the reporsitory or download the zip. Place the whole repository into your libraries folder (i.e. Documents\Arduino\libraries\ArduinoJson).  Set up this repository. using the Windows Virtual Shields for Arduino library  Branch this repository or download the zip. Copy the Arduino/libraries/VirtualShield folder from your repository to your Arduino library (i.e. Documents\Arduino\libraries\VirtualShield).  Set up your Visual Studio 2015  Get the Windows 10 Technical Preview tools, including Visual Studio 2015 from dev.windows.com. Load the Shield.sln from this repository (/Shield/Shield.sln). Ensure your phone is developer-unlocked run the Windows Phone Developer Registration tool installed with the tools. Deploy to your device. Run the Virtual Shields for Arduino application. In the app settings, find your previously paired Bluetooth device and Connect.  Test your setup  From the Arduino IDE, go to the menu item File->Examples->Virtual Shields->Hello Blinky. This should load the Hello Blinky example. Before downloading, temporarily remove the Bluetooth TX and RX wires from the Arduino. (There is only one serial port shared between the USB and Bluetooth. The Bluetooth interferes with the download). Download the sketch. Replace the Bluetooth TX and RX wires into the Arduino pins. (Bluetooth TX to Arduino RX and Bluetooth RX to Arduino TX). Press the Reset button on the Arduino to restart the sketch. (You should see a message on your phone and be able to interact).  Troubleshooting:  Did you build a custom sketch? Remember to include “ shield.begin() “ in your setup():        shield.begin();      // for default 115200 baud rate  If nothing shows up, some devices require a baud of 57600, and even some at 9600. Try these in your sketch “ shield.begin(57600) “ in your setup().        shield.begin(57600); // for 57600 baud rate  Double check that your Bluetooth’s TX pin is connected to your Arduino’s RX pin and that the Bluetooth’s RX pin is connected to your Arduino’s TX pin per this diagram. Is there another strong broadcasting Bluetooth device around, like headphones or a Bluetooth mouse? This can interfere.  Arduino Wiring Sketch : Hello World example     #include <ArduinoJson.h>      #include <VirtualShield.h>     #include <Text.h>      VirtualShield shield;        // identify the shield     Text screen = Text(shield);  // connect the screen      void setup()     { 	    shield.begin();              // begin communication  	    screen.clear();              // clear the screen 	    screen.print(""Hello Windows Virtual Shields for Arduino"");	      }      void loop()     {     } === This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","se.  shields used:  gps web     turn the phone's accelerometer into a remote driving controller for an arduino robot.  shields used:  accelerometer     ask questions and based on voice responses, control pins to turn on lights or motors.  shields used:  text to speech and speech recognition.     security cam: sense movement with an arduino ultrasonic sensor, take a picture, record audio, upload to azure and alert you.  shields used:  web camera microphone      getting started : hardware what you need  arduino uno or compatible device. bluetooth module: sparkfun bluesmirf silver (https://www.sparkfun.com/products/12577) and 4 wires to connect. windows 10 phone (lumia 520, lumia 635)  set up your arduino using the windows virtual shields for arduino library   prepare the bluetooth module if necessary (the bluetooth module may need to have headers soldered onto it).   except for one difference below, connect the bluetooth module to the arduino per your wiring diagram (bluesmirf wiring diagram). difference: use pins 0 and 1 instead of 2 and 3: the bluetooth tx should connect to pin 0 (arduino rx). the bluetooth rx should connect to pin 1 (arduino tx).   set up your windows 10 phone  windows 8 users can get the ""windows insider"" application from the app store. this allows the user to opt into receiving windows 10 technical previews as updates. pair the bluetooth device in the bluetooth settings. bluesmirf default pin code is 1234. note: the red blinking light on the bluesmirf continues to blink red after a successful pairing. this is expected. it only turns green after a connecting with the application. when pairing you should see a device name li roboterometer into a remote driving controller for an arduino robot.  shields used:  accelerometer     as",11,46,136,0,1,1,1
nsynapse/cossb,https://github.com/nsynapse/cossb,0,2015-12-07T03:04:17Z,2015-12-07,03:04:17Z,2015,2020-02-04T10:51:55Z,2020-02-04,10:51:55Z,2020,Component-based Open & Simple Service Broker,"cossb Component-based Open & Simple Service Broker Features  IoT(Internet of Things) software framework for Robotic services. Service components can be downloaded/installed/uninstalled/updated dynamically responding to what devices have services in same network environment. Thus, our goal is that any services can be changed dynamically with no user or supervisor intervention. (Except the initial setting up). Fast by using C/C++. Lesser computational power might be required under the same condition. You can obtain more advantages for the device form-factor, budget and so on.. loose-coupled with other services by Service Broker. This gives advantages like a service re-configuration or re-combination) Not only for High performance computing machine(like a PC), It is going for the cheap embedded hardware devices like Raspberry Pi, Odroid..  Getting Started COSSB Usage COSSB application should be launched with administrative permission.  Run with manifest.xml  $ sudo cossb --run <your manifest.xml>   Show Version  $ sudo cossb --version   Help  $ sudo cossb --help  Supports  TCP Server using epoll (Linux only available)  Examples  Simple Hello World component UART component Simple Message Out component Simple Message Print component I2C component for Intel Edison GPIO component for Intel Edison UART component for Intel Edison  Dependencies  Protobuf 3.1 Boost C++ library (thread, filesystem)  License You can freely use, modify, redistribute this source code under the 3-clause BSD License. Some parts of the components and core libraries use open source code under the other license(LGPL, MIT..).",ures  iot(internet of things) software framework for robotic services. service components can be downloa robotobotic services. service components can be downloaded/installed/uninstalled/updated dynamically resp,3,0,569,0,1,1,1
iot-lab/cli-tools,https://github.com/iot-lab/cli-tools,11,2014-03-04T13:00:57Z,2014-03-04,13:00:57Z,2014,2020-12-17T16:17:34Z,2020-12-17,16:17:34Z,2020,IoT-LAB CLI tools,"IoT-Lab cli-tools       IoT-LAB cli-tools provide a basic set of operations for managing IoT-LAB experiments from the command-line.  License IoT-LAB cli-tools, including all examples, code snippets and attached documentation is covered by the CeCILL v2.1 free software licence.  Commands IoT-LAB cli-tools are available through a shared entrypoint, iotlab, Many subcommands are available:   Command Functions    iotlab auth configure account credentials  iotlab experiment start, stop, query experiments  iotlab node start, stop, reset nodes, update firmwares  iotlab profile manage nodes configurations  iotlab robot manage robot nodes  iotlab status manage informations about testbed sites, nodes and running experiments     Optional commands: When IoT-Lab SSH CLI Tools is installed:   iotlab ssh run commands on A8 open nodes through SSH    When IoT-Lab OML plot Tools is installed:   iotlab plot traj plot robot trajectory  iotlab plot consum plot node consumption  iotlab plot radio plot node sniffer results    When IoT-Lab Aggregation Tools is installed:   iotlab serial aggregate node serial link  iotlab sniffer aggregate node sniffer link    Commands are self-documented, and usually have sub-commands which are also self-documented. Use e.g: iotlab-node --help iotlab-profile add --help   Description The cli-tools leverage the IoT-LAB REST API and simply wrap calls to module iotlabcli, which is a Python client for the API. The cli-tools come as an installable Python package and require that module setuptools be installed before tools installation can happen. Please grab the relevant python-setuptools package for your distribution. To install cli-tools from Pypi, use pip install iotlabcli. To install cli-tools from source, use pip install --user . or python setup.py install Installing cli-tools automatically fetches additional dependencies as needed. Further documentation: https://github.com/iot-lab/iot-lab/wiki/CLI-Tools"," reset nodes, update firmwares  iotlab profile manage nodes configurations  iotlab robot manage robot nodes  iotlab status manage informations about testbed sites, nodes and running experiments     optional commands: when iot-lab ssh cli tools is installed:   iotlab ssh run commands on a8 open nodes through ssh    when iot-lab oml plot tools is installed:   iotlab plot traj plot robot trajectory  iotlab plot consum plot node consumption  iotlab plot radio plot node sniffer results    when iot-lab aggregation tools is installed:   iotlab serial aggregate node serial link   robotnodes configurations  iotlab robot manage robot nodes  iotlab status manage informations about testb",7,13,452,0,1,0,1
hybridgroup/cylon-joystick,https://github.com/hybridgroup/cylon-joystick,10,2013-12-06T20:56:06Z,2013-12-06,20:56:06Z,2013,2020-02-15T16:18:23Z,2020-02-15,16:18:23Z,2020,Cylon adaptor and driver for HID joysticks/controllers,"Cylon.js for Joysticks and Controllers Cylon.js (http://cylonjs.com) is a JavaScript framework for robotics, physical computing, and the Internet of Things (IoT). This repository contains the adaptor/driver for communicating with joysticks and game controllers. It can be used with any [SDL][http://www.libsdl.org/]-compatible controller. Default bindings are provided for the Xbox 360, DualShock 3, DualShock 4, and Logitech F310 controllers. The cylon-gamepad implementation is made possible by the gamepad module https://github.com/creationix/node-gamepad created by @creationix thank you! Want to use Ruby on robots? Check out our sister project Artoo (http://artoo.io) Want to use the Go programming language to power your robots? Check out our sister project Gobot (http://gobot.io).    How to Install Installing Cylon.js with Joystick support is pretty easy. $ npm install cylon cylon-joystick  Note  OS X does not provide native support for Xbox 360 controllers. As such, a third-party driver is required. If you're using a PS3 controller and want to communicate with it over USB, plug it in and then press the PlayStation button to make sure it's connected.  How to Use var Cylon = require('cylon');  Cylon.robot({   connections: {     joystick: { adaptor: 'joystick' }   },    devices: {     controller: { driver: 'dualshock-3' }   },    work: function(my) {     [""square"", ""circle"", ""x"", ""triangle""].forEach(function(button) {       my.controller.on(button + "":press"", function() {         console.log(""Button "" + button + "" pressed."");       });        my.controller.on(button + "":release"", function() {         console.log(""Button "" + button + "" released."");       });     });      my.controller.on(""left_x:move"", function(pos) {       console.log(""Left Stick - X:"", pos);     });      my.controller.on(""right_x:move"", function(pos) {       console.log(""Right Stick - X:"", pos);     });      my.controller.on(""left_y:move"", function(pos) {       console.log(""Left Stick - Y:"", pos);     });      my.controller.on(""right_y:move"", function(pos) {       console.log(""Right Stick - Y:"", pos);     });   } }).start(); How to Connect Plug your USB joystick or game controller into your USB port. If your device is supported by SDL, you are now ready. Custom joysticks If you don't have one of the joysticks we support natively, or want to make changes to the configuration, cylon-joystick supports custom bindings. To use a custom joystick with Cylon, simply supply the joystick bindings file when you're describing the device: var Cylon = require('cylon');  var config = __dirname + ""/controller.json""  Cylon.robot({   connections: {     joystick: { adaptor: 'joystick' }   },    devices: {     controller: { driver: ""joystick"", config: config }   },    work: function(my) {     // your custom mappings will be reflected here as events   } }).start(); A joystick bindings file needs to contain the device's productID, vendorID, and description, as this is how cylon-joystick will find the appropriate device. For an example of what a bindings file should look like, here is the Xbox 360 controller bindings file we use. If you are using a ""white-label"" version of a particular gamepad, you can override the productID, vendorID, and/or description so you can use an existing mapping. For example, a PS3 compatible gamepad that uses the same vendorID & productID, but a different name, you could use the existing Dualshock 3 mapping as follows:   devices: {     controller: { driver: 'dualshock-3', description: 'Coolstick 5000' }   }, cylon-joystick-explorer cylon-joystick includes the cylon-joystick-explorer binary. It's useful for figuring out what compatible gamepads you have connected, as well as making it easier to generate custom bindings JSON files. For best use, install cylon-joystick globally: $ npm install -g cylon-joystick  Then just run the command: $ cylon-joystick-explorer  Documentation We're busy adding documentation to our web site at http://cylonjs.com/ please check there as we continue to work on Cylon.js Thank you! Contributing For our contribution guidelines, please go to https://github.com/hybridgroup/cylon/blob/master/CONTRIBUTING.md . Release History For the release history, please go to https://github.com/hybridgroup/cylon-joystick/blob/master/RELEASES.md . License Copyright (c) 2013-2016 The Hybrid Group. Licensed under the Apache 2.0 license.","http://cylonjs.com) is a javascript framework for robotics, physical computing, and the internet o robottics, physical computing, and the internet of things (iot). this repository contains the adaptor/dri",10,8,182,0,1,10,1
IoT-Lab-Minden/SwarmRob,https://github.com/IoT-Lab-Minden/SwarmRob,0,2019-10-10T11:39:48Z,2019-10-10,11:39:48Z,2019,2019-10-21T08:20:36Z,2019-10-21,08:20:36Z,2019,SwarmRob is a python framework that improves the reproducibility in robotics research by using container technologies and orchestration to enable the simple sharing of experimental artifacts,"Swarmrob     Container Orchestration for Robot Applications Because of the very heterogeneous composition of software and hardware in robotics, the reproduction of experiments is a common problem. SwarmRob is a python framework that uses container technologies and orchestration to enable the simple sharing of experimental artifacts and improve the reproducibility in robotics research. General Informations The reproduction of experiments is one of the fundamental problems of robotics research. SwarmRob tries to solve it by providing a solution to support the re-execution and reproduction of experiments. The solution simplifies the execution of experiments on a cluster of robots with multiple services communicating with each other. For this purpose, SwarmRob uses container virtualization in combination with an orchestration mechanism that is adapted to the requirements of robotics. The software is oriented along the master-worker-pattern. A single master manages the experiment and allocates the services to the participating robots called worker.    The Architecture of SwarmRob - The green cubes represent the worker nodes and the red cubes represent the master nodes. Every bounding box illustrates a swarm.The outer box illustrates the local network of the laboratory and the grey boxes illustrates the repositories where the worker can obtain the definition files. An experiment is described using Docker-like configuration files which can be published using private or public repositories and can be obtained by other researchers. The workflow can be subdivided in two phases: the research phase and the review phase.  Workflow of SwarmRob - The figure illustrates the research phase (left timeline) and the review phase (right timeline) of the workflow with their related subphases. The research phase is the phase where the experiment is developed and specified by the responsible researchers. Every robot participating in the experiment is specified using a Service Definition File (SDF). The SDF includes the complete functional scope of Docker and should be an executable image of this specific robot. An example of a valid SDF is shown in the following code block. FROM iotlab/indigo # Initialize the catkin workspace USER ros WORKDIR /home/ros/src RUN /usr/bin/python /opt/ros/indigo/bin/ catkin_init_workspace RUN git clone --recursive https://gitlab/repository/ ROSMaster.git -b master WORKDIR /home/ros # Build the catkin workspace RUN /opt/ros/indigo/bin/catkin_make #... ENTRYPOINT [""/home/ros/startup.sh""]  Afterwards, the researcher can compose the experiment by defining an experiment definition file (EDF) that references the prior defined SDFs. An EDF is a subset of docker-compose adapted to specific requirements of robotics like the definition of required hardware, e.g. camera, laser scanner etc.. The difference between docker-compose and SwarmRob is that the definition of devices is taken into account within the orchestration and allocation process. An example of a valid EDF is shown in the following code block. services:     rosmaster:       #Specifies the location of the image       image: repository:5000/ros-master       environment:         - ROS_IP=hm_rosmaster_1     camera:       image: repository:5000/ros-smart-camera       #Specifies service-specific environment variables       environment:         - ROS_URI=http://rosmaster_1:11311         - CAMERA_NAME=Cam       #Specifies the cross-service dependencies       depends_on:         - ""rosmaster""       #Specifies the required devices       devices:         - ""/dev/video0:/dev/video0""  Afterwards, the researchers just need to publish the SDFs and EDFs along with the publication to allow other researchers to get them and reproduce the experiment. As previously described, therefore SwarmRob relies on container virtualization to enable the reproduction of software-intensive multi-robot experiments. A key feature of multi-robot systems is the communicaiton. In order to enable a capsulated and inference-free communication between the robots, SwarmRob makes use of the virtual network feature of Docker based on VXLAN. While the use of container virtualization becomes more common in robotics, especially the networking is a very time-consuming and error-prone procedure. SwarmRob automates this by using a distributed key-value store to spread the correct network configuration to all participating robots. This enables the communication between e.g. ROS nodes via virtual networks on top of the original underlying network. The whole configuration of the swarm is carried out by the software. The researchers only need to initialize a swarm, add the workers and start the swarm.    System and Inter-Robot Network Architecture using Overlay Networks - The Underlay Network represents the physical network connection between the hosts, the Intra-Swarm Communication represents the commands and information exchanged between the participants of a swarm and the Overlay Network is the communication channel used for the communication between containerized applications More in-depth information can be found in the publication:  A. Pörtner, M. Hoffmann, S. Zug, and M. König, “SwarmRob: A Toolkit for Reproducibility and Sharing of Experimental Artifacts in Robotics Research,” in 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2018, p. 325–332.  A good start to connect with SwarmRob is to follow the Getting started. Copyright Copyright 2018,2019 Aljoscha Pörtner Copyright 2019 André Kirsch This file is part of SwarmRob. SwarmRob is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. SwarmRob is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with SwarmRob.  If not, see https://www.gnu.org/licenses/.",swarmrob     container orchestration  robotobot applications because of the very heterogeneous composition of software and hardware in robotics,2,0,84,0,1,0,1
0xJeremy/Dum-E-IOT,https://github.com/0xJeremy/Dum-E-IOT,0,2019-02-07T02:46:02Z,2019-02-07,02:46:02Z,2019,2019-03-10T02:00:29Z,2019-03-10,02:00:29Z,2019,Code and Schematics for an internet enabled robotic arm swarm,"Dum-E IOT This repository holds all files needed to create a swarm of internet enabled robotic arms. All .dxf files and 3D models can be found in the ""Robot Files"" folder. All code needed to host a Node.js Heroku server is also included here. This project was made for MakeHarvard 2019 and is intended as a teaching tool for robot enthusiasts. The board used to control these arms is a generic NodeMCU ESP-8266 Wifi board. The servos used were standard 180° rotation servos. The CAD files can be modified to fit servos of different sizes. A wiring diagram is not currently available but will be created in the near future.  NOTE: NodeMCU will not flash if 6V battery pack is plugged in and off. Turn on 6V power to flash board.","iles needed to create a swarm of internet enabled robotic arms. all .dxf files and 3d  robottic arms. all .dxf files and 3d models can be found in the ""robot files"" folder. all code needed to ",14,2,97,0,1,0,1
ZeroSum24/IoT-Room-Occupancy-Monitor,https://github.com/ZeroSum24/IoT-Room-Occupancy-Monitor,0,2019-01-27T11:39:00Z,2019-01-27,11:39:00Z,2019,2019-11-02T11:31:03Z,2019-11-02,11:31:03Z,2019,Full stack room occupancy monitoring project,"IoTSSC - Internet of Things Systems Security and the Cloud A full-stack room occupancy and localization tool. Developed by Stephen Waddell and Samuel Knight Supervised by Paul Patras  System Overview Embedded Devices: Devices used to capture data from sensors. Edge computing consisted of data aggregation and simple computations such as determining if a person entered or exited the room based on the recent distance sensor readings. All embedded devices were nRF52 DK models.   Door-mounted device: Device connected to two distance sensors on either side of the doorway. In charge of reporting enter and exit readings. Distance Sensors - Time of Flight Adafruit VL53L0X   Chair-mounted devices: Device connected to two pressure sensors monitoring the seat and back of the chair respectively. In charge of reporting chair occupancy based on both sensors being active (or inactive) for a duration of time. Each chair device advertises its presence via BLE for the table-mounted device. Pressure Sensors - Force Sensitive Resistor Active Robots   Table-mounted devices: Device connected to the center of each table. Responsible for scanning bluetooth signals emitted by the chair devices and determining their relative signal strengh (RSSI values). These values are used to determine if the chair is positioned at the table.   Gateway Application: Application running on an android device for the purpose of routing data from the embedded devices up to the cloud. Application scans nearby embedded devices and reads in data asynchronously from multiple devices. Gateway is in charge with providing accurate timestamps for the data recorded. The data is then sent to the cloud database for futher processing. Database: Google's Firestore database used to store data organized into specific categories. Raw data is stored on a device-type basis and is timestamped. Other categorizations are used for processing data, holding real-time metrics and for the final visualization of the data. Server and Website: The server is hosted on AWS and is in charge of reading in raw data pushed to Firestore, processing the data and updating real-time metrics. The server used event listeners on all device topics in the database, avoiding costly polling of the entire database for new entries. The website (also hosted on AWS) reads in processed data, as well as real-time metrics and visualizes these accordingly. Visualization is achieved using chartist.js for simple and clean graphs.","ounted device. pressure sensors - force sensitive resistor active robots   table-mounted devices: device connected to the center of each table. responsible for scanning bluetooth signals emitted by the chair devices and determining their relative signal strengh (rssi values). these values are used to determine if the chair is positioned at the table.   gateway application: application running on an android device for the purpose of routing data from the embedded devices up to the cloud. application scans nearby embedded devices and reads in data asynchronously from multiple devices. gateway is in charge with providing accurate timestamps for the data recorded. the data is then sent to the cloud database for futher processing. database: google's firestore database used to store data organized into specific categories. raw data is stored on a device-type basis and is timestamped. other categorizations are used for processing data, holding real-time metrics and for the final visualization of the data. server robotstor active robots   table-mounted devices: device connected to the center of each table. responsibl",3,0,153,0,1,2,1
ANRGUSC/iris-riot,https://github.com/ANRGUSC/iris-riot,1,2018-09-28T04:52:52Z,2018-09-28,04:52:52Z,2018,2019-01-22T19:57:47Z,2019-01-22,19:57:47Z,2019,The IRIS Testbed fork of RIOT-OS slightly modified with IRIS application code.,"Intelligent Robotic Internet of things teStbed (IRIS) - RIOT-OS Newcomers, please start at the ANRGUSC/iris-testbed repository! All RIOT-OS related application code for the IRIS testbed can be found here. This repository is a fork of the main RIOT-OS repository, but has since diverged a little. If you would like to request more documentation or features, please open a Github issue. The IRIS testbed is a fully open source testbed, and we welcome contributors through Github's pull request system. Thanks for stopping by! IRIS Testbed Code IRIS Testbed code can be found in the examples/iris_tesbed directory. Updates from RIOT's Github Repository The maintainers will periodically rebase updates from RIOT's master branch on github into this repository when new features or fixes are needed. (END OF USC ANRG)                       ZZZZZZ                     ZZZZZZZZZZZZ                   ZZZZZZZZZZZZZZZZ                  ZZZZZZZ     ZZZZZZ                 ZZZZZZ        ZZZZZ                 ZZZZZ          ZZZZ                 ZZZZ           ZZZZZ                 ZZZZ           ZZZZ                 ZZZZ          ZZZZZ                 ZZZZ        ZZZZZZ                 ZZZZ     ZZZZZZZZ       777        7777       7777777777           ZZ    ZZZZ   ZZZZZZZZ         777      77777777    77777777777       ZZZZZZZ   ZZZZ  ZZZZZZZ           777     7777  7777       777     ZZZZZZZZZ   ZZZZ    Z               777     777    777       777    ZZZZZZ       ZZZZ                    777     777    777       777   ZZZZZ         ZZZZ                    777     777    777       777  ZZZZZ          ZZZZZ    ZZZZ           777     777    777       777  ZZZZ           ZZZZZ    ZZZZZ          777     777    777       777  ZZZZ           ZZZZZ     ZZZZZ         777     777    777       777  ZZZZ           ZZZZ       ZZZZZ        777     777    777       777  ZZZZZ         ZZZZZ        ZZZZZ       777     777    777       777   ZZZZZZ     ZZZZZZ          ZZZZZ      777     7777777777       777    ZZZZZZZZZZZZZZZ            ZZZZ      777      77777777        777      ZZZZZZZZZZZ               Z         ZZZZZ  The friendly Operating System for IoT! RIOT is a real-time multi-threading operating system that supports a range of devices that are typically found in the Internet of Things (IoT): 8-bit, 16-bit and 32-bit microcontrollers. RIOT is based on the following design principles: energy-efficiency, real-time capabilities, small memory footprint, modularity, and uniform API access, independent of the underlying hardware (this API offers partial POSIX compliance). RIOT is developed by an international open source community which is independent of specific vendors (e.g. similarly to the Linux community). RIOT is licensed with LGPLv2.1, a copyleft license which fosters indirect business models around the free open-source software platform provided by RIOT, e.g. it is possible to link closed-source code with the LGPL code. FEATURES RIOT is based on a microkernel architecture, and provides features including, but not limited to:  a preemptive, tickless scheduler with priorities flexible memory management high resolution, long-term timers support for AVR, MSP430, MIPS, ARM7, and ARM Cortex-M on over 80 boards the native port allows to run RIOT as-is on Linux, BSD, and MacOS. Multiple instances of RIOT running on a single machine can also be interconnected via a simple virtual Ethernet bridge IPv6 6LoWPAN (RFC4944, RFC6282, and RFC6775) UDP RPL (storing mode, P2P mode) CoAP CCN-Lite  GETTING STARTED  You want to start the RIOT? Just follow our quickstart guide or the getting started documentation. The RIOT API itself can be built from the code using doxygen. The latest version is uploaded daily to http://riot-os.org/api.  KNOWN ISSUES  With latest GCC version (>= 6) platforms based on some ARM platforms will raise some warnings, leading to a failing build (see https://github.com/RIOT-OS/RIOT/issues/5519). As a workaround, you can compile with warnings not being treated as errors: WERROR=0 make  USING THE NATIVE PORT WITH NETWORKING If you compile RIOT for the native cpu and include the netdev_tap module, you can specify a network interface like this: PORT=tap0 make term SETTING UP A TAP NETWORK There is a shellscript in RIOT/dist/tools/tapsetup called tapsetup which you can use to create a network of tap interfaces. USAGE To create a bridge and two (or count at your option) tap interfaces: ./dist/tools/tapsetup/tapsetup [-c [<count>]]  CONTRIBUTE To contribute something to RIOT, please refer to the development procedures and read all notes for best practice. MAILING LISTS  RIOT OS kernel developers list devel@riot-os.org (http://lists.riot-os.org/mailman/listinfo/devel) RIOT OS users list users@riot-os.org (http://lists.riot-os.org/mailman/listinfo/users) RIOT commits commits@riot-os.org (http://lists.riot-os.org/mailman/listinfo/commits) Github notifications notifications@riot-os.org (http://lists.riot-os.org/mailman/listinfo/notifications)  LICENSE  Most of the code developed by the RIOT community is licensed under the GNU Lesser General Public License (LGPL) version 2.1 as published by the Free Software Foundation. Some external sources, especially files developed by SICS are published under a separate license.  All code files contain licensing information. For more information, see the RIOT website: http://www.riot-os.org","intelligent  robotic internet of things testbed (iris) - riot-os newcomers, please start at the anrgusc/iris-testbed r",150,0,"14,66",0,1,1,1
nebrius/johnny-five-iot-edge,https://github.com/nebrius/johnny-five-iot-edge,0,2018-04-25T18:38:39Z,2018-04-25,18:38:39Z,2018,2019-05-27T08:47:11Z,2019-05-27,08:47:11Z,2019,An IoT Edge module for interacting with hardware based on http://johnny-five.io/,"IoT Edge for Makers IoT Edge for Makers is an IoT Edge module for interacting with hobbyist and prototype-friendly hardware such as the Raspberry Pi. Introduction IoT Edge is a managed service for managing and deploying IoT devices. IoT Edge devices run modules, which are Docker-compatible containers that run prewritten or custom code. The IoT Edge for Makers module allows developers to easily configure and interface with hobbyist hardware such as the Raspberry Pi to send and receive data from sensors, LEDs, and other peripherals. In keeping with the modular approach, the IoT Edge for Makers module only handles input and output to and from the device to IoT Edge. Developers should create a second module to handle application logic.  In our examples folder, you'll find a sample configuration that would be applied to a Raspberry Pi with a LED, a button, and other components and how the inputs and outputs would be routed to another logic module. How to use The steps below detail each step of setting up and provisioning a Raspberry Pi IoT Edge device and deploying the IoT Edge for Makers module. You will need an Azure account and a Raspberry Pi with an installed operating system such as Raspbian Jessie or Stretch.  Create an Azure IoT Hub and register an IoT Edge Device by following the first two steps at https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-simulate-device-linux. You can complete these steps before setting up the Raspberry Pi.    Set up the Raspberry Pi with prerequisites by running the following commands in Terminal:  sudo apt update sudo apt install python-pip -y sudo pip install -U setuptools pip sudo apt install python2.7-dev libffi-dev libssl-dev -y sudo pip install -U cryptography idna curl -sSL https://get.docker.com | sudo -E sh sudo usermod -aG docker $USER sudo pip install -U azure-iot-edge-runtime-ctl  Then, open /boot/config.txt and add the following lines: dtparam=i2c_arm=on dtparam=i2c=on dtparam=i2c_arm_baudrate=100000  Save, then reboot the Raspberry Pi. 3. Continue with steps 3 and 4 https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-simulate-device-linux to start the IoT Edge runtime. On step 4, ""Deploy a module,"" enter toolboc/johnny5onedge/ in the Image URI field. You will also need to provide the following in the """"Container Create Options"" field:   ""ExposedPorts"": {     ""9229/tcp"": {}   },   ""Env"": [     ""DEBUG_OPTION=--inspect-brk=0.0.0.0:9229""   ],   ""HostConfig"": {     ""PortBindings"": {       ""9229/tcp"": [         {           ""HostPort"": ""9229""         }       ]     },     ""Privileged"": true,     ""Devices"": [       {         ""PathOnHost"": ""/dev/i2c-1"",         ""PathInContainer"": ""/dev/i2c-1"",         ""CgroupPermissions"": ""rwm""       },       {         ""PathOnHost"": ""/dev/gpiomem"",         ""PathInContainer"": ""/dev/gpiomem"",         ""CgroupPermissions"": ""rwm""       }     ],     ""Mounts"": [       {         ""Type"": ""bind"",         ""Source"": ""/lib/modules/"",         ""Target"": ""/lib/modules/""       }     ]   } }  After completing this step, wait a few moments for the container to deploy to your device. Then, when you run sudo docker ps, you should see the IoT Edge for Makers module running. Details Under the hood, the IoT Edge for Makers module uses Johnny-Five, a popular open source Javascript platform for IoT and robotic development that gives various IoT hardware devices a consistent programming interface. The IoT Edge module takes care of some challenges such as exposing GPIO through the Docker container. Currently, the IoT Edge for Makers module supports only the Raspberry Pi. However, it can be extended to support other boards supported by the Johnny-Five platform such as the Arduino UNO and Particle Photon. Troubleshooting and contributing License MIT License Copyright (c) 2018 IoT Edge for Makers contributors Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."," under the hood, the iot edge for makers module uses johnny-five, a popular open source javascript platform for iot and robotic development that gives various iot hardware devices a consistent programming interface. the iot edge module takes care of some challenges such as exposing gpio through the docker container. currently, the iot edge for makers module supports only the raspberry pi. however, it can be extended to support other boards supported by the johnny-five platform such as the arduino uno and particle photon. troubleshooting and contributing license mit license copyright (c) 2018 iot edge for makers contributors permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""software""), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions: the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software. the software is provided ""as is"", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software. robotohnny-five, a popular open source javascript platform for iot and robotic development that gives var",5,8,93,0,1,0,1
hybridgroup/cylon-mqtt,https://github.com/hybridgroup/cylon-mqtt,5,2014-10-25T08:45:11Z,2014-10-25,08:45:11Z,2014,2019-07-11T16:07:37Z,2019-07-11,16:07:37Z,2019,Cylon.js adaptor/driver for MQTT protocol,"Cylon.js For MQTT Cylon.js (http://cylonjs.com) is a JavaScript framework for robotics, physical computing, and the Internet of Things (IoT). This repository contains the Cylon.js adaptor/driver for the MQTT messaging protocol. It uses the MQTT.js node module (https://github.com/adamvr/MQTT.js) created by @adamvr and @mcollina, thank you! Want to use Ruby on robots? Check out our sister project Artoo (http://artoo.io) Want to use the Go programming language to power your robots? Check out our sister project Gobot (http://gobot.io).    How to Install Install cylon-mqtt through NPM: $ npm install cylon cylon-mqtt  Before using cylon-mqtt, you'll need to have a MQTT broker running in order to connect/publish/subscribe to messages. A good, simple broker is mosca. The developers have a tutorial on using Mosca as a standalone service. How to Use There's two different ways to use the cylon-mqtt module. You can use the connection object only, in which case you have pub/sub access to all available MQTT channels: Cylon.robot({   connections: {     server: { adaptor: 'mqtt', host: 'mqtt://localhost:1883' }   },    work: function(my) {     my.server.subscribe('hello');      my.server.on('message', function (topic, data) {       console.log(topic + "": "" + data);     });      every((1).seconds(), function() {       my.server.publish('hello', 'hi there');     });   } }); Or, you can use the device object, which restricts your interaction to a single MQTT channel. This can make it easier to keep track of different channels. Cylon.robot({   connections: {     server: { adaptor: 'mqtt', host: 'mqtt://localhost:1883' }   },    devices: {     hello: { driver: 'mqtt', topic: 'hello' }   },    work: function(my) {     my.hello.on('message', function (data) {       console.log(""hello: "" + data);     });      every((1).seconds(), function() {       my.hello.publish('hi there');     });   } }) Simple var Cylon = require('cylon');  Cylon.robot({   connections: {     server: { adaptor: 'mqtt', host: 'mqtt://localhost:1883' }   },    devices: {     hello: { driver: 'mqtt', topic: 'greetings' }   },    work: function(my) {     my.hello.on('message', function (data) {       console.log(data);     });      every((1).seconds(), function() {       console.log(""Saying hello..."");       my.hello.publish('hi there');     });   } }).start(); Arduino Blink var Cylon = require('cylon');  Cylon.robot({   connections: {     mqtt: { adaptor: 'mqtt', host: 'mqtt://localhost:1883' },     firmata: { adaptor: 'firmata', port: '/dev/ttyACM0' }   },    devices: {     toggle: { driver: 'mqtt', topic: 'toggle', adaptor: 'mqtt' },     led: { driver: 'led', pin: '13', adaptor: 'firmata' },   },    work: function(my) {     my.toggle.on('message', function(data) {       console.log(""Message on 'toggle': "" + data);       my.led.toggle();     });      every((1).second(), function() {       console.log(""Toggling LED."");       my.toggle.publish('toggle');     });   } }).start(); For more examples, please see the examples folder. How to Connect var Cylon = require('cylon');  Cylon.robot({   connections: {     server: { adaptor: 'mqtt', host: 'mqtt://localhost:1883' }   },    work: function(my) {     my.server.subscribe('hello');      my.server.on('message', function (topic, data) {       console.log(topic + "": "" + data);     });      every((1).seconds(), function() {       console.log(""Saying hello..."");       my.server.publish('hello', 'hi there');     });   } }).start(); Authentication     mqtt: { adaptor: ""mqtt"", host: ""mqtt://localhost:1883"",             username: ""iamuser"", password: ""sosecure"" },  Documentation We're busy adding documentation to cylonjs.com. Please check there as we continue to work on Cylon.js. Thank you! Contributing For our contribution guidelines, please go to https://github.com/hybridgroup/cylon/blob/master/CONTRIBUTING.md . Release History For the release history, please go to https://github.com/hybridgroup/cylon-mqtt/blob/master/RELEASES.md . License Copyright (c) 2014-2016 The Hybrid Group. Licensed under the Apache 2.0 license.","http://cylonjs.com) is a javascript framework for robotics, physical computin robottics, physical computing, and the internet of things (iot). this repository contains the cylon.js ad",7,7,65,0,1,0,1
Agile-and-Adaptive-Robotics/Bipedal_Robot,https://github.com/Agile-and-Adaptive-Robotics/Bipedal_Robot,1,,,6,,,,215,2021,Repository for the code and design of a bipedal robot actuated by pneumatic artificial muscles,"Bipedal Robot
The bipedal robot is a cross disciplinary project being worked on in the Agile and Adaptive Robotics lab by PhD, Masters, and Undergraduate Students. The aim is to develop a robot, actuated by pneumatic artificial muscles, that is able to walk.
Current Objectives
Create a Matlab script to optimize PAM placement to replicate human torque profile about joints.
Design lower body skeletal structure that has the range of motion of a human skeletal structure.
Future Objective
Print and assemble lower body of the bipedal robot.
Perform experiments to verify the torque profiles about joints.
Develop a sequence of actuation that can generate sagittal plane walking motion.
Create a neural controller that can integrate sensory information to generate robust walking.
Software
SolidWorks - Tools for creating the model of the lower body, to be printed on Markforge 3D printers.
OpenSim - Used to display biologically realistic muscle placements on human.
Matlab - For creating scripts for torque calculation and muscle placement optimization.
Animatlab - Neural network simulations for a bipedal walker.
Contributors
Ben Bolen - Project Co-Lead, PhD Student
Connor Morrow - Project Co-Lead, PhD Student
Harold Omotoy - Muscle Placement and PAM Testing, Undergraduate Researcher
Previous Contributors
Alex Steele - Knee Joint Designer, Past Master's Student
Ty Fitzgerald - Ankle and Foot Design, Undergraduate Researcher
David Pleshakov - Lower Body Design, Undergraduate Researcher
Mick Leungpathomaram - Various SolidWorks components and assemblies, High School Research Intern
Alaina Clar - Various SolidWorks components and naming convention creator, High School Research Intern",,6,0,215,,,,
hybridgroup/gobot,https://github.com/hybridgroup/gobot,958,2013-09-21T14:09:19Z,2013-09-21,14:09:19Z,2013,2021-01-13T16:00:13Z,2021-01-13,16:00:13Z,2021,"Golang framework for robotics, drones, and the Internet of Things (IoT)","Gobot (https://gobot.io/) is a framework using the Go programming language (https://golang.org/) for robotics, physical computing, and the Internet of Things. It provides a simple, yet powerful way to create solutions that incorporate multiple, different hardware devices at the same time. Want to run Go directly on microcontrollers? Check out our sister project TinyGo (https://tinygo.org/) Getting Started Get the Gobot package by running this command: go get -d -u gobot.io/x/gobot Examples Gobot with Arduino package main  import ( 	""time""  	""gobot.io/x/gobot"" 	""gobot.io/x/gobot/drivers/gpio"" 	""gobot.io/x/gobot/platforms/firmata"" )  func main() { 	firmataAdaptor := firmata.NewAdaptor(""/dev/ttyACM0"") 	led := gpio.NewLedDriver(firmataAdaptor, ""13"")  	work := func() { 		gobot.Every(1*time.Second, func() { 			led.Toggle() 		}) 	}  	robot := gobot.NewRobot(""bot"", 		[]gobot.Connection{firmataAdaptor}, 		[]gobot.Device{led}, 		work, 	)  	robot.Start() } Gobot with Sphero package main  import ( 	""fmt"" 	""time""  	""gobot.io/x/gobot"" 	""gobot.io/x/gobot/platforms/sphero"" )  func main() { 	adaptor := sphero.NewAdaptor(""/dev/rfcomm0"") 	driver := sphero.NewSpheroDriver(adaptor)  	work := func() { 		gobot.Every(3*time.Second, func() { 			driver.Roll(30, uint16(gobot.Rand(360))) 		}) 	}  	robot := gobot.NewRobot(""sphero"", 		[]gobot.Connection{adaptor}, 		[]gobot.Device{driver}, 		work, 	)  	robot.Start() } ""Metal"" Gobot You can use the entire Gobot framework as shown in the examples above (""Classic"" Gobot), or you can pick and choose from the various Gobot packages to control hardware with nothing but pure idiomatic Golang code (""Metal"" Gobot). For example: package main  import ( 	""gobot.io/x/gobot/drivers/gpio"" 	""gobot.io/x/gobot/platforms/intel-iot/edison"" 	""time"" )  func main() { 	e := edison.NewAdaptor() 	e.Connect()  	led := gpio.NewLedDriver(e, ""13"") 	led.Start()  	for { 		led.Toggle() 		time.Sleep(1000 * time.Millisecond) 	} } ""Master"" Gobot You can also use the full capabilities of the framework aka ""Master Gobot"" to control swarms of robots or other features such as the built-in API server. For example: package main  import ( 	""fmt"" 	""time""  	""gobot.io/x/gobot"" 	""gobot.io/x/gobot/api"" 	""gobot.io/x/gobot/platforms/sphero"" )  func NewSwarmBot(port string) *gobot.Robot { 	spheroAdaptor := sphero.NewAdaptor(port) 	spheroDriver := sphero.NewSpheroDriver(spheroAdaptor) 	spheroDriver.SetName(""Sphero"" + port)  	work := func() { 		spheroDriver.Stop()  		spheroDriver.On(sphero.Collision, func(data interface{}) { 			fmt.Println(""Collision Detected!"") 		})  		gobot.Every(1*time.Second, func() { 			spheroDriver.Roll(100, uint16(gobot.Rand(360))) 		}) 		gobot.Every(3*time.Second, func() { 			spheroDriver.SetRGB(uint8(gobot.Rand(255)), 				uint8(gobot.Rand(255)), 				uint8(gobot.Rand(255)), 			) 		}) 	}  	robot := gobot.NewRobot(""sphero"", 		[]gobot.Connection{spheroAdaptor}, 		[]gobot.Device{spheroDriver}, 		work, 	)  	return robot }  func main() { 	master := gobot.NewMaster() 	api.NewAPI(master).Start()  	spheros := []string{ 		""/dev/rfcomm0"", 		""/dev/rfcomm1"", 		""/dev/rfcomm2"", 		""/dev/rfcomm3"", 	}  	for _, port := range spheros { 		master.AddRobot(NewSwarmBot(port)) 	}  	master.Start() } Hardware Support Gobot has a extensible system for connecting to hardware devices. The following robotics and physical computing platforms are currently supported:  Arduino <=> Package Audio <=> Package Beaglebone Black <=> Package Beaglebone PocketBeagle <=> Package Bluetooth LE <=> Package C.H.I.P <=> Package C.H.I.P Pro <=> Package Digispark <=> Package DJI Tello <=> Package DragonBoard <=> Package ESP8266 <=> Package GoPiGo 3 <=> Package Intel Curie <=> Package Intel Edison <=> Package Intel Joule <=> Package Joystick <=> Package Keyboard <=> Package Leap Motion <=> Package MavLink <=> Package MegaPi <=> Package Microbit <=> Package MQTT <=> Package NATS <=> Package Neurosky <=> Package OpenCV <=> Package Particle <=> Package Parrot ARDrone 2.0 <=> Package Parrot Bebop <=> Package Parrot Minidrone <=> Package Pebble <=> Package Raspberry Pi <=> Package Sphero <=> Package Sphero BB-8 <=> Package Sphero Ollie <=> Package Sphero SPRK+ <=> Package Tinker Board <=> Package UP2 <=> Package  Support for many devices that use General Purpose Input/Output (GPIO) have a shared set of drivers provided using the gobot/drivers/gpio package:  GPIO <=> Drivers  AIP1640 LED Button Buzzer Direct Pin EasyDriver Grove Button Grove Buzzer Grove LED Grove Magnetic Switch Grove Relay Grove Touch Sensor LED Makey Button Motor Proximity Infra Red (PIR) Motion Sensor Relay RGB LED Servo Stepper Motor TM1638 LED Controller    Support for many devices that use Analog Input/Output (AIO) have a shared set of drivers provided using the gobot/drivers/aio package:  AIO <=> Drivers  Analog Sensor Grove Light Sensor Grove Piezo Vibration Sensor Grove Rotary Dial Grove Sound Sensor Grove Temperature Sensor    Support for devices that use Inter-Integrated Circuit (I2C) have a shared set of drivers provided using the gobot/drivers/i2c package:  I2C <=> Drivers  Adafruit Motor Hat ADS1015 Analog to Digital Converter ADS1115 Analog to Digital Converter ADXL345 Digital Accelerometer BH1750 Digital Luminosity/Lux/Light Sensor BlinkM LED BME280 Barometric Pressure/Temperature/Altitude/Humidity Sensor BMP180 Barometric Pressure/Temperature/Altitude Sensor BMP280 Barometric Pressure/Temperature/Altitude Sensor BMP388 Barometric Pressure/Temperature/Altitude Sensor DRV2605L Haptic Controller Grove Digital Accelerometer GrovePi Expansion Board Grove RGB LCD HMC6352 Compass INA3221 Voltage Monitor JHD1313M1 LCD Display w/RGB Backlight L3GD20H 3-Axis Gyroscope LIDAR-Lite MCP23017 Port Expander MMA7660 3-Axis Accelerometer MPL115A2 Barometer MPU6050 Accelerometer/Gyroscope PCA9685 16-channel 12-bit PWM/Servo Driver SHT2x Temperature/Humidity SHT3x-D Temperature/Humidity SSD1306 OLED Display Controller TSL2561 Digital Luminosity/Lux/Light Sensor Wii Nunchuck Controller    Support for devices that use Serial Peripheral Interface (SPI) have a shared set of drivers provided using the gobot/drivers/spi package:  SPI <=> Drivers  APA102 Programmable LEDs MCP3002 Analog/Digital Converter MCP3004 Analog/Digital Converter MCP3008 Analog/Digital Converter MCP3202 Analog/Digital Converter MCP3204 Analog/Digital Converter MCP3208 Analog/Digital Converter MCP3304 Analog/Digital Converter SSD1306 OLED Display Controller    More platforms and drivers are coming soon... API: Gobot includes a RESTful API to query the status of any robot running within a group, including the connection and device status, and execute device commands. To activate the API, import the gobot.io/x/gobot/api package and instantiate the API like this:   master := gobot.NewMaster()   api.NewAPI(master).Start() You can also specify the api host and port, and turn on authentication:   master := gobot.NewMaster()   server := api.NewAPI(master)   server.Port = ""4000""   server.AddHandler(api.BasicAuth(""gort"", ""klatuu""))   server.Start() You may access the robeaux React.js interface with Gobot by navigating to http://localhost:3000/index.html. CLI Gobot uses the Gort http://gort.io Command Line Interface (CLI) so you can access important features right from the command line. We call it ""RobotOps"", aka ""DevOps For Robotics"". You can scan, connect, update device firmware, and more! Gobot also has its own CLI to generate new platforms, adaptors, and drivers. You can check it out in the /cli directory. Documentation We're always adding documentation to our web site at https://gobot.io/ please check there as we continue to work on Gobot Thank you! Need help?  Issues: https://github.com/hybridgroup/gobot/issues Twitter: @gobotio Slack: https://gophers.slack.com/messages/C0N5HDB08 Mailing list: https://groups.google.com/forum/#!forum/gobotio  Contributing For our contribution guidelines, please go to https://github.com/hybridgroup/gobot/blob/master/CONTRIBUTING.md . Gobot is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms. You can read about it here. License Copyright (c) 2013-2020 The Hybrid Group. Licensed under the Apache 2.0 license. The Contributor Covenant is released under the Creative Commons Attribution 4.0 International Public License, which requires that attribution be included.","o programming language (https://golang.org/) for robotics, physical computing, and the internet of th robotics, physical computing, and the internet of things. it provides a simple, yet powerful way to creat",127,7200,2683,2,1,0,1
rwaldron/johnny-five,https://github.com/rwaldron/johnny-five,1800,2012-03-30T20:09:52Z,2012-03-30,20:09:52Z,2012,2021-01-13T19:34:07Z,2021-01-13,19:34:07Z,2021,"JavaScript Robotics and IoT programming framework, developed at Bocoup.","Johnny-Five The JavaScript Robotics Programming Framework Artwork by Mike Sgier      Johnny-Five is an Open Source, Firmata Protocol based, IoT and Robotics programming framework, developed at Bocoup. Johnny-Five programs can be written for Arduino (all models), Electric Imp, Beagle Bone, Intel Galileo & Edison, Linino One, Pinoccio, pcDuino3, Raspberry Pi, Particle/Spark Core & Photon, Tessel 2, TI Launchpad and more! Johnny-Five has grown from a passion project into a tool for inspiring learning and creativity for people of all ages, backgrounds, and from all across the world. Just interested in learning and building awesome things? You might want to start with the official Johnny-Five website. The website combines content from this repo, the wiki, tutorials from the Bocoup blog and several third-party websites into a single, easily-discoverable source:  If you want to find the API documentation, that’s right here. Need to figure out what platform to use for a project? We put that stuff here. Need inspiration for your next NodeBot? Check out the examples. Want to stay up-to-date with projects in the community? Check this out. Need NodeBots community or Johnny-Five project updates and announcements? This is what you’re looking for.  Johnny-Five does not attempt to provide ""all the things"", but instead focuses on delivering robust, reality tested, highly composable APIs that behave consistently across all supported hardware platforms. Johnny-Five wants to be a baseline control kit for hardware projects, allowing you the freedom to build, grow and experiment with diverse JavaScript libraries of your own choice. Johnny-Five couples comfortably with:  Popular application libraries such as Express.js and Socket.io. Fellow hardware projects like ar-drone, Aerogel and Spheron Bluetooth game controllers like XBox Controller and DualShock IoT frameworks, such as Octoblu  ...And that's only a few of the many explorable possibilities. Check out these exciting projects: node-pulsesensor, footballbot-workshop-ui, nodebotui, dublin-disco, node-slot-car-bot, servo-calibrator, node-ardx, nodebot-workshop, phone-home, purple-unicorn, webduino, leapduino, lasercat-workshop, simplesense, five-redbot, robotnik, the-blender Why JavaScript? NodeBots: The Rise of JavaScript Robotics Hello Johnny The ubiquitous ""Hello World"" program of the microcontroller and SoC world is ""blink an LED"". The following code demonstrates how this is done using the Johnny-Five framework. const { Board, Led } = require(""johnny-five""); const board = new Board();  board.on(""ready"", () => {   // Create an Led on pin 13   const led = new Led(13);   // Blink every half second   led.blink(500); });   Note: Node will crash if you try to run johnny-five in the node REPL, but board instances will create their own contextual REPL. Put your script in a file.  Supported Hardware Johnny-Five has been tested on a variety of Arduino-compatible Boards. For non-Arduino based projects, a number of platform-specific IO Plugins are available. IO Plugins allow Johnny-Five code to communicate with any non-Arduino based hardware in whatever language that platforms speaks! Documentation Documentation for the Johnny-Five API can be found here and example programs here. Guidance Need help? Ask a question on the NodeBots Community Forum. If you just have a quick question or are interested in ongoing design discussions, join us in the Johnny-Five Gitter Chat. For step-by-step examples, including an electronics primer, check out Arduino Experimenter's Guide for NodeJS by @AnnaGerber Here is a list of prerequisites for Linux, OSX or Windows. Check out the bluetooth guide if you want to remotely control your robot. Setup and Assemble Arduino  Recommended Starting Kit: Sparkfun Inventor's Kit Download Arduino IDE Plug in your Arduino or Arduino compatible microcontroller via USB Open the Arduino IDE, select: File > Examples > Firmata > StandardFirmataPlus  StandardFirmataPlus is available in Firmata v2.5.0 or greater   Click the ""Upload"" button.  If the upload was successful, the board is now prepared and you can close the Arduino IDE. For non-Arduino projects, each IO Plugin's repo will provide its own platform specific setup instructions. Hey you, here's Johnny! Source Code: git clone git://github.com/rwaldron/johnny-five.git && cd johnny-five  npm install npm package: Install the module with: npm install johnny-five Example Programs To get you up and running quickly, we provide a variety of examples for using each Johnny-Five component. One thing we’re especially excited about is the extensive collection of Fritzing diagrams you’ll find throughout the site. A huge part of doing any Johnny-Five project is handling the actual hardware, and we’ve included these as part of the documentation because we realised that instructions on how to write code to control a servo are insufficient without instructions on how to connect a servo! To interactively navigate the examples, visit the Johnny-Five examples page on the official website. If you want to link directly to the examples in this repo, you can use one of the following links. There are presently 362 example programs with code and diagrams! Board  Board - Basic Initialization Board - Cleanup in 'exit' event Board - Multiple in one program Board - Specify Sampling Interval Board - Specify port Custom Data Properties Pin REPL  LED  LED LED - Blink LED - Demo sequence LED - Fade LED - Fade callback LED - Fade with animation LED - PCA9685 LED - Pulse LED - Pulse with animation LED - Slider LED - Tessel Servo Module LEDs - An array of LEDs LEDs - Controlling an array of LEDs  LED: RGB  LED - RGB (Common Anode) LED - RGB (Common Anode) PCA9685 LED - RGB Intensity LED - Rainbow LED - Rainbow BlinkM  LED: Digits & Matrix  LED - Digital Clock LED - Digital Clock, Dual Displays LED - Digital Clock, HT16K33 LED - Draw Matrix Characters Demo LED - Enumerate Matrix Characters & Symbols LED - Matrix LED - Matrix Demo LED - Matrix HT16K33 LED - Matrix HT16K33 16x8  Servo  Servo Servo - Continuous Servo - Drive Servo - Multi-Turn Servo - PCA9685 Servo - Prompt Servo - Slider control Servo - Tessel Servo Module Servos - An array of servos  GPS  GPS - Adafruit Ultimate GPS Breakout GPS - Default GPS GPS - Hardware Serial GPS - Sparkfun GP-20U7  Servo Animation  Servo - Animation Servo - Leg Animation  Color  Color - EVShield EV3 (Code) Color - EVShield EV3 (Raw) Color - EVShield NXT (Code) Color - ISL29125  Motor  Motor Motor - 3 pin Motor - Adafruit DRV8871 DC Motor Driver Breakout Motor - Brake Motor - Current Motor - Directional Motor - EVShield EV3 Motor - EVShield NXT Motor - Enable Pin Motor - GROVE_I2C_MOTOR_DRIVER Motor - H-Bridge Motor - LUDUS Motor - PCA9685 Motor - Pololu VNH5019 Dual Motor Driver Breakout Motor - Sparkfun Dual H-bridge Edison Block Motor - Sparkfun TB6612FNG Motor - l298 Breakout Motors - Dual H-Bridge  Stepper Motor  Stepper - Driver Stepper - Four Wire Stepper - Sweep  ESC & Brushless Motor  ESC - Bidirectional ESC - Keypress controlled ESCs ESC - PCA9685  Button / Switch  Button Button - Bumper Button - EVShield EV3 Button - EVShield NXT Button - Options Button - Pullup Buttons - Collection w/ AT42QT1070 Switch Switch - Magnetic Door Switch - Tilt SW-200D Toggle Switch  Keypad  Keypad - 3x4 I2C Nano Backpack Keypad - 4x4 I2C Nano Backpack Keypad - VKEY Keypad - Waveshare AD Touchpad - Grove QTouch Touchpad - MPR121 Touchpad - MPR121, Sensitivity Touchpad - MPR121QR2_SHIELD Touchpad - MPR121_KEYPAD Touchpad - MPR121_SHIELD  Relay  Relay Relay - Collection Relay On Analog Pin  Shift Register  Shift Register Shift Register - Common Anode Seven Segment controller Shift Register - Common Anode Seven segments, Chained Shift Register - Seven Segment controller Shift Register - Seven segments, Chained  Infrared Reflectance  IR Motion IR Proximity IR Reflectance IR Reflectance Array  Proximity  Proximity Proximity - EVShield EV3 (IR) Proximity - EVShield EV3 (IR) Proximity - EVShield EV3 (Ultrasonic) Proximity - EVShield EV3 (Ultrasonic) Proximity - GP2Y0A710K0F Proximity - HC-SR04 Proximity - HC-SR04 (Analog) Proximity - HC-SR04 I2C Backpack Proximity - LIDAR-Lite Proximity - MB1000 Proximity - MB1003 Proximity - MB1010 Proximity - MB1230 Proximity - SRF10  Motion  Motion Motion - GP2Y0A60SZLF Motion - GP2Y0D805Z0F Motion - GP2Y0D810Z0F Motion - GP2Y0D810Z0F  Joystick  Joystick Joystick - Esplora Joystick - Pan + Tilt control Joystick - Sparkfun Shield  LCD  Grove - RGB LCD Color Previewer LCD LCD - Enumerate characters LCD - I2C LCD - I2C PCF8574 LCD - I2C Runner LCD - Runner 16x2 LCD - Runner 20x4 LCD - Tessel 2 16x2 Tessel 2 + Grove - RGB LCD Color Previewer Tessel 2 + Grove - RGB LCD Display  Compass/Magnetometer  Compass - Find north Compass - HMC5883L Compass - HMC6352 Compass - Logger Compass - MAG3110 Compass - MAG3110 on Tessel 2 Compass / Magnetometer  Piezo  Piezo  IMU/Multi  IMU - BNO055 IMU - BNO055 (Orientation) IMU - LSM303C IMU - MPU6050 Multi - BME280 Multi - BMP085 Multi - BMP180 Multi - DHT11_I2C_NANO_BACKPACK Multi - DHT21_I2C_NANO_BACKPACK Multi - DHT22_I2C_NANO_BACKPACK Multi - HIH6130 Multi - HTU21D Multi - MPL115A2 Multi - MPL3115A2 Multi - MS5611 Multi - SHT31D Multi - SI7020 Multi - SI7021 Multi - TH02  Sensors  Accelerometer Accelerometer - ADXL335 Accelerometer - ADXL345 Accelerometer - LIS3DH Accelerometer - MMA7361 Accelerometer - MMA8452 Accelerometer - MPU6050 Accelerometer - Pan + Tilt Altimeter - BMP085 Altimeter - BMP180 Altimeter - MPL3115A2 Altimeter - MS5611 Barometer - BMP085 Barometer - BMP180 Barometer - MPL115A2 Barometer - MPL3115A2 Barometer - MS5611 Gyro Gyro - Analog LPR5150AL Gyro - I2C MPU6050 Hygrometer - DHT11_I2C_NANO_BACKPACK Hygrometer - DHT21_I2C_NANO_BACKPACK Hygrometer - DHT22_I2C_NANO_BACKPACK Hygrometer - HIH6130 Hygrometer - HTU21D Hygrometer - SHT31D Hygrometer - SI7021 Hygrometer - TH02 Sensor Sensor - Digital Microwave Sensor - Flex sensor Sensor - Force sensitive resistor Sensor - Microphone Sensor - Photoresistor Sensor - Potentiometer Sensor - Slide potentiometer Thermometer - BMP085 Thermometer - BMP180 Thermometer - DHT11_I2C_NANO_BACKPACK Thermometer - DHT21_I2C_NANO_BACKPACK Thermometer - DHT22_I2C_NANO_BACKPACK Thermometer - DS18B20 Thermometer - Dual DS18B20 Thermometer - HIH6130 Thermometer - HTU21D Thermometer - LM335 Thermometer - LM35 Thermometer - MAX31850 Thermometer - MCP9808 Thermometer - MPL115A2 Thermometer - MPL3115A2 Thermometer - MPU6050 Thermometer - MS5611 Thermometer - SHT31D Thermometer - SI7020 Thermometer - SI7021 Thermometer - TH02 Thermometer - TMP102 Thermometer - TMP36  Expander  Expander - 74HC595 Expander - CD74HC4067, 16 Channel Analog Input Breakout Expander - LIS3DH Expander - MCP23008 Expander - MCP23017 Expander - MUXSHIELD2, Analog Sensors Expander - MUXSHIELD2, Digital Input and Output Expander - PCA9685 Expander - PCF8574 Expander - PCF8575 Expander - PCF8591  Photon Weather Shield  Photon Weather Shield: Moisture  Lego EVShield  Button - EVShield EV3 Button - EVShield NXT Color - EVShield EV3 (Code) Color - EVShield EV3 (Raw) Color - EVShield NXT (Code) Light - BH1750 Light - EVShield EV3 (Ambient) Light - EVShield EV3 (Reflected) Light - EVShield NXT (Ambient) Light - EVShield NXT (Reflected) Light - TSL2561 Motor - EVShield EV3 Motor - EVShield NXT Proximity - EVShield EV3 (IR) Proximity - EVShield EV3 (Ultrasonic)  Intel Edison + Grove IoT Kit  Intel Edison + Grove - Accelerometer (ADXL345) Intel Edison + Grove - Accelerometer (MMA7660) Intel Edison + Grove - Air quality sensor Intel Edison + Grove - Barometer (BMP180) Intel Edison + Grove - Button Intel Edison + Grove - Compass (HMC588L) Intel Edison + Grove - Flame Sensor Intel Edison + Grove - Gas (MQ2) Intel Edison + Grove - Humidity & Temperature (TH02) Intel Edison + Grove - I2C Motor Driver Intel Edison + Grove - Joystick Intel Edison + Grove - LED Intel Edison + Grove - Light Sensor (TSL2561) Intel Edison + Grove - Moisture Sensor Intel Edison + Grove - Q Touch Intel Edison + Grove - RGB LCD Intel Edison + Grove - RGB LCD Color Previewer Intel Edison + Grove - RGB LCD temperature display Intel Edison + Grove - Relay Intel Edison + Grove - Rotary Potentiometer Intel Edison + Grove - Servo Intel Edison + Grove - Touch  Grove IoT Kit (Seeed Studio)  Grove - Button Grove - Joystick Grove - LED Grove - Motor (I2C Driver) Grove - RGB LCD Grove - RGB LCD temperature display Grove - Rotary Potentiometer Grove - Servo Grove - Touch  Micro Magician V2  Micro Magician V2 - Accelerometer Micro Magician V2 - Motor Micro Magician V2 - Servo  TinkerKit  TinkerKit - Accelerometer TinkerKit - Blink TinkerKit - Button TinkerKit - Combo TinkerKit - Continuous servo TinkerKit - Gyro TinkerKit - Joystick TinkerKit - Linear potentiometer TinkerKit - Rotary potentiometer TinkerKit - Temperature TinkerKit - Tilt TinkerKit - Touch  Wii  Wii Classic Controller Wii Nunchuck  Complete Bots / Projects  Bug Kinect Robotic Arm Controller Laser Trip Wire Line Follower Lynxmotion Biped BRAT Motobot Navigator Nodebot Phoenix Hexapod Radar Robotic Claw Whisker  Component Plugin Template  Example plugin  IO Plugins  Led Blink on Electric Imp Led Blink on Intel Edison Arduino Board Led Blink on Intel Edison Mini Board Led Blink on Intel Galileo Gen 2 Led Blink on Raspberry Pi Led Blink on Spark Core Led Blink on pcDuino3  Many fragments. Some large, some small. Wireless Nodebot Kinect Controlled Robot Arm Biped Nodebot LCD Running Man Slider Controlled Panning Servo Joystick Controlled Laser (pan/tilt) 1 Joystick Controlled Laser (pan/tilt) 2 Joystick Controlled Claw Robot Claw Joystick, Motor & Led Build you own drone Make: JavaScript Robotics  Contributing All contributions must adhere to the Idiomatic.js Style Guide, by maintaining the existing coding style. Add unit tests for any new or changed functionality. Lint and test your code using grunt. License Copyright (c) 2012, 2013, 2014 Rick Waldron waldron.rick@gmail.com Licensed under the MIT license. Copyright (c) 2014, 2015 The Johnny-Five Contributors Licensed under the MIT license.","johnny-five the javascript robottics programming framework artwork by mike sgier      johnny-five is an open source, firmata protoco",134,12200,3341,0,1,0,1
